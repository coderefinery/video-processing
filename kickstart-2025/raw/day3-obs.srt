1
00:00:00,000 --> 00:00:02,000
CodeRefinery.org

2
00:00:30,000 --> 00:00:32,060
you

3
00:01:00,000 --> 00:01:02,060
you

4
00:01:30,000 --> 00:01:32,060
you

5
00:02:00,000 --> 00:02:02,060
you

6
00:02:30,000 --> 00:02:32,060
you

7
00:03:00,000 --> 00:03:02,060
you

8
00:03:30,000 --> 00:03:32,060
you

9
00:04:00,000 --> 00:04:02,060
you

10
00:04:30,000 --> 00:04:32,060
you

11
00:12:30,000 --> 00:12:59,020
____________________________________________________________________________________________________________

12
00:12:59,020 --> 00:13:07,660
the course or not using what's in the course and what you would do otherwise.

13
00:13:09,420 --> 00:13:15,500
I got stun locked in the fruit section, wondering about the avocado. Is that a fruit?

14
00:13:16,060 --> 00:13:21,420
Like for me it goes into the vegetable family, but maybe it's a fruit like technically. Maybe

15
00:13:21,420 --> 00:13:26,060
there's seeds inside of it or something. I don't know like what's the definition, but

16
00:13:26,060 --> 00:13:29,300
But yeah, that's an interesting one.

17
00:13:29,300 --> 00:13:31,860
But avocados are really cool.

18
00:13:31,860 --> 00:13:36,540
OK, well, Wikipedia says it's a botanical fruit.

19
00:13:36,540 --> 00:13:40,140
OK, well, the more you know, the more you know.

20
00:13:45,980 --> 00:13:49,740
Wait, no, I searched the wrong thing.

21
00:13:49,740 --> 00:13:54,140
I opened the fruit page instead of the avocado page.

22
00:13:54,140 --> 00:13:57,340
Evergreen tree.

23
00:13:57,340 --> 00:14:02,340
It was prized for its large, unusually oily fruit.

24
00:14:02,340 --> 00:14:04,900
Yeah, I think maybe the fruit is part of the plant

25
00:14:04,900 --> 00:14:06,500
or something.

26
00:14:06,500 --> 00:14:07,020
But yeah.

27
00:14:07,020 --> 00:14:10,420
Only mention of vegetables in vegetable salad.

28
00:14:10,420 --> 00:14:11,980
So OK.

29
00:14:11,980 --> 00:14:17,180
Have to go for a Wikipedia deep dive again in the afternoon.

30
00:14:17,180 --> 00:14:19,260
OK.

31
00:14:19,260 --> 00:14:22,780
So [name] and [name], will you get to do much computing stuff

32
00:14:22,780 --> 00:14:23,780
next week.

33
00:14:23,780 --> 00:14:24,780
Yeah, hopefully.

34
00:14:24,780 --> 00:14:34,380
Hopefully, there's customers who have computing problems like for us, it's quite often like

35
00:14:34,380 --> 00:14:38,800
we are often the people who help people with the tools.

36
00:14:38,800 --> 00:14:47,120
We don't necessarily use the tools like by ourselves if like, because we often help the

37
00:14:47,120 --> 00:14:52,080
researchers with their problems, not so often that we do the research ourselves.

38
00:14:52,080 --> 00:14:55,340
So it depends on what the researchers are doing.

39
00:14:55,340 --> 00:15:01,820
So maybe some people from this course will drop by the garage and we get to do some fancy

40
00:15:01,820 --> 00:15:02,820
computation stuff.

41
00:15:02,820 --> 00:15:03,820
Yeah.

42
00:15:03,820 --> 00:15:04,820
Yeah.

43
00:15:04,820 --> 00:15:05,820
Cool.

44
00:15:05,820 --> 00:15:06,820
Yeah, that's a good answer.

45
00:15:06,820 --> 00:15:09,980
It's like really depends on the customers.

46
00:15:09,980 --> 00:15:16,100
So hopefully we would get new customers and it would do a lot of computations.

47
00:15:16,100 --> 00:15:19,340
But at the moment, I have something to do as well.

48
00:15:19,340 --> 00:15:20,340
Yeah.

49
00:15:20,340 --> 00:15:21,340
It's not related.

50
00:15:21,340 --> 00:15:26,860
It's more like a Kunda environment thing than the computational stuff, but yeah.

51
00:15:26,860 --> 00:15:27,860
Yeah.

52
00:15:27,860 --> 00:15:28,860
Okay.

53
00:15:28,860 --> 00:15:35,260
Do you want to know how long it takes to make videos?

54
00:15:35,260 --> 00:15:41,380
So [name] and [name], how long do you think is worth making videos?

55
00:15:41,380 --> 00:15:45,780
Like if you're doing it on the same day as you have taught the whole day, like I would

56
00:15:45,780 --> 00:15:47,100
say zero hours.

57
00:15:47,100 --> 00:15:54,220
I think that you should like leave them for another day, but let's say, I guess it takes like

58
00:15:56,300 --> 00:16:02,620
two hours, let's say. But how long is it worth taking? Oh, but you guessed, yeah.

59
00:16:04,140 --> 00:16:12,220
Zane, quick guess. I would say less than one, but I don't think it takes that long. I think

60
00:16:12,220 --> 00:16:23,900
it's like five. But yeah. So my guess is it takes between two and three hours each day.

61
00:16:24,620 --> 00:16:29,340
The most annoying part is revising the subtitles, which maybe I should ask

62
00:16:29,980 --> 00:16:39,340
[name] to write an AI-related thing that will do that for me automatically. But yeah.

63
00:16:39,340 --> 00:16:40,820
Yeah.

64
00:16:40,820 --> 00:16:42,460
OK, let's go on.

65
00:16:42,460 --> 00:16:49,700
So if we go to the schedule, which here, what do we have for?

66
00:16:53,660 --> 00:16:58,020
We have How to ask for help with supercomputers

67
00:16:58,020 --> 00:17:02,140
So I start with this How to Ask for Help, which is a talk.

68
00:17:02,140 --> 00:17:04,060
Actually, I'll add it later.

69
00:17:04,060 --> 00:17:06,900
Then we have two talks, which are basically

70
00:17:06,900 --> 00:17:12,020
about GPU-related things, for which [name] and [name] and you

71
00:17:12,020 --> 00:17:15,180
do, and then an end time where you can basically

72
00:17:15,180 --> 00:17:18,540
ask us questions about anything.

73
00:17:18,540 --> 00:17:20,940
And we'll answer that.

74
00:17:20,940 --> 00:17:23,460
Also, I guess it serves as a bit of a buffer.

75
00:17:23,460 --> 00:17:26,740
So with that being said, let's go to this.

76
00:17:26,740 --> 00:17:33,980
I will, [name], I need to share my screen here.

77
00:17:36,900 --> 00:17:47,220
Hmm, let's see, oh wait, there's too many buttons to click.

78
00:17:54,420 --> 00:18:06,020
Okay, yeah, some fun with the streaming stuff there. Okay, so welcome.

79
00:18:06,900 --> 00:18:09,180
The first talk is this,

80
00:18:09,180 --> 00:18:12,020
How to ask for help with supercomputers.

81
00:18:12,020 --> 00:18:14,120
This is actually not my talk,

82
00:18:14,120 --> 00:18:17,180
not my slides, though I've contributed a bit to it.

83
00:18:17,180 --> 00:18:19,380
It's by one of our colleagues,

84
00:18:19,380 --> 00:18:23,540
[name], who worked in Norway,

85
00:18:23,540 --> 00:18:27,980
basically doing a lot of the same stuff that we do.

86
00:18:28,100 --> 00:18:30,140
What's the point of this talk?

87
00:18:30,140 --> 00:18:34,340
To give you a perspective of whenever you come to us,

88
00:18:34,340 --> 00:18:36,700
when should you come to us and how to come with us,

89
00:18:36,700 --> 00:18:38,140
and ask us things.

90
00:18:41,980 --> 00:18:51,180
So yeah, what kind of help is available? When to ask? Especially this like how to write a message

91
00:18:51,900 --> 00:18:57,340
that looks good and we can answer quickly because we probably both want to be able to

92
00:18:58,700 --> 00:19:03,260
for you to be able to answer stuff or to have good messages so we can answer quickly

93
00:19:03,260 --> 00:19:10,520
and a little bit more advice on getting started. So yeah, here's the credits.

94
00:19:10,520 --> 00:19:19,580
There's some people you know there. Okay, so when you need help with something,

95
00:19:19,580 --> 00:19:26,260
what are the different places or ways you can ask for the help? So there's

96
00:19:26,260 --> 00:19:32,060
different things available. So often a good place to start is asking your

97
00:19:32,060 --> 00:19:37,340
colleague. So it's really simple to ask someone who sits at a desk next to you, hey, have you had

98
00:19:37,340 --> 00:19:43,100
this problem? If they say yes, then that helps you. If they say no, then you can keep working

99
00:19:43,100 --> 00:19:51,980
on it yourself before asking again. There's these official, like, live help sessions,

100
00:19:53,900 --> 00:19:59,900
garage office hours, stuff like that, search engines, chat, the real information.

101
00:19:59,900 --> 00:20:06,780
One interesting place is that you can search, if you're at Aalto, the issue tracker we have.

102
00:20:06,780 --> 00:20:12,300
So we ask people to make their issues there in public, so that way you can help all the

103
00:20:12,300 --> 00:20:18,460
next generation of things. That also encourages us to give better answers, because we know that

104
00:20:18,460 --> 00:20:27,420
it won't be helping just you, but all the people in the future. Anyway, people sometimes wonder

105
00:20:27,420 --> 00:20:32,460
when do you ask for help? So should you work on it forever? Are you going to get bitten if you go

106
00:20:32,460 --> 00:20:39,660
ask for help? Is that seen as a bad thing? So my philosophy is that computers are way too complex

107
00:20:39,660 --> 00:20:45,820
these days and no one can possibly tend to know everything they need to know about it. It's just

108
00:20:45,820 --> 00:20:51,180
too much. Even within our team here, we have a policy that if you don't know something, ask

109
00:20:51,180 --> 00:20:56,700
someone with a really low threshold because it's just not worth spending time trying to

110
00:20:57,420 --> 00:21:01,420
solve a problem that's already been solved. Unless, of course, you want to

111
00:21:04,220 --> 00:21:10,460
do an exercise to solve it. But, you know, your exercises are probably the actual research or

112
00:21:10,460 --> 00:21:17,020
other work you're doing and not some solved computer problem. But do at least try a little

113
00:21:17,020 --> 00:21:23,260
bit yourself, do your googling, start at the lower levels, and then work up as you see it's needed.

114
00:21:24,140 --> 00:21:29,340
You know, basically do enough that you learn something so next time maybe you can

115
00:21:30,780 --> 00:21:35,020
solve it yourself or know where to look for the future.

116
00:21:37,020 --> 00:21:42,620
So this is an advice I got when I asked someone what do you wish you knew now that

117
00:21:42,620 --> 00:21:46,860
what do you know now that you wish someone had told you? They had this philosophy. So,

118
00:21:46,860 --> 00:21:51,900
for every problem they had, they set an internal limit in their mind. So, they might say, okay,

119
00:21:52,540 --> 00:21:58,380
I'm trying to get Python to run on the cluster. I think it's reasonable that it should take me

120
00:21:59,260 --> 00:22:05,340
an hour to do that. So, I'll work on it for an hour, and then I'll go to the

121
00:22:05,340 --> 00:22:13,580
Science IT Garage and ask for help because that's the reasonable thing. If it's

122
00:22:13,580 --> 00:22:20,780
not done within an hour, then there's clearly something wrong here that is taking much longer

123
00:22:20,780 --> 00:22:29,020
than it should. When you ask for help, if you come especially to our live thing, you can ask,

124
00:22:29,020 --> 00:22:34,140
can you do this for me? Can we do it together? Or you could ask, should this be a hard problem?

125
00:22:34,140 --> 00:22:37,660
am I looking the right direction and should I keep going with where I am?

126
00:22:37,660 --> 00:22:42,460
And at least if you come to us and you say, is this the right direction? Like, can you help

127
00:22:42,460 --> 00:22:47,820
point me in the right documentation? Am I looking at the right page? We're happy to say, yeah, it

128
00:22:47,820 --> 00:22:54,060
looks like the right page. And then you can keep working and come back the next day if you need to.

129
00:22:56,780 --> 00:23:02,780
So, I guess if you're seeing this talk, you've already seen a lot of the people that are on

130
00:23:02,780 --> 00:23:11,180
the other side here. But the people answering are also humans and we're also not great at

131
00:23:11,180 --> 00:23:17,180
everything. We're not superheroes or something. We just try to be as good as we can.

132
00:23:18,460 --> 00:23:25,340
So this says something for the other side. So we do a lot of work in our team to try to

133
00:23:26,140 --> 00:23:32,460
teach, like remind people that we need to be nice when people are coming to us. So we have

134
00:23:32,460 --> 00:23:38,220
people at all sorts of different levels of education, or not education, but different

135
00:23:38,220 --> 00:23:43,820
points in their careers, knowing more or less who are studying computers or are not studying

136
00:23:43,820 --> 00:23:49,260
computers and it's just some sort of tool, and we need to be respectful and understanding of

137
00:23:49,260 --> 00:23:55,740
all these different types of things. And on the other hand, realize that, yeah, like it says,

138
00:23:55,740 --> 00:24:01,660
Because we don't know everything either.

139
00:24:01,660 --> 00:24:07,620
We're also really busy with possibly too many problems.

140
00:24:07,620 --> 00:24:11,380
So this is an example from the author of these slides.

141
00:24:11,380 --> 00:24:17,800
So someone wrote a super long message saying something, and it was actually hard for the

142
00:24:17,800 --> 00:24:22,260
person who was reading it to figure out what you even needed.

143
00:24:22,260 --> 00:24:25,740
So they responded with this one-line email saying,

144
00:24:25,740 --> 00:24:28,820
oh, I guess you want to get the code.

145
00:24:28,820 --> 00:24:29,820
Sure, here it is.

146
00:24:29,820 --> 00:24:32,580
So it could have been a much quicker request there.

147
00:24:36,700 --> 00:24:40,260
It's good to have a good email subject and things like that.

148
00:24:40,260 --> 00:24:42,860
So of course, you don't know what the actual problem is,

149
00:24:42,860 --> 00:24:46,340
but try to give a little bit of clarity of what it is.

150
00:24:46,340 --> 00:24:50,020
Because when someone goes to the issue tracker,

151
00:24:50,020 --> 00:24:51,780
they see a bunch of titles.

152
00:24:51,780 --> 00:24:57,060
and they'll start at looking at the stuff that they think there's the highest chance of them

153
00:24:57,060 --> 00:25:04,900
knowing and answering first. So that is something like my job is crashing today but not tomorrow

154
00:25:04,900 --> 00:25:12,180
or yesterday. Then we know, okay, so this might be something that has changed in the last day.

155
00:25:12,180 --> 00:25:16,660
we can look in there more quickly.

156
00:25:20,860 --> 00:25:23,740
It's good to have all this different context.

157
00:25:23,740 --> 00:25:28,260
So the more we know about what happened better.

158
00:25:28,260 --> 00:25:30,500
Sometimes someone will send a request saying,

159
00:25:30,500 --> 00:25:32,500
hey, my job isn't working.

160
00:25:32,500 --> 00:25:34,900
But they don't say what cluster they're on.

161
00:25:34,900 --> 00:25:37,580
So if you send it to the Alto people, that's obvious.

162
00:25:37,580 --> 00:25:40,420
But if you send it to, for example, CSE people,

163
00:25:40,420 --> 00:25:42,580
that's not obvious at all.

164
00:25:42,580 --> 00:25:45,100
And that just makes more time that has

165
00:25:45,100 --> 00:25:47,240
to be spent to figure that out.

166
00:25:47,240 --> 00:25:48,980
Or we have to send something back saying,

167
00:25:48,980 --> 00:25:50,860
can you please tell us what you actually did?

168
00:25:50,860 --> 00:25:52,420
And that's not fun.

169
00:25:52,420 --> 00:25:56,680
And we don't want to just say, reply always with,

170
00:25:56,680 --> 00:25:59,220
well, we don't really like your request, so just tell us more

171
00:25:59,220 --> 00:26:02,540
so we don't have to do anything.

172
00:26:02,540 --> 00:26:05,220
The more initial stuff, the better.

173
00:26:05,220 --> 00:26:08,660
So anything special that might be here?

174
00:26:08,660 --> 00:26:11,540
Is there anything special about the environment?

175
00:26:11,540 --> 00:26:14,100
Are we allowed to look at your files directly?

176
00:26:14,100 --> 00:26:14,820
And so on.

177
00:26:17,660 --> 00:26:23,900
OK, and this is something that I had thought up.

178
00:26:23,900 --> 00:26:29,060
So there, as soon as you see these lists of how to ask

179
00:26:29,060 --> 00:26:32,660
questions, and it's a long multi pages of do this, and this,

180
00:26:32,660 --> 00:26:33,860
and this, and this.

181
00:26:33,860 --> 00:26:35,860
And that's basically saying, we don't

182
00:26:35,860 --> 00:26:37,740
want you to ask questions because we

183
00:26:37,740 --> 00:26:40,620
I want you to have done all this other stuff first.

184
00:26:40,620 --> 00:26:43,900
So I tried to narrow it down to like a really simple thing.

185
00:26:43,900 --> 00:26:47,340
What are the most important things for us to know to answer

186
00:26:47,340 --> 00:26:49,500
without making it seem too overwhelming?

187
00:26:49,500 --> 00:26:50,800
And this is what it is.

188
00:26:50,800 --> 00:26:52,260
So has it ever worked?

189
00:26:52,260 --> 00:26:54,860
So if you're trying to run your first job on the cluster,

190
00:26:54,860 --> 00:26:58,220
that's a lot different than the jobs works yesterday,

191
00:26:58,220 --> 00:26:59,340
but not today.

192
00:26:59,340 --> 00:27:00,940
If it's never worked at all,

193
00:27:00,940 --> 00:27:04,420
that means we need to start with a much broader perspective.

194
00:27:04,420 --> 00:27:06,900
If something worked yesterday, but not today,

195
00:27:06,900 --> 00:27:13,940
and you know it's the same, we know to immediately ask the person that's been working on the cluster

196
00:27:13,940 --> 00:27:17,380
recently and, hey, have you changed anything? Did we just break something there?

197
00:27:19,220 --> 00:27:24,180
And what are you trying to accomplish? So not just like I'm trying to run this job on the cluster,

198
00:27:24,180 --> 00:27:30,260
but I'm trying to run, what's the program? I'm trying to run PyTorch on the cluster to do

199
00:27:30,260 --> 00:27:36,740
inference or whatever. So we'll see a little bit more about this later, but having the big picture

200
00:27:36,740 --> 00:27:41,060
of what you're trying to do is important, because if we don't have the big picture,

201
00:27:41,060 --> 00:27:49,300
we might give a too specific or too general answer somehow. Actually, no, I'll come back to this

202
00:27:49,300 --> 00:27:58,900
later. So what did you do? So what have you actually typed and done there? If you say,

203
00:27:58,900 --> 00:28:03,140
my job doesn't work, but you don't give the job script, well, we'll be like,

204
00:28:03,140 --> 00:28:07,340
please tell us the job script or whatever.

205
00:28:07,340 --> 00:28:10,980
The more you can copy and paste, the better.

206
00:28:10,980 --> 00:28:13,620
I recommend don't use screenshots because we

207
00:28:13,620 --> 00:28:16,780
can't copy and paste text from screenshots.

208
00:28:16,780 --> 00:28:18,580
Also, it doesn't appear in

209
00:28:18,580 --> 00:28:21,820
the e-mail messages we get and so on.

210
00:28:21,820 --> 00:28:23,460
But go to the terminal,

211
00:28:23,460 --> 00:28:25,100
copy, paste it into the message.

212
00:28:25,100 --> 00:28:26,380
It doesn't matter if it looks bad,

213
00:28:26,380 --> 00:28:30,460
we can dig through it.

214
00:28:30,460 --> 00:28:35,900
And this is like the exact commands you ran, the files,

215
00:28:35,900 --> 00:28:36,860
stuff like that.

216
00:28:36,860 --> 00:28:40,500
It's usually better to give more stuff rather than too few

217
00:28:40,500 --> 00:28:45,660
stuff, because we're quick at scanning through all this text

218
00:28:45,660 --> 00:28:46,900
and seeing what we need.

219
00:28:46,900 --> 00:28:48,500
And it doesn't take you very long

220
00:28:48,500 --> 00:28:54,020
to copy and paste relative to trying to describe it.

221
00:28:54,020 --> 00:28:56,020
And finally, what do you need?

222
00:28:56,020 --> 00:29:00,020
So are you looking for a complete solution to someone

223
00:29:00,020 --> 00:29:03,220
to solve the problem for you and give it back to you?

224
00:29:03,220 --> 00:29:07,140
Or are you looking for, hey, is this expected?

225
00:29:07,140 --> 00:29:09,100
Should I keep working on it?

226
00:29:09,100 --> 00:29:13,880
If you say, I just need help with this one little bit,

227
00:29:13,880 --> 00:29:14,580
that's OK.

228
00:29:14,580 --> 00:29:18,540
If you say, hey, I'm completely overwhelmed with what this is,

229
00:29:18,540 --> 00:29:21,740
can we look in more depth and do it from the beginning?

230
00:29:21,740 --> 00:29:25,820
Then we can start reserving time and doing it that way.

231
00:29:25,820 --> 00:29:27,740
Especially if you need something small,

232
00:29:27,740 --> 00:29:29,460
say that you need something small.

233
00:29:29,460 --> 00:29:30,580
That helps a lot.

234
00:29:33,860 --> 00:29:35,700
Yeah, this is the tell us what you tried.

235
00:29:35,700 --> 00:29:37,980
So have you tried narrowing it down?

236
00:29:37,980 --> 00:29:41,820
Is it failing in only one way or multiple ways?

237
00:29:41,820 --> 00:29:45,300
There's different ways to tell us

238
00:29:45,300 --> 00:29:48,940
how you've done the narrowing.

239
00:29:48,940 --> 00:29:53,380
And this is something that has been called the XY problem.

240
00:29:53,380 --> 00:29:54,720
There's some website around it.

241
00:29:54,720 --> 00:29:56,660
I don't know where it came from.

242
00:29:56,660 --> 00:30:01,220
But the problem is that someone has a problem.

243
00:30:01,220 --> 00:30:04,020
They try to do some solution to the problem.

244
00:30:04,020 --> 00:30:05,740
The solution doesn't work, so they ask

245
00:30:05,740 --> 00:30:08,300
about what the solution is.

246
00:30:08,300 --> 00:30:11,100
Like, I can't run this or that script.

247
00:30:11,100 --> 00:30:13,660
We go, we help them with that solution,

248
00:30:13,660 --> 00:30:17,220
and then it turns out that they were

249
00:30:17,220 --> 00:30:20,140
trying to do the original problem that wasn't actually

250
00:30:20,140 --> 00:30:22,020
a good solution to the problem.

251
00:30:22,020 --> 00:30:24,780
So we could have said, hey, you're

252
00:30:24,780 --> 00:30:27,720
trying to get a content environment solved,

253
00:30:27,720 --> 00:30:29,780
do you know we already have an environment

254
00:30:29,780 --> 00:30:32,700
that's designed for LM usage

255
00:30:32,700 --> 00:30:34,260
and it probably has what you need,

256
00:30:34,260 --> 00:30:36,260
so why don't you use that instead?

257
00:30:36,260 --> 00:30:39,460
So basically this big picture is quite important

258
00:30:39,460 --> 00:30:43,860
so we can see, do we need to take a step back

259
00:30:43,860 --> 00:30:46,180
and go down a different way?

260
00:30:46,180 --> 00:30:47,380
Also, if you don't do this,

261
00:30:47,380 --> 00:30:50,100
there's a problem that the staff will answer

262
00:30:50,100 --> 00:30:52,980
the one exact question you answer

263
00:30:52,980 --> 00:30:58,420
then say okay it's done and close the issue because they get measured by how quickly they

264
00:30:58,420 --> 00:31:03,940
close issues and how few issues there are so yeah

265
00:31:09,540 --> 00:31:14,260
I guess this is more advice for us that are here but if you're ever helping people

266
00:31:14,260 --> 00:31:21,860
think of these kind of things so it's important that we the staff answering make the person

267
00:31:21,860 --> 00:31:26,980
realize or think that they're valued and we want to actually help them. The worst thing to do is

268
00:31:26,980 --> 00:31:33,060
send back different links saying have you read these pages first what you need is in here and

269
00:31:33,060 --> 00:31:41,860
then like a two sentence answer or something like that. So we try to consider this and do well

270
00:31:42,420 --> 00:31:48,100
but if we're ever not perfect we're sorry and it's okay to let us know.

271
00:31:48,100 --> 00:31:53,100
So when you send requests, I guess

272
00:31:53,100 --> 00:31:57,780
this is perhaps more from other people.

273
00:31:57,780 --> 00:31:59,700
I guess it basically applied to us.

274
00:31:59,700 --> 00:32:02,900
So when you send a request in written format,

275
00:32:02,900 --> 00:32:05,500
it goes to a issue tracker.

276
00:32:05,500 --> 00:32:07,900
And then there's a bunch of issues there.

277
00:32:07,900 --> 00:32:11,620
And the people coming have to find what's available.

278
00:32:11,620 --> 00:32:15,020
So at least within Aalto, we sort of all

279
00:32:15,020 --> 00:32:17,340
try to look at it and ask the right person

280
00:32:17,340 --> 00:32:22,860
to answer it no matter what time it is. But at some point, in some organizations, there's like

281
00:32:22,860 --> 00:32:26,940
one person that's answering for one week and then they have multiple weeks when they're off

282
00:32:26,940 --> 00:32:33,180
and doing other work. So there can be very different types of people answering these questions.

283
00:32:37,180 --> 00:32:42,700
And if something takes too long and you don't get an answer, then consider asking a different way.

284
00:32:42,700 --> 00:32:46,780
if you're at Alto, then come to our daily garage

285
00:32:46,780 --> 00:32:48,700
and ask questions.

286
00:32:48,700 --> 00:32:51,980
Because if we get a written message

287
00:32:51,980 --> 00:32:56,980
and it's not clear what the question really is,

288
00:32:57,000 --> 00:32:59,200
people are always going to first work

289
00:32:59,200 --> 00:33:00,780
on the ones that are more clear.

290
00:33:00,780 --> 00:33:02,900
And if you come talk to us live,

291
00:33:02,900 --> 00:33:06,820
then it is much easier to do this quick back and forth

292
00:33:06,820 --> 00:33:09,060
and narrow things down and that works better.

293
00:33:09,060 --> 00:33:16,300
OK, we're a bit running out of time,

294
00:33:16,300 --> 00:33:17,980
so let's not go into this too much.

295
00:33:17,980 --> 00:33:24,220
But example request, I'm this username.

296
00:33:24,220 --> 00:33:25,460
I'm trying to do something.

297
00:33:28,380 --> 00:33:30,340
I can log into the other clusters.

298
00:33:30,340 --> 00:33:33,940
I can't log into this cluster and the exact message

299
00:33:33,940 --> 00:33:35,100
that's there.

300
00:33:35,100 --> 00:33:36,860
This is a pretty good request.

301
00:33:36,860 --> 00:33:39,220
So it gives the context.

302
00:33:39,220 --> 00:33:41,180
It gives what does work.

303
00:33:41,180 --> 00:33:44,020
And then it gives exactly what doesn't work.

304
00:33:44,020 --> 00:33:48,620
And from us, at least me as a cluster admin, if I see this,

305
00:33:48,620 --> 00:33:50,540
I immediately know what to look at.

306
00:33:50,540 --> 00:33:52,060
And I will check something.

307
00:33:55,620 --> 00:33:56,780
Here's another request.

308
00:33:56,780 --> 00:33:59,820
So it says, I'm not sure this is the right support.

309
00:33:59,820 --> 00:34:01,940
I'd like to do this and that.

310
00:34:01,940 --> 00:34:05,860
They're asking for a web server and a database.

311
00:34:05,860 --> 00:34:11,960
But then they say, this is the kind of thing,

312
00:34:11,960 --> 00:34:15,160
like this is what the big problem is.

313
00:34:15,160 --> 00:34:20,020
So we look at this and can say, hey, for 2,000 records,

314
00:34:20,020 --> 00:34:21,080
that's really not much.

315
00:34:21,080 --> 00:34:22,960
Maybe you don't need a full database.

316
00:34:22,960 --> 00:34:25,320
And here's a much easier way to do things.

317
00:34:25,320 --> 00:34:28,400
And we save you and us time.

318
00:34:28,400 --> 00:34:29,860
So it was very important that they

319
00:34:29,860 --> 00:34:32,200
gave this extra piece of information

320
00:34:32,200 --> 00:34:35,720
about sort of the size of the problem and the usage.

321
00:34:38,320 --> 00:34:39,680
Okay, maybe I'll just stop here.

322
00:34:39,680 --> 00:34:42,280
You can read here a little bit later

323
00:34:42,280 --> 00:34:44,000
or maybe I'll summarize.

324
00:34:44,000 --> 00:34:46,040
Yeah, so the summary here is that

325
00:34:49,540 --> 00:34:52,200
when you go to a new computer for the first time,

326
00:34:52,200 --> 00:34:54,980
don't immediately start with the biggest thing possible.

327
00:34:54,980 --> 00:34:56,280
Start with the smallest job.

328
00:34:56,280 --> 00:34:59,220
Like we started with an interactive job on one core

329
00:34:59,220 --> 00:35:02,180
and then a serial job, a batch job on one core.

330
00:35:02,180 --> 00:35:07,860
and then a few multi-processing. So start small and slowly work up so you can tell at exactly what

331
00:35:07,860 --> 00:35:14,020
point it stops working and it stops being efficient. And then that helps you to figure

332
00:35:14,020 --> 00:35:20,820
out only one thing at a time and it helps us to know where the problem is. Okay, and with that

333
00:35:20,820 --> 00:35:27,220
being said, I will shrink the screen. When you are with us,

334
00:35:31,300 --> 00:35:37,300
we have this thing called the Scientific Computing Garage. This is an Aalto University thing.

335
00:35:38,420 --> 00:35:45,540
I think some other people might be trying it some. But the basic idea is that

336
00:35:45,540 --> 00:35:53,140
that every workday at 1 o'clock, we, as in a lot of us,

337
00:35:53,140 --> 00:35:56,340
instructors and more, are in Zoom.

338
00:35:56,340 --> 00:35:58,980
And you can drop by and ask us any questions.

339
00:35:58,980 --> 00:36:02,340
This is a time you can join and share screen and sort of show

340
00:36:02,340 --> 00:36:04,460
us what's going on.

341
00:36:04,460 --> 00:36:06,800
You can ask, oh, is this a real problem?

342
00:36:06,800 --> 00:36:07,500
Should I do this?

343
00:36:07,500 --> 00:36:11,140
Should I file a issue request about this?

344
00:36:11,140 --> 00:36:12,580
And so on.

345
00:36:12,580 --> 00:36:17,540
And well, that's basically it. So it tells a lot about stuff here.

346
00:36:19,620 --> 00:36:26,980
Join us and ask for help with a low threshold. We have multiple people coming per day.

347
00:36:28,020 --> 00:36:35,700
And yeah. Oh, and finally, I'm running out of time here, but if you're new and you're at

348
00:36:35,700 --> 00:36:41,060
Alta University, we have people that have time in the next few weeks specifically to help with

349
00:36:41,060 --> 00:36:56,500
new problems and new questions. So, by all means, if you're new, okay, basically everyone that's

350
00:36:56,500 --> 00:37:02,660
starting new projects as a summer intern, we're trying to have time for you to come and spend

351
00:37:02,660 --> 00:37:07,060
an hour, half an hour, whatever, talking with someone live about how you're doing the problem.

352
00:37:07,060 --> 00:37:18,060
And we can't do everything in one garage next week, but we'd like for all of you to try to find the time to come by and have a quick chat with us.

353
00:37:18,060 --> 00:37:22,060
And we think we can probably do it in the next month.

354
00:37:22,060 --> 00:37:29,060
So, who knows how this will work, maybe we'll send an email with more instructions for reserving time or something like that.

355
00:37:29,060 --> 00:37:34,060
But, um, yes, please do.

356
00:37:34,060 --> 00:37:38,880
So OK, with that being said, I'm done.

357
00:37:38,880 --> 00:37:43,920
Hopefully this was a sort of useful starting point.

358
00:37:43,920 --> 00:37:46,200
And next we have GPUs, I believe.

359
00:37:46,200 --> 00:37:55,200
That's [name] and [name].

360
00:37:55,200 --> 00:37:57,480
Yeah, thanks, [name].

361
00:37:57,480 --> 00:37:59,760
Yeah, with that being said, I guess I'll go.

362
00:37:59,760 --> 00:38:01,920
See you later.

363
00:38:01,920 --> 00:38:02,420
Bye.

364
00:38:02,420 --> 00:38:10,260
we'll jump from how to ask questions into a topic that raises a lot of questions.

365
00:38:11,140 --> 00:38:21,460
So GPUs are the kind of stuff that often, like nowadays they are very popular,

366
00:38:21,460 --> 00:38:30,900
so they introduce a lot of questions to our users. So let's maybe start by talking about

367
00:38:30,900 --> 00:38:41,700
what is a GPU? Maybe we could look at quickly into this sort of a diagram and maybe use that

368
00:38:42,660 --> 00:38:53,220
as a starting point for the GPU. It's a bit small. [name], how would you describe a GPU?

369
00:38:55,700 --> 00:38:59,780
I think GPU is also a component of a computer that does the calculation

370
00:38:59,780 --> 00:39:07,780
but in a faster way. It's very good for parallelism. If you have small calculations,

371
00:39:07,780 --> 00:39:11,460
it's very good for it. But otherwise, the CPU is the way to go.

372
00:39:12,340 --> 00:39:21,460
Yeah. So, GPUs are, like [name] said, they're like a specific kind of an extra hardware component

373
00:39:21,460 --> 00:39:30,500
in the computer. And usually GPUs, they are like special cards or pieces in the computer

374
00:39:30,500 --> 00:39:38,980
that have this graphical processing unit that has multiple computing cores inside of it.

375
00:39:38,980 --> 00:39:44,420
So basically, I hope the font isn't too small here, but basically there's usually like,

376
00:39:44,420 --> 00:39:49,420
like you transfer your program and your data into the GPU,

377
00:39:50,900 --> 00:39:52,780
it has its own memory

378
00:39:52,780 --> 00:39:56,980
and it has a lot of these parallel computing cores

379
00:39:56,980 --> 00:40:00,940
and you can run various kinds of calculations on the GPU

380
00:40:02,540 --> 00:40:06,980
in parallel using all of these cores.

381
00:40:06,980 --> 00:40:11,180
But because it's a separate thing to the CPU,

382
00:40:11,180 --> 00:40:12,940
the central processing unit,

383
00:40:12,940 --> 00:40:22,060
it means that the code needs to specifically be written to utilize these cards because

384
00:40:22,060 --> 00:40:29,260
otherwise it won't use it. You cannot run normal code on a GPU, it needs to be a GPU code or needs

385
00:40:29,260 --> 00:40:38,780
to be using libraries that use GPUs in order to be able to access all of the power inside the GPU.

386
00:40:38,780 --> 00:40:39,780
Yeah.

387
00:40:39,780 --> 00:40:47,600
So how do you, like, nowadays there's lots of different GPUs available.

388
00:40:47,600 --> 00:40:54,100
So [name], how would you choose between, like, how would you know what is inside your

389
00:40:54,100 --> 00:40:58,820
GPU or what kind of GPUs are available nowadays?

390
00:40:58,820 --> 00:41:04,140
So I can talk about Triton, but in general, when you're talking about the GPU, you can

391
00:41:04,140 --> 00:41:09,180
see the model of the GPU but the beside the model and like that the specifics

392
00:41:09,180 --> 00:41:13,020
about the watt and energy usage and everything, the most important thing

393
00:41:13,020 --> 00:41:17,140
that you have to consider is the memory available for the GPU because as [name]

394
00:41:17,140 --> 00:41:21,540
mentioned the GPU memory is different than the normal RAM memory of the

395
00:41:21,540 --> 00:41:27,100
computer and the reason is they have a specific memory inside the GPU component

396
00:41:27,100 --> 00:41:31,700
and it's like very faster to write and read from. So if you go to the bottom of

397
00:41:31,700 --> 00:41:41,260
this page, [name], we have a list here that shows the different GPU types that we have,

398
00:41:41,260 --> 00:41:44,040
and also it shows the memory as well.

399
00:41:44,040 --> 00:41:45,980
It's a bit narrow, but if you can scroll...

400
00:41:45,980 --> 00:41:50,740
Yeah, I'll try to change the size a bit.

401
00:41:50,740 --> 00:41:51,740
Yes.

402
00:41:51,740 --> 00:41:52,740
Okay.

403
00:41:52,740 --> 00:41:53,740
Yes.

404
00:41:53,740 --> 00:41:55,820
If you go to the left a bit, yes.

405
00:41:55,820 --> 00:42:03,540
So now, for example, you can see that different GPUs have different number of, amount of memory

406
00:42:03,540 --> 00:42:04,540
that they have.

407
00:42:04,540 --> 00:42:09,540
For example, one have 616, one have 81, the advanced ones that we have, and at the moment

408
00:42:09,540 --> 00:42:14,780
the state of the art of the industry is 141 gigabytes.

409
00:42:14,780 --> 00:42:16,700
So it depends what you are choosing.

410
00:42:16,700 --> 00:42:21,860
You also have another thing that's called the architecture of a GPU, which I would say

411
00:42:21,860 --> 00:42:26,820
you are not an advanced user and you haven't heard of architecture of the GPU, you don't have to be

412
00:42:27,620 --> 00:42:33,700
worried about that. But just to have an idea, if you are compiling your program for a specific

413
00:42:33,700 --> 00:42:39,700
architecture, it doesn't run on another architecture. You can think of it as like

414
00:42:41,300 --> 00:42:50,420
ARM, like in a CPU world, ARM versus Intel, or like in the old days like 32 bits versus 64 bits.

415
00:42:50,420 --> 00:42:56,100
So if your program is compiled for another architecture, it doesn't run another architecture.

416
00:42:56,100 --> 00:43:01,380
Nowadays in the AI world, some of the advanced techniques like flash attention and these kind of

417
00:43:01,380 --> 00:43:06,260
things that we were going to talk about in the LLM part are actually

418
00:43:09,540 --> 00:43:13,220
compiled for a specific architecture, so you cannot use it in the old ones.

419
00:43:13,220 --> 00:43:17,220
But if you haven't heard of it, you don't have to worry about it and you can just

420
00:43:17,220 --> 00:43:23,220
go ahead and learn your program. Yeah, like in most cases, when you are going to be using the

421
00:43:23,220 --> 00:43:29,060
GPUs, you're going to use them through some, like in vast majority of cases, you're going to be

422
00:43:29,060 --> 00:43:35,860
using them through some libraries that are compiled with various backends of this architecture,

423
00:43:35,860 --> 00:43:45,780
so they can run on different versions of the GPUs. So the question usually arises that when we

424
00:43:45,780 --> 00:43:49,220
talked about, let's say the memory and the architecture and stuff like that, is that

425
00:43:50,900 --> 00:43:56,660
the GPU you choose, especially in case of Triton, where we have so many different

426
00:43:56,660 --> 00:44:04,100
packagings of these GPUs, is that it depends quite a bit on the problem that you have.

427
00:44:04,100 --> 00:44:09,940
If you have a very small problem, like you try to do some, let's say the first MNIST

428
00:44:09,940 --> 00:44:18,580
data set, PyTorch model or whatever, you try to do a quick AI test thing, it won't require

429
00:44:18,580 --> 00:44:25,460
any much resources. So it will run basically in the same time with any of these GPUs because

430
00:44:25,460 --> 00:44:32,180
the model is so small, it cannot use all of the compute capability in the card and it doesn't use

431
00:44:32,180 --> 00:44:38,020
all of the memory in the card. So it doesn't matter which thing you use. But if you're going to be

432
00:44:38,020 --> 00:44:43,140
doing the LLMs that we'll be talking about later. You might have heard a lot of talk about the

433
00:44:43,700 --> 00:44:50,340
parameters, like how many parameters these models have and stuff like that. The number of parameters

434
00:44:50,980 --> 00:44:58,500
means also the amount of memory consumed by storing those parameters in memory increases.

435
00:44:58,500 --> 00:45:05,540
If you have these big LLM models or stuff like that, they require a lot of memory. In those

436
00:45:05,540 --> 00:45:16,660
cases you're usually forced to use the GPUs in the higher end that have more memory available.

437
00:45:16,660 --> 00:45:25,140
But you notice that the total amount of GPUs, we have a lot of these V100 cards, so we have

438
00:45:25,140 --> 00:45:33,300
plenty of these smaller cards that you can use for smaller projects. And it's not bad to use

439
00:45:33,300 --> 00:45:41,100
GPUs, like different kinds of GPUs. But maybe we should quickly show how you can see, how

440
00:45:41,100 --> 00:45:52,260
you can differentiate or choose between these. And how do you do it in many clusters in Aalto

441
00:45:52,260 --> 00:45:59,900
as well is by setting up in your code which partition you want to use. So you can, there's

442
00:45:59,900 --> 00:46:05,100
other ways as well, but the easiest way is to set up the different partitions. So, when you submit

443
00:46:05,100 --> 00:46:11,020
a Slurm job, you want to give it that, hey, I want the card from this partition. Or you can

444
00:46:11,020 --> 00:46:23,980
put multiple partitions over there, and you can use slurm-e in your terminal to view all of the

445
00:46:23,980 --> 00:46:29,420
available partitions. It will print a lot of information over here, but what you see is that

446
00:46:29,420 --> 00:46:35,980
you can get the partition names and you can of course look at the reference as well. But if you

447
00:46:35,980 --> 00:46:43,980
want to use a certain kind of a GPU, you usually need to provide the partition that I want to use

448
00:46:43,980 --> 00:46:50,380
this sort of GPU. That is true. I guess in our cluster you can also make the constraint for the

449
00:46:50,380 --> 00:46:56,540
architecture as well, but the GPU partition is the easier way to go. And if you don't specify

450
00:46:56,540 --> 00:47:01,820
a partition, but you request for a GPU, the Slurm would automatically assign to all of the

451
00:47:01,820 --> 00:47:07,100
partitions available, except for the one that you have to specifically ask for, the advanced ones

452
00:47:07,100 --> 00:47:11,820
that we have, which you specifically have to ask for it to be assigned to that specific partition.

453
00:47:12,860 --> 00:47:24,380
Yes, and usually when you're working with these cards, quite often, like I mentioned

454
00:47:24,380 --> 00:47:29,420
previously, you are going to be using them through some libraries. You're not actually

455
00:47:30,380 --> 00:47:35,980
going to be using them directly because all of these cars, they utilize something called CUDA.

456
00:47:35,980 --> 00:47:46,220
You might have heard about it. CUDA is this computing stack that NVIDIA provides. AMD has

457
00:47:46,220 --> 00:47:56,380
its own called ROCm, but basically they are the same kind of idea. And in there, there's usually

458
00:47:57,820 --> 00:48:03,980
you have the GPU, which is the hardware part, and then you have this driver library that is

459
00:48:03,980 --> 00:48:14,140
installed on the GPU nodes, and then you have some CUDA library that your code uses to actually do

460
00:48:14,140 --> 00:48:23,580
the calculations. So in the case of most cases that you see, you need to have some CUDA library

461
00:48:23,580 --> 00:48:30,060
installed into your environment or you need to use a module with a CUDA in order to actually

462
00:48:30,060 --> 00:48:37,580
use the GPUs. And in most cases, you probably will use something like you install PyTorch with

463
00:48:37,580 --> 00:48:42,860
GPU capability and it will bring you the corresponding CUDA toolkit that you need.

464
00:48:44,140 --> 00:48:50,780
So that's about installation or setup that we forgot to mention.

465
00:48:50,780 --> 00:49:00,220
Usually you want to have the specific CUDA toolkit for your program that will then use the GPUs.

466
00:49:00,220 --> 00:49:16,460
But, okay, maybe we should mention also other ways you can use these, maybe interactive

467
00:49:16,460 --> 00:49:17,460
usage.

468
00:49:17,460 --> 00:49:23,520
Yeah, that's actually a good part because GPUs and GPU computing can be a bit tricky

469
00:49:23,520 --> 00:49:29,380
and when you have a large program, as [name] mentioned, and if you have a large pipeline

470
00:49:29,380 --> 00:49:35,540
of training or doing simulations. It's very hard to and if it doesn't work it's very hard to

471
00:49:35,540 --> 00:49:41,940
understand why it's not working. So it's very good to start small at the beginning and when you are

472
00:49:41,940 --> 00:49:47,940
when you you know that your code works then you can jump to the higher end of GPUs if you need them.

473
00:49:49,060 --> 00:49:58,020
So in the SLURM that the partitions that [name] is showing we have a partition called the

474
00:49:58,020 --> 00:50:04,260
gpu debug partition if you jump back to the yeah it's i don't think we have it here but

475
00:50:04,820 --> 00:50:13,300
yeah it's probably yeah i think it's mentioned somewhere there but uh yeah i think it's mentioned

476
00:50:13,300 --> 00:50:20,180
here yes yes uh so we have a partition called gpu debug and the limitation is you have only 30

477
00:50:20,180 --> 00:50:26,420
minutes of the time uh so it's like very good for testing your program or start uh running your

478
00:50:26,420 --> 00:50:33,380
program at the beginning for one run or one epoch if you're training a neural network and then see

479
00:50:33,380 --> 00:50:39,700
if it works then you can request for a large node. The reason is sometimes if your code doesn't work

480
00:50:39,700 --> 00:50:43,620
it's not your fault but like there are a lot of things that you have to be considerate about

481
00:50:43,620 --> 00:50:50,740
and it's good to start small because if you request for a large one, one it's harder to debug

482
00:50:50,740 --> 00:50:54,420
Second, you have to wait probably longer time in the queue,

483
00:50:54,420 --> 00:50:58,340
so then your program would run and then it would fail.

484
00:50:58,340 --> 00:51:00,300
Yeah, if it fails, you can run

485
00:51:00,300 --> 00:51:03,940
the partition with the debug and see what is going on.

486
00:51:03,940 --> 00:51:07,620
It's hard to work with the terminal.

487
00:51:07,620 --> 00:51:09,380
There are some ways you can

488
00:51:09,380 --> 00:51:11,460
connect with the VS Code and everything,

489
00:51:11,460 --> 00:51:13,620
but come and ask us in the garage.

490
00:51:13,620 --> 00:51:17,340
It's a bit complicated to talk about it here.

491
00:51:17,340 --> 00:51:19,820
But since the beginning of this year,

492
00:51:19,820 --> 00:51:25,660
we also have the GPU debug available with some conditions on the Jupyter on demand.

493
00:51:26,780 --> 00:51:28,540
Should we show that, [name]?

494
00:51:30,060 --> 00:51:33,180
Maybe we could show the interactive uses from the terminal.

495
00:51:37,580 --> 00:51:45,660
If we want to reserve a GPU, like if we go to the, like how do we get a GPU?

496
00:51:45,660 --> 00:51:50,380
Let's first take one from a GPU debug partition.

497
00:51:50,380 --> 00:51:56,700
So what we do is we would run the exact same things that we would do normally.

498
00:51:57,660 --> 00:52:04,620
Like we run the srun, we would add all of the things we want to have here.

499
00:52:04,620 --> 00:52:07,020
So let's say like 10-minute job.

500
00:52:07,020 --> 00:52:09,900
And I'm going to run a program called NVIDIA SMI.

501
00:52:09,900 --> 00:52:21,660
This was mentioned in the monitoring part that this NVIDIA SMI is a program that shows what

502
00:52:21,660 --> 00:52:28,060
the GPU is doing. Basically, it shows what the GPU is doing. But currently, I haven't requested

503
00:52:28,060 --> 00:52:36,300
any GPUs here. I haven't actually added any GPU requests here. I can add that by first adding the

504
00:52:36,300 --> 00:52:45,340
partition I wanted to use. So I want from the GPU debug and then I can say GPUs one.

505
00:52:45,340 --> 00:52:51,260
So that's the only thing you need and GPUs equal one. And now if I submit this to the queue,

506
00:52:54,220 --> 00:52:57,340
it's allocated resources and let me make this a bit

507
00:52:57,340 --> 00:53:06,460
And you can see over here, it prints out the output of the nvidia-smi, so now it only runs this

508
00:53:06,460 --> 00:53:10,860
one command in the node and then it returns back.

509
00:53:10,860 --> 00:53:19,220
And you can see that I have a GPU called Tesla V100, 16 gigabytes.

510
00:53:19,220 --> 00:53:25,340
So I got the 16 gigabyte V100 card.

511
00:53:25,340 --> 00:53:33,140
So, yeah, as you can see, like the GPU debug is almost instantly allocate the resources

512
00:53:33,140 --> 00:53:35,300
because it has the limitation of 30 minutes.

513
00:53:35,300 --> 00:53:38,900
So usually there are like free GPUs for you to get allocated.

514
00:53:38,900 --> 00:53:45,620
And then you don't have to wait a long time for that for the slurm queue to get your resources.

515
00:53:45,620 --> 00:53:50,940
But at the same time, the topic that I want to mention is that when we are talking about

516
00:53:50,940 --> 00:53:56,380
GPUs because they are more expensive at the moment as well because of the demand and everything.

517
00:53:56,380 --> 00:53:59,740
If you look at the stock of the NVIDIA, you would notice this.

518
00:53:59,740 --> 00:54:10,300
But be more careful and be more responsible about the GPU usage because we don't have a lot,

519
00:54:10,300 --> 00:54:14,300
we have much, but we don't have enough, I would say.

520
00:54:14,300 --> 00:54:23,020
So when you are requesting, try to not be, like, make it idle or, like, don't actually utilize it.

521
00:54:23,020 --> 00:54:28,300
So if you notice that you are not utilizing the GPU, the utilization is not high or you are not,

522
00:54:29,980 --> 00:54:35,500
I don't know, like, your program is not as fast as possible, please come to us and we will try to

523
00:54:35,500 --> 00:54:40,540
help you with optimizing your code. So your simulation or your training will run faster and

524
00:54:40,540 --> 00:54:46,700
also, we would make the computation that is available as much as we can.

525
00:54:47,580 --> 00:54:55,660
Yeah, especially the interactive usage, it can get quite quickly. You could use the GPU,

526
00:54:55,660 --> 00:55:00,140
but whenever you're not actually running GPU code, the GPU isn't doing anything.

527
00:55:00,140 --> 00:55:08,380
So, it's just waiting there. So, it can quite quickly just underutilize all of the GPUs and

528
00:55:08,380 --> 00:55:14,460
because the demand is heavy, it's a good idea to utilize them as much as possible.

529
00:55:18,460 --> 00:55:23,100
What we usually recommend is that if you, let's say, want to do a training or something,

530
00:55:24,060 --> 00:55:31,020
like a deep learning training, if you can do it one batch, basically, if you can train it for one

531
00:55:31,020 --> 00:55:37,420
batch or one epoch, you can train it for a thousand epochs, basically, because you can do it, just

532
00:55:37,420 --> 00:55:45,340
continue doing what you're doing previously. So just run it longer, basically. So it's quite

533
00:55:45,340 --> 00:55:53,420
easy to run them usually in the debug partition. You can just run the GPUs in the debug partition

534
00:55:53,420 --> 00:56:02,700
like a small job. And once you have run the code, you notice that, okay, now it has run.

535
00:56:02,700 --> 00:56:14,060
And now, basically, I can start coding again without preserving the GPU for the whole of my

536
00:56:14,060 --> 00:56:19,100
coding session. But let's quickly look at what the GPUs can actually do. So, this is again,

537
00:56:19,100 --> 00:56:24,220
like, unfortunately, a bit of a toy model. So, what we're going to be doing is from the

538
00:56:24,220 --> 00:56:28,980
the documentation we're, whoops, clicked the wrong link.

539
00:56:28,980 --> 00:56:33,060
We are going to be running this Pi example again,

540
00:56:33,060 --> 00:56:36,340
our friend Pi, and what we're going to be doing

541
00:56:36,340 --> 00:56:41,340
is that we are going to be building our own GPU code.

542
00:56:41,340 --> 00:56:46,040
So let's first check which folder am I in.

543
00:56:47,380 --> 00:56:52,380
I can get, sorry, terminal, I'm hiding, yes.

544
00:56:52,380 --> 00:57:03,300
I'm currently at the wrong folder, so let me go to the work directory and then I'm going

545
00:57:03,300 --> 00:57:06,900
to go to the hpc-examples.

546
00:57:06,900 --> 00:57:13,100
What we are going to be doing is now building a GPU program, basically.

547
00:57:13,100 --> 00:57:16,780
For that, we need a compiler and a CUDA toolkit.

548
00:57:16,780 --> 00:57:27,800
This is basically, if you don't use a program that is pre-built, and most of them nowadays

549
00:57:27,800 --> 00:57:30,180
are, but some aren't.

550
00:57:30,180 --> 00:57:35,840
And if you don't use that sort of program, you need to give this massive, usually these

551
00:57:35,840 --> 00:57:42,260
flags when you're compiling in order to make it so that it works with different architectures,

552
00:57:42,260 --> 00:57:44,740
the code works with different GPUs.

553
00:57:44,740 --> 00:57:50,180
And now we have a program called pi-gpu.c.

554
00:57:50,180 --> 00:57:56,100
So you can try this in your own cluster in a minute.

555
00:57:56,100 --> 00:57:58,500
You can try it out.

556
00:57:58,500 --> 00:58:03,180
And let's run this pi-gpu in the queue.

557
00:58:03,180 --> 00:58:05,740
So let's add the GPU.

558
00:58:05,740 --> 00:58:13,820
So this is the exact same kind of a Py calculating code, but this time it's written so that it

559
00:58:13,820 --> 00:58:17,140
gives us the GPUs.

560
00:58:17,140 --> 00:58:22,180
So I will add all of the Slurm flags.

561
00:58:22,180 --> 00:58:24,980
So we want the interactive terminal,

562
00:58:24,980 --> 00:58:29,940
and then 10 minutes, 500 megabytes of memory, one GPU,

563
00:58:29,940 --> 00:58:32,860
and we run in the debug partition.

564
00:58:32,860 --> 00:58:34,780
And then the pi-gpu.

565
00:58:34,780 --> 00:58:37,900
And let's give it some nice numbers.

566
00:58:37,900 --> 00:58:41,580
So let's say, because this is so much faster than the CPU one,

567
00:58:41,580 --> 00:58:44,020
So let's say like, is it a hundred million?

568
00:58:44,100 --> 00:58:44,460
Yes.

569
00:58:49,140 --> 00:58:52,060
So previously we run, yeah, yeah.

570
00:58:52,060 --> 00:58:53,300
It's, it's way too low.

571
00:58:53,380 --> 00:58:55,900
Uh, so this is like billion.

572
00:58:56,180 --> 00:58:58,900
So previously we run with the CPU.

573
00:58:58,940 --> 00:59:04,700
It took 40 seconds to run 50 million and it's basically instantaneous

574
00:59:04,700 --> 00:59:06,420
to run a billion with the GPU.

575
00:59:06,420 --> 00:59:10,380
So it's, they, they are for this sort of calculations.

576
00:59:10,380 --> 00:59:18,460
GPUs are much, much faster, so in order to actually utilize the GPU, we need to, well,

577
00:59:18,460 --> 00:59:25,980
even add more zeros there. They are way too fast for this sort of a program.

578
00:59:26,700 --> 00:59:33,740
So, you can see already the power compared to the CPU. And this is, of course, just a toy example.

579
00:59:33,740 --> 00:59:45,740
But now we can of course write a script that would do the same exact thing.

580
00:59:45,740 --> 00:59:52,740
So here's an example script that would run the same thing in the queue.

581
00:59:52,740 --> 00:59:57,740
But maybe it would be a good time to have some exercises.

582
00:59:57,740 --> 01:00:00,740
So you can try out the demo that I just posted here.

583
01:00:00,740 --> 01:00:04,540
Yeah, of course, the major caveat is that in your cluster,

584
01:00:04,540 --> 01:00:06,780
the modules might be different.

585
01:00:06,780 --> 01:00:08,620
So in order to get the CUDA module,

586
01:00:08,620 --> 01:00:11,500
you probably need to maybe load a different compiler

587
01:00:11,500 --> 01:00:12,340
or different module.

588
01:00:12,340 --> 01:00:13,780
So they might be different

589
01:00:13,780 --> 01:00:15,540
and the partitions might be different.

590
01:00:15,540 --> 01:00:18,340
So now it's a good time to also look at the documentation

591
01:00:18,340 --> 01:00:22,740
and check in your cluster, what are the GPU partitions,

592
01:00:22,740 --> 01:00:24,860
what GPUs are available

593
01:00:24,860 --> 01:00:29,860
and what CUDA modules, for example, are available.

594
01:00:30,740 --> 01:00:39,700
I just want to add one more thing that although GPUs are very powerful to use, but it doesn't

595
01:00:39,700 --> 01:00:41,960
mean that it's for every code.

596
01:00:41,960 --> 01:00:47,180
So every code is different and if your code knows how to do depolarization and you have

597
01:00:47,180 --> 01:00:54,020
a small task to do, I would say, they are very good, but it's not for everyone.

598
01:00:54,020 --> 01:00:56,400
It's not for every computation.

599
01:00:56,400 --> 01:01:03,600
For this Pi example, it's a very, very good match and it can be run a lot faster than the CPU

600
01:01:03,600 --> 01:01:08,320
because you can do all of the calculations in parallel.

601
01:01:13,840 --> 01:01:19,520
Let's have the exercises. We'll probably answer a few of the questions in the notes.

602
01:01:19,520 --> 01:01:27,640
like we'll answer them through the exercises and we'll probably answer a few of those and

603
01:01:27,640 --> 01:01:35,520
then we'll have a break. So try out the demo that I posted here. I'll post this into the

604
01:01:35,520 --> 01:01:37,520
notes as well.

605
01:01:37,520 --> 01:01:39,520
Yeah.

606
01:02:05,520 --> 01:02:34,160
Is it supposed to be exercise now? Time now? Should I go to the break or?

607
01:02:35,520 --> 01:02:45,520
Yes, let's have exercise until 45, sorry, 55.

608
01:02:45,520 --> 01:02:46,520
Yeah.

609
01:02:51,520 --> 01:02:54,520
Okay, then I will say bye.

610
01:03:05,520 --> 01:03:07,580
you

611
01:03:35,520 --> 01:03:37,580
you

612
01:04:05,520 --> 01:04:07,580
you

613
01:04:35,520 --> 01:04:37,580
you

614
01:05:05,520 --> 01:05:07,580
you

615
01:05:35,520 --> 01:05:37,580
you

616
01:06:05,520 --> 01:06:07,580
you

617
01:06:35,520 --> 01:06:37,580
you

618
01:07:05,520 --> 01:07:07,580
you

619
01:07:35,520 --> 01:07:37,580
you

620
01:08:05,520 --> 01:08:07,580
you

621
01:08:35,520 --> 01:08:37,580
you

622
01:09:05,520 --> 01:09:07,580
you

623
01:09:35,520 --> 01:09:37,580
you

624
01:10:05,520 --> 01:10:07,580
you

625
01:10:35,520 --> 01:10:37,580
you

626
01:11:05,520 --> 01:11:07,580
you

627
01:11:35,520 --> 01:11:37,580
you

628
01:12:05,520 --> 01:12:07,580
you

629
01:12:35,520 --> 01:12:37,580
you

630
01:13:05,520 --> 01:13:15,240
And hello, we are back.

631
01:13:15,240 --> 01:13:23,720
So I hope you had a possibility to attend the exercise, it was quite a short amount

632
01:13:23,720 --> 01:13:25,920
of time, sorry about that.

633
01:13:25,920 --> 01:13:31,000
There was a lot of good questions or good discussion in the notes.

634
01:13:31,000 --> 01:13:44,520
So there was a question about the GPU RAM, and what is the rule of thumb for requesting

635
01:13:44,520 --> 01:13:46,900
RAM and GPU?

636
01:13:46,900 --> 01:13:56,440
So the RAM in the computer and the VRAM, like the RAM in the GPU, they are different.

637
01:13:56,440 --> 01:14:05,080
they often are different when you're doing all sorts of calculations. So as an example,

638
01:14:05,080 --> 01:14:12,360
like if you're doing machine learning or kind of a thing, you usually need to reserve the VRAM for

639
01:14:12,360 --> 01:14:18,280
the model that you're going to be training and the data that the model is going to be taking in and

640
01:14:18,280 --> 01:14:23,800
the data that the model is going to be putting out. But you often don't need to reserve data

641
01:14:23,800 --> 01:14:28,200
for the whole data set, unless you load the whole data set into the GPU memory.

642
01:14:28,200 --> 01:14:31,400
But quite often, the data set doesn't even fit into the GPU memory.

643
01:14:31,960 --> 01:14:38,760
So you have a situation where you might request more RAM, like the normal RAM,

644
01:14:38,760 --> 01:14:42,760
and then you have a GPU with a different amount of VRAM.

645
01:14:44,840 --> 01:14:50,120
But sometimes, of course, like so often the situation is something like you might request

646
01:14:50,120 --> 01:14:59,880
the same amount of RAM as the GPU RAM or a bit more, but it depends on how your data is going

647
01:14:59,880 --> 01:15:04,920
to be loaded into the GPU. But they're different, and you cannot request a different amount of

648
01:15:05,560 --> 01:15:15,640
VRAM unless you choose a different GPU. You cannot partition or split up that VRAM in any way.

649
01:15:15,640 --> 01:15:18,280
you will either get the card or you won't get the card.

650
01:15:19,320 --> 01:15:23,160
It is also true about the computation as well, right? When you are requesting the GPU,

651
01:15:23,160 --> 01:15:29,720
the whole GPU is for you, the whole RAM of the GPU and also the whole compute nodes.

652
01:15:29,720 --> 01:15:35,400
Yes. And there was also a question about what is the difference between the GPUs, so

653
01:15:36,280 --> 01:15:44,360
the different numbers here. So they are like the technical numbers, what is the brand of the GPU,

654
01:15:44,360 --> 01:15:49,080
basically. And the newer the brand, they are usually faster, they have more memory,

655
01:15:49,080 --> 01:15:56,120
they are faster, they have also some more advanced specialized computing units for

656
01:15:56,120 --> 01:16:02,120
certain kinds of calculations that we don't have time to go through here. But basically,

657
01:16:03,000 --> 01:16:11,080
that's sort of a situation happening there. But usually it's like the newer is more powerful

658
01:16:11,080 --> 01:16:18,680
and bigger. That's the situation.

659
01:16:22,040 --> 01:16:27,880
Yeah, and there's lots of great questions here that we'll answer, but we need to go to break,

660
01:16:28,920 --> 01:16:37,320
I think, at this point. We'll return to the questions here, and we'll talk a lot.

661
01:16:37,320 --> 01:16:42,760
we'll answer all of them here and we can after the LLM session we have the panel discussion where we

662
01:16:42,760 --> 01:16:49,880
can probably raise some of these questions there as well but in the LLM section you will hear more

663
01:16:49,880 --> 01:16:55,320
about like what is the actual use like how do you actually use the GPUs like how do how does it

664
01:16:55,320 --> 01:17:01,240
actually look like when you use the GPUs but yeah in order to get to there we first need to take a

665
01:17:01,240 --> 01:17:11,560
bit of a break. So, we'll be back at 11.10. Is that good?

666
01:17:12,360 --> 01:17:13,000
That's good, yeah.

667
01:17:13,560 --> 01:17:17,320
Yeah, we'll be back at 11.10. Bye.

668
01:17:18,920 --> 01:17:19,400
See you.

669
01:17:31,240 --> 01:17:33,300
you

670
01:18:01,240 --> 01:18:03,300
you

671
01:18:31,240 --> 01:18:33,300
you

672
01:19:01,240 --> 01:19:03,300
you

673
01:19:31,240 --> 01:19:33,300
you

674
01:20:01,240 --> 01:20:03,300
you

675
01:20:31,240 --> 01:20:33,300
you

676
01:21:01,240 --> 01:21:03,300
you

677
01:21:31,240 --> 01:21:33,300
you

678
01:22:01,240 --> 01:22:03,300
you

679
01:22:31,240 --> 01:22:33,300
you

680
01:23:01,240 --> 01:23:03,300
you

681
01:23:31,240 --> 01:23:33,300
you

682
01:24:01,240 --> 01:24:03,300
you

683
01:24:31,240 --> 01:24:33,300
you

684
01:25:01,240 --> 01:25:03,300
you

685
01:25:31,240 --> 01:25:33,300
you

686
01:26:01,240 --> 01:26:03,300
you

687
01:26:31,240 --> 01:27:00,920
Hello again, hello everyone, we're back, I guess we can skip

688
01:27:00,920 --> 01:27:11,000
introduction. You have seen me and [name] before. This section will be about the LMs and generative

689
01:27:11,000 --> 01:27:19,160
AI tools. Please let me shortly overview the generative AI services available in Aalto.

690
01:27:20,600 --> 01:27:28,440
So we have a public service as a web service. It's similar as ChatGPT, but it's under

691
01:27:28,440 --> 01:27:33,120
agreement between Aalto and the company.

692
01:27:33,120 --> 01:27:36,400
We call it Ardor GPT or Aalto AI Assistant.

693
01:27:36,400 --> 01:27:42,040
It's similar as Chat GPT and based on the same models.

694
01:27:42,040 --> 01:27:48,520
Secondly, you can use some remote APIs to run models.

695
01:27:48,520 --> 01:27:57,160
Aalto also has institutional agreement with Azure OpenAI API,

696
01:27:57,160 --> 01:27:59,160
so you can request that.

697
01:27:59,160 --> 01:28:02,200
It has a better data security.

698
01:28:02,200 --> 01:28:03,980
In addition to that,

699
01:28:03,980 --> 01:28:07,780
we have hosted some local models

700
01:28:07,780 --> 01:28:12,680
on entirely on our own infrastructure.

701
01:28:12,680 --> 01:28:17,280
Those APIs are based on open source models,

702
01:28:17,280 --> 01:28:21,640
so they have very good data privacy.

703
01:28:21,640 --> 01:28:24,320
So basically, you can use them and

704
01:28:24,320 --> 01:28:27,600
maybe request for different models.

705
01:28:27,600 --> 01:28:31,640
This job is in developing.

706
01:28:31,640 --> 01:28:35,400
Of course, if you want to run models locally on

707
01:28:35,400 --> 01:28:41,520
your computer or on a cluster to get higher performance,

708
01:28:41,520 --> 01:28:46,320
it's also good. Actually, that will be our main focus today.

709
01:28:46,320 --> 01:28:51,520
But usually, your computer doesn't have enough resource.

710
01:28:51,520 --> 01:28:56,240
so Triton or the other cluster you're working on will be a good option.

711
01:28:58,000 --> 01:29:06,720
But in addition to these general AI tools, we also have a dedicated tool for speech-to-text.

712
01:29:07,680 --> 01:29:15,360
[name] knows better about this. Yeah, but just to add something about the API that is provided by

713
01:29:15,360 --> 01:29:23,280
ITS. If I'm not mistaken you can use it with confidential data as well but it has this cluster

714
01:29:23,280 --> 01:29:28,000
structure and you have to go and like fill a form and everything but if you host your model locally

715
01:29:28,000 --> 01:29:32,800
you don't have to be worried about the cost and everything all except for the GPU time that you

716
01:29:32,800 --> 01:29:41,200
are you are getting for that for the cluster. So yeah in the project. But yeah but we have also

717
01:29:41,200 --> 01:29:46,480
another service called Speech-to-Text. It's based on the OpenAI Whisper and it's very good for

718
01:29:46,480 --> 01:29:53,360
transcribing your videos or audios or interviews that you have. We would show how to run it on,

719
01:29:53,360 --> 01:29:57,440
or maybe we don't have time, I don't know, but if you go to the docs and click on the link

720
01:29:57,440 --> 01:30:02,400
there is a very good documentation that you can follow both on on-demand on the web service and

721
01:30:02,400 --> 01:30:09,520
also on the terminal if you prefer that. All of the processing is done on Triton so you can process

722
01:30:09,520 --> 01:30:14,960
confidential data as well, and it supports multiple languages. It's at the moment very

723
01:30:14,960 --> 01:30:23,040
fast because it utilizes on the GPUs. I guess that's it.

724
01:30:24,000 --> 01:30:30,560
Is there anything else that you want to add? I think that's it. Let's go to our main focus

725
01:30:30,560 --> 01:30:42,720
today, which is running LLMs on the cluster. So we know that LLMs, LLMs models, they are very big

726
01:30:42,720 --> 01:30:49,760
and they usually need quite a lot of disk space to store the models. So we pre-downloaded

727
01:30:50,960 --> 01:30:59,840
the popular models on Triton. So you can access them and you can use them. It's in a shared

728
01:30:59,840 --> 01:31:05,400
the folder so it won't occupy your own disk space.

729
01:31:05,400 --> 01:31:10,760
But there's some tricks to use the models.

730
01:31:10,760 --> 01:31:15,380
We will talk about the coding part later.

731
01:31:15,380 --> 01:31:20,460
But firstly, maybe you want to know which models are available.

732
01:31:20,460 --> 01:31:27,100
So simply run this command on Triton, of course.

733
01:31:27,100 --> 01:31:30,580
you will see all the models listed there.

734
01:31:30,580 --> 01:31:33,820
You can request more models if you need,

735
01:31:33,820 --> 01:31:36,140
and we will also be keeping

736
01:31:36,140 --> 01:31:40,500
our eyes on the latest models and download them for you.

737
01:31:40,500 --> 01:31:43,660
The most common way to use the models

738
01:31:43,660 --> 01:31:47,900
is by using Transformers Python library.

739
01:31:47,900 --> 01:31:50,460
It's developed by Hugging Face,

740
01:31:50,460 --> 01:31:55,740
and it also provides a lot of tools and models,

741
01:31:55,740 --> 01:32:04,700
of course, to use it has a very good interfaces to use the models. We will demonstrate some

742
01:32:04,700 --> 01:32:14,140
examples later. But after you know that where to find the model, the next thing to consider is to

743
01:32:15,100 --> 01:32:24,540
how much resources to request to use the model. You have just listened to the wonderful talk from

744
01:32:24,540 --> 01:32:26,940
similar and the whole thing about GPU computing.

745
01:32:26,940 --> 01:32:30,020
So when it comes to LLMs,

746
01:32:30,020 --> 01:32:33,360
GPU is where most of the time needed.

747
01:32:33,360 --> 01:32:36,740
So you needed to request the GPUs.

748
01:32:36,740 --> 01:32:41,540
The thing is, what kind of GPUs to request?

749
01:32:41,540 --> 01:32:44,180
That depends on the model,

750
01:32:44,180 --> 01:32:46,260
which model you're using.

751
01:32:46,260 --> 01:32:48,700
Meanwhile, you needed to request

752
01:32:48,700 --> 01:32:55,700
some number of CPUs and system memories.

753
01:32:55,700 --> 01:32:59,220
You have just listened to this,

754
01:32:59,220 --> 01:33:01,740
that the system memory is

755
01:33:01,740 --> 01:33:04,580
something separated from the GPU memory.

756
01:33:04,580 --> 01:33:09,140
Let's first talk a bit about the GPU memory.

757
01:33:09,140 --> 01:33:11,740
When you request a GPU,

758
01:33:11,740 --> 01:33:13,660
you get the whole GPU,

759
01:33:13,660 --> 01:33:16,900
so all the memory belongs to you.

760
01:33:16,900 --> 01:33:23,700
but different GPUs have different amount of memory.

761
01:33:23,940 --> 01:33:28,300
On Triton, we can use it as infer.

762
01:33:28,300 --> 01:33:30,500
I think [name] also showed that

763
01:33:30,500 --> 01:33:34,780
to get a list of partitions available.

764
01:33:34,780 --> 01:33:41,340
Partition names can reflect the GPU type.

765
01:33:41,340 --> 01:33:45,820
For example, GPU V100 with 32 gigabytes memory,

766
01:33:45,820 --> 01:33:54,660
There are also some other newer GPUs like A100 or H100 and H200.

767
01:33:54,660 --> 01:33:57,140
But the thing is,

768
01:33:57,140 --> 01:33:59,780
how much memory do you need?

769
01:33:59,780 --> 01:34:02,980
It depends on the model size.

770
01:34:02,980 --> 01:34:11,620
We have a table here as a reference that usually,

771
01:34:11,620 --> 01:34:15,140
when you find a model on Hugging Face,

772
01:34:15,140 --> 01:34:17,020
there will be a model card.

773
01:34:17,020 --> 01:34:22,820
The model card will tell you how many parameters the model has,

774
01:34:22,820 --> 01:34:27,480
and also from the model name,

775
01:34:27,480 --> 01:34:30,500
you can also know the number of parameters.

776
01:34:30,500 --> 01:34:33,820
For example, it's a Mistral 7-billion model,

777
01:34:33,820 --> 01:34:36,700
then it has seven billion parameters.

778
01:34:36,700 --> 01:34:41,220
But it depends on the data type used by the model.

779
01:34:41,220 --> 01:34:46,220
it could be like 28 gigabytes or 14 gigabytes,

780
01:34:46,380 --> 01:34:48,540
or if you quantize the model,

781
01:34:48,540 --> 01:34:53,540
then it will be even like a half the memory.

782
01:34:53,980 --> 01:34:58,980
So this table can be roughly telling you

783
01:34:59,220 --> 01:35:03,100
that how much GPU memory you need

784
01:35:03,100 --> 01:35:06,040
and then which kind of GPU you can request.

785
01:35:07,620 --> 01:35:09,980
But there's some other tricky things here

786
01:35:09,980 --> 01:35:14,260
Because AI is developing very fast,

787
01:35:14,260 --> 01:35:18,940
some models, they use different operators.

788
01:35:18,940 --> 01:35:22,340
And some operators or model layers

789
01:35:22,340 --> 01:35:28,660
may need a different GPU computing capacity.

790
01:35:28,660 --> 01:35:36,900
So it's a bit tricky that maybe the model can fit in the GPU,

791
01:35:36,900 --> 01:35:41,340
but the GPU doesn't support the model

792
01:35:41,340 --> 01:35:45,620
or the software you use to run the model.

793
01:35:45,620 --> 01:35:49,420
So you may also face this kind of errors.

794
01:35:49,420 --> 01:35:54,940
Don't be very worried.

795
01:35:54,940 --> 01:35:55,940
You can come to us.

796
01:35:55,940 --> 01:35:59,260
We can help you to do the troubleshooting.

797
01:35:59,260 --> 01:36:03,020
And you can also try out, generally speaking,

798
01:36:03,020 --> 01:36:05,660
try out newer GPUs.

799
01:36:05,660 --> 01:36:09,220
So that's about the GPU memory.

800
01:36:09,220 --> 01:36:12,780
Let's go back to this again.

801
01:36:14,220 --> 01:36:17,500
And there's the system memory.

802
01:36:17,500 --> 01:36:22,500
The system memory, well, like [name] mentioned before,

803
01:36:23,080 --> 01:36:27,460
it depends on how your program was written

804
01:36:27,460 --> 01:36:30,920
and how the data will be loaded.

805
01:36:30,920 --> 01:36:34,660
So usually we will request a bit more memory,

806
01:36:34,660 --> 01:36:37,940
system memory than GPU memory.

807
01:36:37,940 --> 01:36:42,460
For example, here, the GPU memory is 32 gigabytes.

808
01:36:42,460 --> 01:36:44,700
I put 80 gigabytes here,

809
01:36:44,700 --> 01:36:49,460
but it's probably a bit too much, I would say.

810
01:36:49,460 --> 01:36:52,640
But roughly speaking,

811
01:36:52,640 --> 01:36:55,380
the system memory is less expensive,

812
01:36:55,380 --> 01:36:59,720
so I would use a slightly higher number.

813
01:36:59,720 --> 01:37:03,500
For example, maybe 40 gigabytes is enough here.

814
01:37:03,500 --> 01:37:10,140
And then the CPUs, a number of CPUs.

815
01:37:10,140 --> 01:37:15,240
Sometimes you need multiple CPUs to run the models.

816
01:37:15,240 --> 01:37:20,420
Some framework can use multi-processes to do

817
01:37:20,420 --> 01:37:26,420
some data pre-processing or even some intermediate steps,

818
01:37:26,420 --> 01:37:28,500
many more CPUs.

819
01:37:28,500 --> 01:37:33,340
So in general, I would put a few more CPUs here

820
01:37:33,340 --> 01:37:38,260
And again, it's less expensive than GPUs.

821
01:37:38,260 --> 01:37:40,820
So what do you think, [name]?

822
01:37:40,820 --> 01:37:44,420
Do you have anything to add here?

823
01:37:44,420 --> 01:37:46,700
I was trying to answer a question.

824
01:37:46,700 --> 01:37:48,620
But no, I think it's fine.

825
01:37:48,620 --> 01:37:50,820
But the thing that you were mentioning

826
01:37:50,820 --> 01:37:53,020
about the architecture of the model

827
01:37:53,020 --> 01:37:54,980
is also an important thing.

828
01:37:54,980 --> 01:37:57,700
And again, if you don't know what is happening,

829
01:37:57,700 --> 01:37:59,300
I guess you don't have to worry about.

830
01:37:59,300 --> 01:38:02,660
But sometimes, some part of the model

831
01:38:02,660 --> 01:38:05,020
or like some part of the calculation needs

832
01:38:05,020 --> 01:38:08,300
and a specific kind of architecture like flash attention

833
01:38:08,300 --> 01:38:10,200
that it's like very popular nowadays.

834
01:38:11,100 --> 01:38:14,220
But if you run it on a normal GPU

835
01:38:14,220 --> 01:38:15,740
that doesn't support the flash attention,

836
01:38:15,740 --> 01:38:17,500
it would just raise an error.

837
01:38:17,500 --> 01:38:21,760
So it's like very likely, especially at the beginning

838
01:38:21,760 --> 01:38:23,540
when you're working with these huge models

839
01:38:23,540 --> 01:38:26,500
to care to face a lot of issues

840
01:38:26,500 --> 01:38:29,860
and then don't get worried, come and ask us

841
01:38:29,860 --> 01:38:31,900
and we would try to help you.

842
01:38:31,900 --> 01:38:36,340
Or ask GPT in that case, but we prefer human interaction.

843
01:38:36,340 --> 01:38:37,940
Yes.

844
01:38:37,940 --> 01:38:39,020
OK.

845
01:38:39,020 --> 01:38:42,460
Yeah, I mentioned that we have downloaded a lot of models

846
01:38:42,460 --> 01:38:46,720
for you and put them in a shared folder.

847
01:38:46,720 --> 01:38:49,540
The thing is Hugging Face Transformers

848
01:38:49,540 --> 01:38:53,540
doesn't look for the models by default.

849
01:38:53,540 --> 01:38:58,820
There is an environment variable called hifhome.

850
01:38:58,820 --> 01:39:01,940
By default, it's pointing to your home folder.

851
01:39:01,940 --> 01:39:04,620
As you have learned on the first day,

852
01:39:04,620 --> 01:39:07,420
I guess, your home folder is kind of small.

853
01:39:07,420 --> 01:39:13,460
It has only 20 gigabytes disk space.

854
01:39:13,460 --> 01:39:15,860
So what you should do essentially

855
01:39:15,860 --> 01:39:22,820
is to change the Hugging Face home to the shared folder

856
01:39:22,820 --> 01:39:24,060
we have.

857
01:39:24,060 --> 01:39:28,260
But again, as you have learned before on Triton,

858
01:39:28,260 --> 01:39:30,380
or I would say most of the clusters,

859
01:39:31,700 --> 01:39:35,660
a lot of softwares and tools will be managed by modules.

860
01:39:35,660 --> 01:39:38,420
So here we also use a module to manage

861
01:39:38,420 --> 01:39:42,380
the access to the models.

862
01:39:42,380 --> 01:39:46,380
So you need to module load the model hacking face.

863
01:39:46,380 --> 01:39:51,380
What this does is basically setting the hacking face home

864
01:39:53,020 --> 01:39:57,540
to the correct location, the correct directory,

865
01:39:57,540 --> 01:40:02,300
which is this one here, yeah.

866
01:40:02,300 --> 01:40:04,900
And by doing this,

867
01:40:04,900 --> 01:40:08,560
transformers will know where to find the models.

868
01:40:08,560 --> 01:40:13,560
But after that, you need to have a proper conda environment

869
01:40:13,580 --> 01:40:15,860
to use the models.

870
01:40:15,860 --> 01:40:19,580
We have created a model, sorry,

871
01:40:19,580 --> 01:40:22,420
we have created a conda environment,

872
01:40:22,420 --> 01:40:26,900
which is called scicomp-llm-env.

873
01:40:26,900 --> 01:40:29,180
This is managed by us.

874
01:40:29,180 --> 01:40:33,300
It has most of the popular frameworks,

875
01:40:33,300 --> 01:40:38,300
like Transformers, like llama.cpp and the stuff like that.

876
01:40:39,900 --> 01:40:42,260
But if you need your own ones,

877
01:40:42,260 --> 01:40:45,740
you can make a customized conda info

878
01:40:45,740 --> 01:40:50,740
by using the knowledge you learned from yesterday, I think.

879
01:40:50,980 --> 01:40:54,500
But anyway, for this demo, we can use this one.

880
01:40:54,500 --> 01:41:00,380
And most of the times, this one would work, I would say.

881
01:41:00,380 --> 01:41:04,260
So that's the Python environment.

882
01:41:04,260 --> 01:41:07,020
But after that, we needed to do something else

883
01:41:07,020 --> 01:41:13,740
to let Transformer to use the local model instead

884
01:41:13,740 --> 01:41:18,540
of downloading models from HuggingFace.

885
01:41:18,540 --> 01:41:22,620
So we needed to set up two environment variables.

886
01:41:22,620 --> 01:41:27,620
Here is transformers offline, kind of set it to true,

887
01:41:27,660 --> 01:41:30,860
and then export another one.

888
01:41:30,860 --> 01:41:35,460
It's also for transformers to load the local models,

889
01:41:35,460 --> 01:41:39,060
and then run your Python script.

890
01:41:39,060 --> 01:41:42,620
We will show the example later.

891
01:41:43,780 --> 01:41:46,180
I will skip this Python script.

892
01:41:46,180 --> 01:41:49,020
We will see it in the demo.

893
01:41:52,620 --> 01:41:56,460
Yeah. I think that's all from here.

894
01:41:56,460 --> 01:41:59,280
We can start to the demo now.

895
01:41:59,280 --> 01:42:02,680
Maybe we can show a real example.

896
01:42:02,680 --> 01:42:03,800
Sorry?

897
01:42:03,800 --> 01:42:05,360
Yeah, we can show a real example.

898
01:42:05,360 --> 01:42:10,280
It can be a bit overwhelming to hear all of this in a 30-minute session,

899
01:42:10,280 --> 01:42:15,280
but once you understand a bit of it and then get your hands dirty,

900
01:42:15,280 --> 01:42:17,720
you would, don't worry, you will get it.

901
01:42:17,720 --> 01:42:20,560
But at the beginning, it can be really overwhelming with

902
01:42:20,560 --> 01:42:23,080
all of the environments and different parameters

903
01:42:23,080 --> 01:42:24,620
that you have to set.

904
01:42:24,620 --> 01:42:25,900
Yeah.

905
01:42:25,900 --> 01:42:27,900
Okay, let me go to the,

906
01:42:28,940 --> 01:42:33,440
I'll use the on-demand Jupyter Notebook to do the demo.

907
01:42:35,860 --> 01:42:38,140
This is my work directory.

908
01:42:38,140 --> 01:42:42,140
I haven't downloaded the examples.

909
01:42:43,860 --> 01:42:45,960
I will go to the repo.

910
01:42:47,700 --> 01:42:48,540
Okay.

911
01:42:50,560 --> 01:42:53,160
The LLM examples report, yeah.

912
01:42:53,160 --> 01:42:55,680
Yes, it's in the note.

913
01:43:00,320 --> 01:43:05,000
You can see we have a few different examples here.

914
01:43:05,000 --> 01:43:08,040
Today we will focus on the hacking face models.

915
01:43:08,040 --> 01:43:12,520
We also have models for VLLM and

916
01:43:12,520 --> 01:43:19,200
also a small example for like a chat with your PDF.

917
01:43:19,200 --> 01:43:25,240
It's a kind of AI application example,

918
01:43:25,240 --> 01:43:28,800
but we will focus to HuggingFace.

919
01:43:29,160 --> 01:43:34,040
So basically, we have a minimal example here.

920
01:43:34,040 --> 01:43:37,960
I have the Summator script.

921
01:43:40,640 --> 01:43:43,040
So this is the one.

922
01:43:43,040 --> 01:43:48,600
This is a batch script you have seen from the previous days.

923
01:43:48,600 --> 01:43:55,880
Basically, I have the time here and the number of CPUs and the memory.

924
01:43:55,880 --> 01:43:57,920
Okay, I have 40 gigabytes here.

925
01:43:57,920 --> 01:44:00,540
It's more reasonable.

926
01:44:00,540 --> 01:44:02,860
I request one GPU.

927
01:44:02,860 --> 01:44:05,820
Of course, you can request the two GPUs if

928
01:44:05,820 --> 01:44:10,540
the model is large and couldn't fit in one GPU.

929
01:44:10,540 --> 01:44:13,420
But that's a little bit advanced.

930
01:44:13,420 --> 01:44:18,420
Maybe we can look at that later.

931
01:44:18,420 --> 01:44:23,500
If you need help, come to our help session.

932
01:44:23,500 --> 01:44:27,260
Then the partition, I will use this V100.

933
01:44:27,260 --> 01:44:29,980
I would say this is a bit old GPU,

934
01:44:29,980 --> 01:44:35,900
but the model I'm going to run is also not a very new one.

935
01:44:35,900 --> 01:44:38,860
As I mentioned before,

936
01:44:38,860 --> 01:44:43,000
load the necessary modules and run the script.

937
01:44:43,000 --> 01:44:47,140
I will submit the job firstly because it will take a while.

938
01:44:47,140 --> 01:44:51,140
Yes. But what does it do, the script?

939
01:44:51,140 --> 01:44:53,660
Can you also show the script and we can see?

940
01:44:53,660 --> 01:44:55,660
The Python script.

941
01:44:55,660 --> 01:45:00,460
I will show that later after I submit the job.

942
01:45:00,460 --> 01:45:02,980
Now it's submitted and let's

943
01:45:02,980 --> 01:45:06,220
check out if it can start immediately.

944
01:45:06,220 --> 01:45:10,740
It's pending. Let's go to the script.

945
01:45:11,180 --> 01:45:14,820
This is the script I submitted by using

946
01:45:14,820 --> 01:45:17,620
the shell script.

947
01:45:17,620 --> 01:45:21,820
It's an example to use the model.

948
01:45:21,820 --> 01:45:24,420
This is the model I'm using here.

949
01:45:24,420 --> 01:45:28,700
HuggingFace provides basically

950
01:45:28,700 --> 01:45:32,660
two different ways to run the model.

951
01:45:32,660 --> 01:45:36,140
The first one is a little bit complex.

952
01:45:36,140 --> 01:45:40,280
You can see it has multiple steps.

953
01:45:40,280 --> 01:45:43,960
Firstly, you need to load the model,

954
01:45:43,960 --> 01:45:49,080
like initialize the model and specify the data type.

955
01:45:49,080 --> 01:45:51,260
I use auto here.

956
01:45:51,260 --> 01:45:54,060
By using this, it will automatically choose

957
01:45:54,060 --> 01:45:57,760
the best data type or

958
01:45:57,760 --> 01:46:02,960
the default data type when the model was released.

959
01:46:02,960 --> 01:46:11,240
I also did some memory usage reduction by quantization.

960
01:46:11,240 --> 01:46:20,120
I will load the model with 8-bit instead of float 32 or float 16.

961
01:46:20,120 --> 01:46:26,200
I set the device map to auto to let it automatically find the GPUs available.

962
01:46:28,520 --> 01:46:31,320
Maybe I shouldn't go to that detailed stuff.

963
01:46:33,880 --> 01:46:40,920
We have the tokenizer and the tokenizer will be initialized under the messages.

964
01:46:41,240 --> 01:46:52,240
You want to pass to the model and then use the tokenizer to do some preprocessing for the text, like apply some chat template.

965
01:46:52,240 --> 01:46:59,240
But again, we won't go to the details. This is necessary for some models.

966
01:46:59,240 --> 01:47:07,240
And then the inputs after the preprocessing, the inputs will be passed to the model.

967
01:47:07,240 --> 01:47:15,240
The model has a generate method and then we will see the generated stuff.

968
01:47:15,240 --> 01:47:22,240
But this is a slightly complex procedure to use HuggingFace models.

969
01:47:22,240 --> 01:47:30,240
There's a higher level help API from Transformers or from HuggingFace.

970
01:47:30,240 --> 01:47:36,240
It's easier, but before that, maybe let's go back to the terminal to see if it's running.

971
01:47:36,240 --> 01:47:40,420
Okay, it's done, I think.

972
01:47:42,240 --> 01:47:43,840
Okay, it's done.

973
01:47:43,840 --> 01:47:46,840
Actually, I want to show you the GPU usage.

974
01:47:48,720 --> 01:47:52,560
Maybe I should submit it again.

975
01:47:53,640 --> 01:47:56,720
But we can also look at the seff data as well.

976
01:47:56,720 --> 01:47:57,960
Yeah, yeah, of course.

977
01:47:59,360 --> 01:48:04,220
And let me see if it's still running.

978
01:48:04,220 --> 01:48:05,680
Okay, it's running.

979
01:48:05,680 --> 01:48:07,800
Anyway, let's go back to this one.

980
01:48:08,760 --> 01:48:12,240
So a easier way to use the transformers

981
01:48:12,240 --> 01:48:17,240
is by using the pipeline API provided by Hugging Face.

982
01:48:18,160 --> 01:48:20,960
You don't need to specify the tokenizers

983
01:48:20,960 --> 01:48:25,840
and like initialize the model separately.

984
01:48:25,840 --> 01:48:28,800
You just put all the stuff to the pipeline,

985
01:48:28,800 --> 01:48:32,600
the name of the model, sorry, the task,

986
01:48:32,600 --> 01:48:43,160
name of the model and the device, the GPU, and some other parameters, some generation parameters

987
01:48:43,160 --> 01:48:50,120
or model arguments and all the configuration stuff here. Then the message, then it's...

988
01:48:51,720 --> 01:48:55,800
The job is running, so if you want to show that... Yes, let's see.

989
01:48:55,800 --> 01:49:08,800
It's quite, I hope it won't be stop.

990
01:49:08,800 --> 01:49:15,800
Okay, now we can see the usage of GPU.

991
01:49:15,800 --> 01:49:20,800
So, I gave it 32 gigabytes of memory GPU,

992
01:49:20,800 --> 01:49:25,040
GPU, but it used only half of the memory.

993
01:49:25,040 --> 01:49:32,080
GPU utilization is also lower than 50%.

994
01:49:32,080 --> 01:49:36,640
Well, this is something you can see yearly.

995
01:49:36,640 --> 01:49:40,320
OK, now I think the calculation has started.

996
01:49:40,320 --> 01:49:43,920
The usage of the memory is increasing to 24.

997
01:49:43,920 --> 01:49:58,240
Okay, yeah, so basically, you can have a look at the GPU memory usage and the computation

998
01:49:58,240 --> 01:50:06,040
usage in real time, but as [name] mentioned that after the job is done, we have the other

999
01:50:06,040 --> 01:50:11,680
tools to show the resource usage.

1000
01:50:11,680 --> 01:50:16,680
Okay, now, maybe let's have a look at the output.

1001
01:50:18,880 --> 01:50:21,160
We have two, the first one.

1002
01:50:22,560 --> 01:50:27,560
Yeah, the output, well, it's not something very exciting.

1003
01:50:29,000 --> 01:50:34,000
I just give it a message and it will apply a chat template,

1004
01:50:34,000 --> 01:50:37,680
it will apply a chat template,

1005
01:50:38,540 --> 01:50:42,660
kind of give the model some reminders.

1006
01:50:42,660 --> 01:50:46,540
Okay, this is the starting point of the prompt,

1007
01:50:46,540 --> 01:50:47,380
this is the end,

1008
01:50:47,380 --> 01:50:51,220
and this is the starting point of your message.

1009
01:50:51,220 --> 01:50:56,220
Sorry, so firstly is the system prompt

1010
01:50:56,580 --> 01:50:59,100
like you are helpful assistant thing,

1011
01:50:59,100 --> 01:51:02,500
and then it's the starting point of your message,

1012
01:51:02,500 --> 01:51:03,660
your question.

1013
01:51:03,660 --> 01:51:08,020
And then it will also add the starting point

1014
01:51:08,020 --> 01:51:10,580
for the assistant, kind of telling it,

1015
01:51:10,580 --> 01:51:15,180
now it's your turn to continue the generation

1016
01:51:15,180 --> 01:51:16,920
to answer the questions.

1017
01:51:18,380 --> 01:51:19,420
Yeah, we can see.

1018
01:51:20,520 --> 01:51:25,520
Yeah, then the pipeline, by using the pipeline,

1019
01:51:25,660 --> 01:51:29,820
we kind of skip the intermediate steps

1020
01:51:29,820 --> 01:51:32,620
and get the response.

1021
01:51:33,660 --> 01:51:41,660
By default, it will give you the prompts and the answers.

1022
01:51:41,660 --> 01:51:45,860
You can do post-processing for this.

1023
01:51:47,540 --> 01:51:51,940
Do we have any questions?

1024
01:51:52,820 --> 01:51:57,140
I was trying to answer the questions.

1025
01:51:57,140 --> 01:52:02,180
We have a user who faced an error.

1026
01:52:02,180 --> 01:52:10,180
with the CUDA, I guess, but I'm not sure. But I just want to ask a question. When we're

1027
01:52:10,180 --> 01:52:16,180
looking at the utilization, the utilization was like around 20 or 30 percent.

1028
01:52:17,940 --> 01:52:22,020
How can we try to increase that or why is the utilization low, I would say?

1029
01:52:23,540 --> 01:52:31,700
I think the utilization is low mainly telling us that the GPU is waiting for CPU doing stuff.

1030
01:52:32,180 --> 01:52:36,860
or sometimes it's the input to output things going on,

1031
01:52:36,860 --> 01:52:38,540
and the GPU is waiting for that.

1032
01:52:38,540 --> 01:52:44,780
But in our case, I think it's mainly GPU waiting for CPU to do stuff.

1033
01:52:44,780 --> 01:52:47,420
But our example is quite small.

1034
01:52:47,420 --> 01:52:51,780
We don't have very large input data,

1035
01:52:51,780 --> 01:52:54,460
and also we requested,

1036
01:52:54,460 --> 01:52:58,500
I think we do some quantization of the model.

1037
01:52:58,500 --> 01:53:05,180
Yes, we did. We used this to do some quantization.

1038
01:53:05,180 --> 01:53:09,180
It will also make the computation lower,

1039
01:53:09,180 --> 01:53:11,060
and the usage lower.

1040
01:53:11,060 --> 01:53:14,020
But if you are running something very big,

1041
01:53:14,020 --> 01:53:18,540
you would definitely need to optimize the usage of GPUs.

1042
01:53:18,540 --> 01:53:20,220
But don't worry about that.

1043
01:53:20,220 --> 01:53:23,780
It's a bit complex or tricky sometimes,

1044
01:53:23,780 --> 01:53:28,740
so don't hesitate to talk to us.

1045
01:53:28,740 --> 01:53:32,740
What essentially it means that load utilization,

1046
01:53:32,740 --> 01:53:38,140
it means that your GPU can do much more computation,

1047
01:53:38,140 --> 01:53:44,500
and because we are only answering simple questions here in this example,

1048
01:53:44,500 --> 01:53:46,500
it means there are not enough data

1049
01:53:46,500 --> 01:53:49,180
or not enough computation for the GPU to be done.

1050
01:53:49,180 --> 01:53:50,920
If you have load utilization,

1051
01:53:50,920 --> 01:53:56,120
And one of the things that you can try is like feed it more data or more complex data to a more complex

1052
01:53:58,120 --> 01:54:03,000
model in this case. And as you mentioned, because the model is not complex, it only

1053
01:54:03,000 --> 01:54:10,120
has eight bits of each parameter. It means every calculation is like done very fast.

1054
01:54:11,160 --> 01:54:17,480
So yeah, there are like some techniques to make that GPU more useful or like make it more work.

1055
01:54:17,480 --> 01:54:25,160
But yeah, it can be very complicated, so come and ask us and we will try to help you with that.

1056
01:54:28,600 --> 01:54:33,560
Yes. Yeah, so anything else that you want to add here?

1057
01:54:36,600 --> 01:54:42,440
I don't have too much from my side. I mean, we only give a very simple demo. There are

1058
01:54:42,440 --> 01:54:51,960
too many the other details. Yeah, unfortunately we don't have enough time for the exercise today,

1059
01:54:53,080 --> 01:55:04,120
but we will be there. So what I would recommend is to try to run the LLM example by yourself.

1060
01:55:04,120 --> 01:55:09,160
I would look at the example again and see if we can find the error that the user was

1061
01:55:09,160 --> 01:55:14,560
facing and would reply in the in the notes but try to rerun the example and

1062
01:55:14,560 --> 01:55:19,020
see if you are facing an error and if you can do that of course you can do

1063
01:55:19,020 --> 01:55:23,920
like much more you can like request multiple GPUs with like larger amount of

1064
01:55:23,920 --> 01:55:28,400
like larger model so to divide the model and everything but it's more complicated

1065
01:55:28,400 --> 01:55:33,560
it doesn't fit in that 30 minute session so yeah if you want to learn more or if

1066
01:55:33,560 --> 01:55:39,000
you have any questions come and ask us and yeah I think it was good and we are

1067
01:55:39,000 --> 01:55:41,920
are all right on time.

1068
01:55:41,920 --> 01:55:43,280
So what is next?

1069
01:55:43,280 --> 01:55:45,760
I guess we have a, no, we don't have a short break.

1070
01:55:45,760 --> 01:55:49,600
So we have the wrap-up and summary.

1071
01:55:49,600 --> 01:55:55,480
So maybe [name], again.

1072
01:55:55,480 --> 01:55:57,480
Now you can hear me.

1073
01:55:57,480 --> 01:56:01,200
Yeah, I'll switch to the notes.

1074
01:56:01,200 --> 01:56:04,560
And so as usual, at the very bottom,

1075
01:56:04,560 --> 01:56:07,120
there's the feedback of the day.

1076
01:56:07,120 --> 01:56:09,880
And not just the day, I guess, this

1077
01:56:09,880 --> 01:56:11,600
should be for the whole course.

1078
01:56:16,160 --> 01:56:17,960
Do we have two feedbacks, one for the day

1079
01:56:17,960 --> 01:56:19,360
and one for the course?

1080
01:56:22,320 --> 01:56:25,120
If someone wants to duplicate it twice, you can.

1081
01:56:25,120 --> 01:56:27,720
But usually it's just one.

1082
01:56:27,720 --> 01:56:29,920
No, let's go with one.

1083
01:56:29,920 --> 01:56:31,720
At least for this part.

1084
01:56:31,720 --> 01:56:35,640
But then now for general Q&A. So now's

1085
01:56:35,640 --> 01:56:39,840
the time when we can all be here,

1086
01:56:39,840 --> 01:56:43,400
and we'll just reflect on the course itself.

1087
01:56:46,360 --> 01:56:48,480
Yeah, so who remembers at the beginning

1088
01:56:48,480 --> 01:56:52,600
when we said that it was impossible to teach everything

1089
01:56:52,600 --> 01:57:00,520
that was needed in a, what's it called, in a 10-hour course?

1090
01:57:00,520 --> 01:57:02,560
HPC system?

1091
01:57:02,560 --> 01:57:15,200
Yeah, so I still think that's true. I mean, here in this course, we've only had time to give you

1092
01:57:15,200 --> 01:57:22,160
a summary, do some examples, and so on. But by no means do we think that this course is the end of

1093
01:57:22,160 --> 01:57:30,240
what you need. It's just a starting point. It basically, hopefully, lets you realize what's

1094
01:57:30,240 --> 01:57:44,640
possible and have had the initial experience with stuff. And now, you will know when and

1095
01:57:44,640 --> 01:57:54,080
how to go read for what comes next. What do the other instructors think? Was it?

1096
01:57:54,080 --> 01:58:05,240
Yeah, I think, I don't remember, when was the first time I was doing this course back

1097
01:58:05,240 --> 01:58:14,920
in 2016 or something, and the ecosystem has changed dramatically from those times.

1098
01:58:14,920 --> 01:58:21,280
Don't be afraid of thinking that, okay, this is a lot of stuff happening, because it's

1099
01:58:21,280 --> 01:58:25,700
There's a lot of stuff happening for everybody.

1100
01:58:25,700 --> 01:58:32,720
Back then, it was basically like we wouldn't have even, let's say, the GPU day at all basically

1101
01:58:32,720 --> 01:58:38,960
because the GPUs weren't being utilized, but not in this sort of a level.

1102
01:58:38,960 --> 01:58:47,160
Many of the concepts like AI and whatever, they weren't things that are happening constantly.

1103
01:58:47,160 --> 01:58:53,240
a lot of stuff currently happening in the space, like the computational space is getting

1104
01:58:54,280 --> 01:59:00,360
very complex. But at the same time, I would say that it's good to differentiate.

1105
01:59:02,840 --> 01:59:09,000
If you have certain things that you need and you want, it's good to know that, okay,

1106
01:59:09,000 --> 01:59:17,080
I don't need to necessarily know everything about everything. You can focus on learning

1107
01:59:17,080 --> 01:59:21,560
those things that you need for your work and then learning the rest later.

1108
01:59:31,160 --> 01:59:34,440
Yeah, so some of these questions. The first question is about

1109
01:59:34,440 --> 01:59:38,280
COMSOL and how to get started. What would you all think?

1110
01:59:40,440 --> 01:59:45,800
Depends on your time limitations. I would suggest coming in on one of the COMSOL focus

1111
01:59:45,800 --> 01:59:51,240
days that we have in Garage, because then there are plenty of experts here that have a lot of

1112
01:59:51,240 --> 01:59:55,880
knowledge of Comsol and also a lot of detailed knowledge on how Comsol exactly works.

1113
01:59:56,680 --> 02:00:03,400
While if you come in on another day, we do have a few people that do know about CompSort,

1114
02:00:03,960 --> 02:00:11,000
but you might be unlucky and, well, not get the right person on that day.

1115
02:00:11,000 --> 02:00:18,120
But it doesn't hurt to show up any other day, and you're welcome to, but it's just possible that

1116
02:00:18,840 --> 02:00:24,120
we might not be able to help you on the spot. And I would say that Comsol is one of

1117
02:00:24,120 --> 02:00:29,240
these kinds of programs that once you get it sorted out and it works, it works.

1118
02:00:29,240 --> 02:00:34,760
It's the kind of a program that, because it has this, you can submit it straight from

1119
02:00:35,720 --> 02:00:39,480
the graphical user interface and then it just runs stuff on the background and you don't need

1120
02:00:39,480 --> 02:00:45,400
to think about it. But to get it working, that's the problem. And that's usually like,

1121
02:00:46,920 --> 02:00:54,840
yeah, it's a good idea to ask for help because it's not fun doing it by yourself based on

1122
02:00:56,920 --> 02:01:05,400
just looking at the manuals. Yeah. And it's the kind of thing that you'll always find people

1123
02:01:05,400 --> 02:01:11,640
that know about the cluster itself but might not know all the buttons Comsol. So if you can get

1124
02:01:11,640 --> 02:01:15,240
it most of the way there and show us an error message that comes from the cluster,

1125
02:01:15,960 --> 02:01:21,480
you can probably get help almost any day. But if it's not that, then no.

1126
02:01:24,120 --> 02:01:33,000
Next question is also pretty good. So a custom library which is not written for Triton,

1127
02:01:33,000 --> 02:01:35,720
and not available, and the default instructions

1128
02:01:35,720 --> 02:01:37,440
don't work on Triton.

1129
02:01:37,440 --> 02:01:41,000
So yeah, what would people do there?

1130
02:01:45,080 --> 02:01:49,200
I guess, what's the name of the library?

1131
02:01:49,200 --> 02:01:51,840
Like, is there a public link or something?

1132
02:01:51,840 --> 02:01:53,920
Is it a common library?

1133
02:01:53,920 --> 02:01:56,720
So if it's.

1134
02:01:56,720 --> 02:02:00,600
So I would say, in general, I would say, yeah,

1135
02:02:00,600 --> 02:02:03,920
come to Garage and let us have a look at it together.

1136
02:02:06,560 --> 02:02:10,960
I would say it really depends on what kind of library it is.

1137
02:02:10,960 --> 02:02:13,480
If this is something that is, for example,

1138
02:02:15,280 --> 02:02:18,600
or is a Python library, then you can normally

1139
02:02:18,600 --> 02:02:22,960
just do it yourself if you know enough about Python

1140
02:02:22,960 --> 02:02:24,800
so that you can just run the code

1141
02:02:24,800 --> 02:02:28,160
from a customally installed library.

1142
02:02:28,160 --> 02:02:30,880
If you don't, also, come to Garage.

1143
02:02:30,880 --> 02:02:33,080
We can try to help you there.

1144
02:02:33,080 --> 02:02:36,400
And if there are no really strange things happening,

1145
02:02:36,400 --> 02:02:38,160
normally we get the things running.

1146
02:02:40,640 --> 02:02:41,960
But yeah.

1147
02:02:41,960 --> 02:02:45,080
Yeah, quite often, especially for scientific coding

1148
02:02:45,080 --> 02:02:50,080
and scientific programs, for the whole ecosystem,

1149
02:02:50,160 --> 02:02:52,320
like the Linux ecosystem and stuff like that,

1150
02:02:52,320 --> 02:02:56,280
like the GNU ecosystem and everything related to that,

1151
02:02:56,280 --> 02:03:04,320
There's standards that are very engineer focused, but they follow them very well.

1152
02:03:04,320 --> 02:03:09,680
So they have, usually if you have a code, you have certain configurations or configure

1153
02:03:09,680 --> 02:03:14,800
flags or whatever, and you can spot the pattern and use that.

1154
02:03:14,800 --> 02:03:18,880
But the unfortunate thing is that because it's so engineering focused is that I remember

1155
02:03:18,880 --> 02:03:28,880
one CMake manual, a tutorial from those people was like 500 pages. So it was insanely long,

1156
02:03:28,880 --> 02:03:33,520
like all of the tutorials, but it's very strict and very regimented and everything is where it's

1157
02:03:33,520 --> 02:03:40,320
supposed to be. But when it comes to scientific programs, nobody has time to read those 500 pages.

1158
02:03:40,320 --> 02:03:45,680
So you might end up that the scientific program, it doesn't follow those standards completely.

1159
02:03:45,680 --> 02:03:50,480
So it becomes kind of like a translation problem where you have like, okay, this is what the

1160
02:03:51,840 --> 02:03:59,360
creators of the program intended to be done, but they use different names for the flags or

1161
02:03:59,360 --> 02:04:04,240
whatever. And now suddenly you need to do this kind of translation. Okay, what did they, like

1162
02:04:04,240 --> 02:04:10,560
this XY problem kind of situation, what did they want to do and what things that the installation

1163
02:04:10,560 --> 02:04:17,840
thing need and it becomes this kind of guessing game. You need to go through the source usually

1164
02:04:17,840 --> 02:04:25,840
and check what it actually tries to do. And sometimes getting outside feedback on this,

1165
02:04:25,840 --> 02:04:33,520
it can be really helpful. Like somebody else who has done enough of the pattern recognition

1166
02:04:33,520 --> 02:04:41,120
to see that, okay, like, hey, they're probably trying to do this kind of thing and they mean

1167
02:04:41,120 --> 02:04:47,920
these and these things. And sometimes that can happen with many libraries.

1168
02:04:47,920 --> 02:04:53,440
Or even just knowing where in the error message you actually have to look for what the actual

1169
02:04:53,440 --> 02:05:00,600
error message is, because you get like three pages of outputs. And if you don't, or if

1170
02:05:00,600 --> 02:05:07,560
haven't done this before. Okay, this is so much stuff. I don't even know where to look

1171
02:05:07,560 --> 02:05:14,580
for things. And there's a thousand different warnings, errors and stuff. And yeah, we kind

1172
02:05:14,580 --> 02:05:21,520
of have seen these things and we know which things are important and need to be considered

1173
02:05:21,520 --> 02:05:26,320
and which ones are, yeah, this is just standard output that it always prints.

1174
02:05:26,320 --> 02:05:34,160
But I would also say that I wouldn't discourage you to try checking out the pattern recognition

1175
02:05:34,160 --> 02:05:35,160
yourself.

1176
02:05:35,160 --> 02:05:39,960
But the problem, the important thing to realize is that the problem often necessarily isn't

1177
02:05:39,960 --> 02:05:40,960
in you.

1178
02:05:40,960 --> 02:05:46,680
It might be that somebody misheard something five years ago and now it's in the code.

1179
02:05:46,680 --> 02:05:53,760
Now it's like this game of telephone that happens with programs and sometimes somebody

1180
02:05:53,760 --> 02:05:57,580
doesn't follow a standard or something, and then it gets baked into the code and everybody

1181
02:05:57,580 --> 02:06:01,640
uses the code already, and now it's like a standard in the code, but you need to know

1182
02:06:01,640 --> 02:06:02,640
about it.

1183
02:06:02,640 --> 02:06:08,840
And it's like hidden in a readme five pages deep, and it's not necessarily your fault

1184
02:06:08,840 --> 02:06:12,600
that you didn't recognize the pattern, or you didn't know about this, or you didn't

1185
02:06:12,600 --> 02:06:13,880
know about the thing.

1186
02:06:13,880 --> 02:06:22,180
But what you can try, like at least I do, is that if I see something, I'll usually go

1187
02:06:22,180 --> 02:06:27,060
back to the source and check, like, okay, how is this supposed to be written?

1188
02:06:27,060 --> 02:06:33,480
Like, let's say, like, how are they expected these flags to be given?

1189
02:06:33,480 --> 02:06:37,780
And why aren't they given like they are and what flags they want to give them and that

1190
02:06:37,780 --> 02:06:38,780
sort of stuff.

1191
02:06:38,780 --> 02:06:45,740
But it becomes this kind of like, it's very hard to say, how do you recognize these things?

1192
02:06:45,740 --> 02:06:50,380
Like what [name] said about the error messages, it's very hard to say how you spot the right

1193
02:06:50,380 --> 02:06:53,460
line in the error message.

1194
02:06:53,460 --> 02:07:00,180
It only comes through time, but it's not the problem that you're wrong because you didn't

1195
02:07:00,180 --> 02:07:01,180
understand it.

1196
02:07:01,180 --> 02:07:06,820
The message always can be bad, like the error message can always be bad, but you just plow

1197
02:07:06,820 --> 02:07:07,820
through it.

1198
02:07:07,820 --> 02:07:11,940
At a certain point, you just know to look for a certain place after you encountered

1199
02:07:11,940 --> 02:07:14,420
it more and more times.

1200
02:07:14,420 --> 02:07:17,540
It's like, unfortunately, it's sometimes like that.

1201
02:07:17,540 --> 02:07:20,340
But so I would encourage everybody

1202
02:07:20,340 --> 02:07:22,220
to just also try it out.

1203
02:07:22,220 --> 02:07:27,980
Like, just try reading it and try googling as well.

1204
02:07:27,980 --> 02:07:30,100
Because after a certain time, you

1205
02:07:30,100 --> 02:07:34,340
get accustomed to seeing those patterns.

1206
02:07:34,340 --> 02:07:38,180
And just where you mentioned this

1207
02:07:38,180 --> 02:07:40,420
has become standard in the code, or there is

1208
02:07:40,420 --> 02:07:45,060
something that is happening.

1209
02:07:45,060 --> 02:07:48,320
It might also be good if this is a project that

1210
02:07:48,320 --> 02:07:52,420
is hosted on GitHub to just look for something

1211
02:07:52,420 --> 02:07:54,980
similar or something where you think this could

1212
02:07:54,980 --> 02:07:58,340
be related in the issues.

1213
02:07:58,340 --> 02:08:01,540
Because often enough, there is an issue from five years back

1214
02:08:01,540 --> 02:08:05,780
that, why the heck is this the default?

1215
02:08:05,780 --> 02:08:10,020
This is completely unintuitive and not documented anywhere.

1216
02:08:10,020 --> 02:08:13,540
And could this place be changed?

1217
02:08:13,540 --> 02:08:15,640
And yeah, it just never was.

1218
02:08:20,420 --> 02:08:22,300
Yeah, and I have a personal example for that

1219
02:08:22,300 --> 02:08:23,800
just from a Microsoft library.

1220
02:08:27,460 --> 02:08:28,300
Yeah.

1221
02:08:29,900 --> 02:08:32,060
And my comment on this question,

1222
02:08:32,060 --> 02:08:34,420
when I see there's an install.py file,

1223
02:08:34,420 --> 02:08:39,120
that's not the standard name of a package project thing.

1224
02:08:39,120 --> 02:08:42,400
So if it was setup.py or pyproject.toml,

1225
02:08:42,400 --> 02:08:44,520
I'd think, OK, most likely, I can

1226
02:08:44,520 --> 02:08:46,960
make a virtual environment or content environment

1227
02:08:46,960 --> 02:08:48,000
and run a command.

1228
02:08:48,000 --> 02:08:51,760
And it will install it and mostly do all the stuff

1229
02:08:51,760 --> 02:08:52,400
automatically.

1230
02:08:52,400 --> 02:08:54,360
I might need to tune some, but not.

1231
02:08:54,360 --> 02:08:56,520
But install.py makes me think someone

1232
02:08:56,520 --> 02:08:59,460
has written a custom script that tries to install it.

1233
02:08:59,460 --> 02:09:01,160
Who knows where it installs?

1234
02:09:01,160 --> 02:09:03,960
And if I need to do it myself, it's

1235
02:09:03,960 --> 02:09:05,720
probably going to be a lot of manual work

1236
02:09:05,720 --> 02:09:08,280
in figuring out how they did it.

1237
02:09:08,280 --> 02:09:13,720
Of course, if you're using this, there's nothing you can do now, but this is a lesson for the

1238
02:09:13,720 --> 02:09:20,440
future. When you're releasing something, it's worth taking a little bit of time to understand

1239
02:09:20,440 --> 02:09:26,360
how the project, how Python stuff is supposed to be packaged, and so on, and then do that if you

1240
02:09:26,360 --> 02:09:31,960
want people to use it. We actually have other courses in the Python for Scientific Computing

1241
02:09:31,960 --> 02:09:37,480
course that's usually in the autumns. There's a session where we very briefly talk about the

1242
02:09:37,480 --> 02:09:48,040
packaging standards and it's not that hard. And I want to add on that, also to consider

1243
02:09:48,040 --> 02:09:54,840
what should be happening in an installation script in Python and what should not. Like

1244
02:09:54,840 --> 02:10:04,680
things where Python assumes that underlying system libraries are installed. So your installation

1245
02:10:04,680 --> 02:10:09,960
script should not try to install system libraries, because that is always operating system dependent

1246
02:10:09,960 --> 02:10:17,640
and Python says, no, I don't want to do this. This is something that the user has to take care of

1247
02:10:17,640 --> 02:10:24,440
in a different way. Conda, for example, does a lot of wrapping around that and adds

1248
02:10:24,440 --> 02:10:32,280
some things that are system libraries in the environments, but don't try to do this in a

1249
02:10:32,280 --> 02:10:40,160
the custom installation script, you will just most likely make it extremely complicated

1250
02:10:40,160 --> 02:10:43,040
for someone else to actually install it.

1251
02:10:43,040 --> 02:10:44,040
Yeah.

1252
02:10:44,040 --> 02:10:45,040
Yeah.

1253
02:10:45,040 --> 02:10:46,040
Yeah.

1254
02:10:46,040 --> 02:10:53,200
But like, like, again, like, yeah, I will quickly mention that the, like, again, like,

1255
02:10:53,200 --> 02:10:58,520
but it's often, like, also high burden, like, to, like, think that, okay, this needs to

1256
02:10:58,520 --> 02:11:06,440
be perfect and whatever first time. If you want to do something and you want to publish it,

1257
02:11:06,440 --> 02:11:14,600
don't worry if it's bad the first time. It's the continuous process of improvement that matters

1258
02:11:14,600 --> 02:11:23,480
more. I have published code that is really bad, and I have made repos that I still look back and

1259
02:11:23,480 --> 02:11:29,080
think that these are bad, but they were bad because I didn't know better at that time.

1260
02:11:30,360 --> 02:11:36,120
You will go through the process and that will happen for everybody. Everybody will publish

1261
02:11:36,120 --> 02:11:41,800
code that is not using the standards. It's missing something that you realize later that,

1262
02:11:41,800 --> 02:11:48,520
hey, I should have used this and that. Don't put too high of a burden on yourself.

1263
02:11:48,520 --> 02:11:52,760
But in the next iteration, know that, OK, maybe I

1264
02:11:52,760 --> 02:11:54,840
shouldn't use the previous one.

1265
02:11:54,840 --> 02:11:58,800
Maybe I should try improving it in the next iteration.

1266
02:12:01,440 --> 02:12:02,920
Yeah.

1267
02:12:02,920 --> 02:12:04,680
Yeah, I sort of say something similar.

1268
02:12:04,680 --> 02:12:09,000
So you don't have to be perfect, because perfect is always

1269
02:12:09,000 --> 02:12:10,040
too far away.

1270
02:12:10,040 --> 02:12:11,760
Just try to do a little bit better

1271
02:12:11,760 --> 02:12:13,400
each time you do something.

1272
02:12:13,400 --> 02:12:18,200
And then once you, like, if you do something often,

1273
02:12:18,200 --> 02:12:23,200
you'll get better at it and reach the level you need.

1274
02:12:23,200 --> 02:12:27,800
So what, I mean, OK, so for the feedback parts here,

1275
02:12:27,800 --> 02:12:31,040
it looks pretty similar to other days.

1276
02:12:36,120 --> 02:12:40,000
Yeah, so what should we recommend people do next?

1277
02:12:40,000 --> 02:12:43,160
So what we covered was in the schedule.

1278
02:12:43,160 --> 02:12:46,600
There's a lot more written material to review.

1279
02:12:48,200 --> 02:13:04,040
Yeah, like at least I would recommend just trying out whatever you are planning on working

1280
02:13:04,040 --> 02:13:05,040
on.

1281
02:13:05,040 --> 02:13:10,800
Like it's like if you have a bunch of cookbooks in your bookshelf, but you never go to the

1282
02:13:10,800 --> 02:13:14,600
kitchen, you will never learn how to do like cook food.

1283
02:13:14,600 --> 02:13:21,300
Like you really need to go to the place and start cooking and tasting the food for yourself.

1284
02:13:21,300 --> 02:13:24,960
And then of course, use the cookbooks as this kind of like reference point that, okay, it

1285
02:13:24,960 --> 02:13:29,860
should look like this, but why does it, why my food doesn't look like this?

1286
02:13:29,860 --> 02:13:35,640
And then you're like, you can do the improvement procedure, but like, I would highly recommend

1287
02:13:35,640 --> 02:13:40,320
that you pick a program or a thing that you want to do in the cluster.

1288
02:13:40,320 --> 02:13:47,760
And first, let's say you have a program that you run currently on your laptop, and let's

1289
02:13:47,760 --> 02:13:52,960
say it heats up your laptop and you can't have it in your lab anymore, it's too hot

1290
02:13:52,960 --> 02:13:55,680
because it's constantly running some program.

1291
02:13:55,680 --> 02:14:00,680
Can you move that to the cluster and what does it require from you to do that?

1292
02:14:00,680 --> 02:14:05,640
And try out, would it be possible that it heats up the computer in a machine room somewhere

1293
02:14:05,640 --> 02:14:09,720
so that your computer isn't overheating?

1294
02:14:09,720 --> 02:14:14,520
So try some program, move it into the cluster,

1295
02:14:14,520 --> 02:14:17,680
and get accustomed to the workflow

1296
02:14:17,680 --> 02:14:22,800
that you basically put something running somewhere else,

1297
02:14:22,800 --> 02:14:25,440
and you don't have to worry about it anymore.

1298
02:14:28,800 --> 02:14:33,040
Yeah, that's a good way to do it.

1299
02:14:33,040 --> 02:14:38,480
What courses would you recommend people to follow up next with?

1300
02:14:39,720 --> 02:14:56,160
Well, on the autumn, we have the Python for Scientific Computing coming up again, like

1301
02:14:56,160 --> 02:14:57,160
that's our course.

1302
02:14:57,160 --> 02:15:00,800
And there's also the Code Refinery course coming at some point.

1303
02:15:00,800 --> 02:15:04,200
They are good ones, of course.

1304
02:15:04,200 --> 02:15:10,280
I think Code Refinery is mid-September now.

1305
02:15:10,280 --> 02:15:15,360
So I guess you'll get links to this in your email

1306
02:15:15,360 --> 02:15:18,080
if you've registered.

1307
02:15:18,080 --> 02:15:20,200
And Code Refinery and Python for SciComp

1308
02:15:20,200 --> 02:15:22,520
are a lot of these same kind of instructors

1309
02:15:22,520 --> 02:15:24,640
and the same teaching style, where

1310
02:15:24,640 --> 02:15:27,520
it's live stream with the notes to ask stuff

1311
02:15:27,520 --> 02:15:32,040
and lots of exercises and so on.

1312
02:15:32,040 --> 02:15:37,640
Yeah, the focus is more on the workflows and how to use Git and that sort of stuff.

1313
02:15:39,720 --> 02:15:41,880
Improving your productivity, basically.

1314
02:15:43,080 --> 02:15:43,580
Yeah.

1315
02:15:45,640 --> 02:15:47,880
Should we discuss any feedback?

1316
02:15:52,920 --> 02:15:53,420
Yeah.

1317
02:15:54,920 --> 02:15:57,480
I think the feedback is good and honest.

1318
02:15:57,480 --> 02:16:05,080
And it's always complicated with this course, and especially with the time frame, how to

1319
02:16:06,840 --> 02:16:09,000
get everything into that.

1320
02:16:12,040 --> 02:16:19,080
Yeah. And there's this thing we struggle with for every course. So if you make it

1321
02:16:19,080 --> 02:16:27,960
advanced enough to be useful, then it excludes most of the people that could benefit from it,

1322
02:16:27,960 --> 02:16:34,760
and is basically saying, do all these other more basic courses before you can get to what you need.

1323
02:16:34,760 --> 02:16:40,760
But with today's academics and so on, like the academic system and the time pressures people

1324
02:16:40,760 --> 02:16:48,520
have, people just don't have time to do a ton of prerequisite courses before they start getting

1325
02:16:48,520 --> 02:16:56,840
stuff done. And that's why we emphasize the big courses that sort of set the basic level,

1326
02:16:56,840 --> 02:17:03,880
ways for people to study what they need themselves, and then also our help so we can

1327
02:17:03,880 --> 02:17:11,880
help you with the specific parts that you need help for. And, you know, sort of like one-on-one

1328
02:17:11,880 --> 02:17:23,560
working together. So time's up. Are there any final things to say? So I propose

1329
02:17:23,560 --> 02:17:32,500
that we paste in the student, the Zoom room here, and then all of us instructors

1330
02:17:32,500 --> 02:17:40,260
and anyone who wants can join there for a live, well this is live, a meeting style

1331
02:17:40,260 --> 02:17:46,500
feedback and you can briefly talk with us and tell us what went well and not well and so on.

1332
02:17:47,860 --> 02:17:49,140
Would anyone want to do that?

1333
02:17:52,340 --> 02:17:55,300
Well, I guess we can paste the link here and see.

1334
02:17:59,380 --> 02:17:59,940
Let's see.

1335
02:17:59,940 --> 02:18:13,740
see. So actually, I don't have the right Zoom link. Can someone put it here?

1336
02:18:13,740 --> 02:18:21,100
Oh, this Zoom link?

1337
02:18:21,100 --> 02:18:29,900
Not the broadcast studio, but the student Zoom room, which at least [name] has, I think.

1338
02:18:29,900 --> 02:18:36,340
Wait, for this one?

1339
02:18:36,340 --> 02:18:38,460
No, the student one.

1340
02:18:38,460 --> 02:18:39,460
The student one.

1341
02:18:39,460 --> 02:18:40,460
I have it now.

1342
02:18:40,460 --> 02:18:41,460
I have it.

1343
02:18:41,460 --> 02:18:42,460
You have it.

1344
02:18:42,460 --> 02:18:43,460
Okay.

1345
02:18:43,460 --> 02:18:44,460
And where to post?

1346
02:18:44,460 --> 02:18:46,900
In the notes in the News for Day 3.

1347
02:18:46,900 --> 02:18:47,900
Okay.

1348
02:18:47,900 --> 02:18:48,900
But if.

1349
02:18:48,900 --> 02:18:49,900
Yeah.

1350
02:18:49,900 --> 02:18:50,900
[name], are you going to post it or?

1351
02:18:50,900 --> 02:18:51,900
No.

1352
02:18:51,900 --> 02:18:52,900
You can put it.

1353
02:18:52,900 --> 02:18:53,900
Yeah.

1354
02:18:53,900 --> 02:18:54,900
Okay.

1355
02:18:54,900 --> 02:18:55,900
But anyway, let's wrap up now.

1356
02:18:55,900 --> 02:18:56,900
So, thanks for attending.

1357
02:18:56,900 --> 02:19:01,300
Um, we, at least I enjoyed all the feedback.

1358
02:19:01,300 --> 02:19:05,100
It's nice to have all these chats going on at the same time.

1359
02:19:05,100 --> 02:19:11,580
And I think we all hope that you have the document now.

1360
02:19:11,580 --> 02:19:13,060
OK.

1361
02:19:13,060 --> 02:19:15,980
And I'll be heading over there.

1362
02:19:15,980 --> 02:19:20,140
We all, that you all have a good career in computing,

1363
02:19:20,140 --> 02:19:24,500
however much you would like.

1364
02:19:24,500 --> 02:19:26,860
More or less, everyone will be unique.

1365
02:19:26,860 --> 02:19:29,500
OK, bye then.

1366
02:19:29,500 --> 02:19:30,340
Bye.

1367
02:19:30,340 --> 02:19:31,900
Bye-bye.

1368
02:19:56,860 --> 02:19:58,920
you

1369
02:20:26,860 --> 02:20:28,920
you

1370
02:20:56,860 --> 02:20:58,920
you

1371
02:21:26,860 --> 02:21:28,920
you

