1
00:00:00,000 --> 00:00:00,920
you will hear later about this.

2
00:00:00,920 --> 00:00:05,600
So to have a little break and consider if what you're doing is worth doing and I don't

3
00:00:05,600 --> 00:00:12,600
want to, you know, depress anyone here, it's good to also consider and be aware of the

4
00:00:12,600 --> 00:00:18,960
responsible conduct of research and specifically in the case of computation one could talk

5
00:00:18,960 --> 00:00:26,800
about responsible computational research and you can write if you want in the notes document

6
00:00:26,800 --> 00:00:32,600
what type of ethical or moral or legal issues that you might not even know that you have

7
00:00:32,600 --> 00:00:37,080
or maybe you think that you have but they're not issues at all but in general what is making

8
00:00:37,080 --> 00:00:43,120
you doubtful about your research. I wrote there, for example, that reproducibility,

9
00:00:43,120 --> 00:00:51,320
which we have mentioned many times, is something that kind of I feel that often in my past

10
00:00:51,320 --> 00:00:57,920
as a researcher that I had issues of reproducibility. What about you, [name]? What is your kind

11
00:00:57,920 --> 00:00:58,920
of...

12
00:00:58,920 --> 00:01:06,320
I completely agree. Either other people's stuff that I wanted to replicate or my own

13
00:01:06,320 --> 00:01:12,160
stuff. In both cases, I had issues with getting things to run again.

14
00:01:12,160 --> 00:01:16,920
Yeah. And you actually mentioned really well my own stuff, because one could think that

15
00:01:16,920 --> 00:01:22,120
reproducibility is something, you know, I'm trying to figure out the universal law of

16
00:01:22,120 --> 00:01:27,800
whatever. And, you know, it's good that other people repeat my experiment. But sometimes

17
00:01:27,800 --> 00:01:32,360
the issue is really that I was doing six months ago something, and I was able to obtain some

18
00:01:32,360 --> 00:01:37,560
results and I'm not able to rerun the same code anymore. Was it that the library has changed?

19
00:01:37,560 --> 00:01:44,520
The operating system has changed? So this is very common. I've seen it many times with my work,

20
00:01:44,520 --> 00:01:51,000
with other people's work. So a few of the topics that I will mention now, they cover or they show

21
00:01:51,000 --> 00:01:56,440
you ways on how to deal with reproducibility issues. And you kind of already got a glimpse

22
00:01:56,440 --> 00:02:01,400
of this when earlier we talked about Conda and the Conda environments and now someone was just

23
00:02:01,400 --> 00:02:06,600
asking about Python 2 which is like traveling back in time and trying to reproduce or replicate

24
00:02:06,600 --> 00:02:14,920
something from the past. So it's impossible to cover everything exhaustively when it comes to

25
00:02:15,800 --> 00:02:20,920
research ethics and responsible conduct of research so I will try to give you a very

26
00:02:20,920 --> 00:02:28,680
short five minutes intro but this page has lots of links that ideally every researcher should check

27
00:02:28,680 --> 00:02:35,800
and read and be aware. The kind of conceptual framework that you can think of is the so-called

28
00:02:35,800 --> 00:02:43,080
normative cascade which is how basically society works. You can understand that or you maybe feel

29
00:02:43,080 --> 00:02:49,720
that there are these general ethics principles that can drive basically the creation of laws.

30
00:02:49,720 --> 00:02:56,120
In the ideal case, ethics, all the good ethical principles would overlap completely with the laws

31
00:02:56,120 --> 00:03:01,640
but in practice it is not like that, so there is never a full overlap between ethics and law.

32
00:03:02,360 --> 00:03:09,880
And then of course laws will cause the creation or will influence so-called research policies,

33
00:03:09,880 --> 00:03:16,360
so how the legislations stemming from the ethical principle will actually be applied in the work you

34
00:03:16,360 --> 00:03:21,160
need to do with research. And then at the end of course it's the actual researchers that are

35
00:03:21,160 --> 00:03:26,320
affected by the cascade. On the other hand, if the researchers notice that there are new

36
00:03:26,320 --> 00:03:33,400
issues that the core ethical principle needs to be revised, they can together influence

37
00:03:33,400 --> 00:03:38,960
the cascade and go back at the beginning and change the ethical principles.

38
00:03:38,960 --> 00:03:43,440
So if we talk about the ethical principle, I'm not going to talk about as many ethical

39
00:03:43,440 --> 00:03:49,180
principles. The Universal Declaration of Human Rights is one example. But in the context

40
00:03:49,180 --> 00:03:54,520
of Research, I hope that everyone here, if you are in Europe, you should all read this

41
00:03:54,520 --> 00:04:00,380
very short book which is the ALLEA European Code of Conduct for Research Integrity. Basically

42
00:04:00,380 --> 00:04:04,760
there's everything there. If you know that book by heart, you just need to apply that

43
00:04:04,760 --> 00:04:11,180
book whether you do computational science, I don't know, artistic research or whatever

44
00:04:11,180 --> 00:04:17,500
type of research activities that you are doing. The four principles of the ALLEA Code of Conduct

45
00:04:17,500 --> 00:04:25,020
are reliability, ensuring basically the quality of your of your research. Honesty, that what you

46
00:04:25,020 --> 00:04:30,620
develop and what you do can be basically audited by others. So again, we're kind of covering this

47
00:04:30,620 --> 00:04:36,300
type of reproducibility and transparency. And of course, respect is very important,

48
00:04:36,300 --> 00:04:41,820
respect towards your colleagues, respect towards the administrator of your cluster, respect towards

49
00:04:41,820 --> 00:04:48,660
the people that you will encounter on your career, others, authors in other universities

50
00:04:48,660 --> 00:04:53,740
and more in general respects towards society and the ecosystem and so on.

51
00:04:53,740 --> 00:05:01,100
And finally, accountability, given that, you know, from the accountability for the research

52
00:05:01,100 --> 00:05:07,020
from the idea stage, the publication stage, and all its type of management and organisation,

53
00:05:07,020 --> 00:05:12,580
which also requires you know this type of relationship between supervisor and mentors

54
00:05:12,580 --> 00:05:20,140
and let's say junior researchers who are supervised and so on.

55
00:05:20,140 --> 00:05:26,060
So that is kind of the general level and then from this core ethical principle there are

56
00:05:26,060 --> 00:05:32,140
many legislations that apply to research where I will not go into the detail of any of these

57
00:05:32,140 --> 00:05:35,740
But the typical case, maybe I can ask this actually to [name],

58
00:05:35,740 --> 00:05:38,660
have you ever experienced in your work

59
00:05:38,660 --> 00:05:41,020
that some researcher thinks

60
00:05:41,020 --> 00:05:43,380
that they're not working with personal data,

61
00:05:43,380 --> 00:05:46,180
but actually they were processing personal data?

62
00:05:49,300 --> 00:05:50,620
Almost the other way around,

63
00:05:50,620 --> 00:05:52,540
is there anything that's not personal data?

64
00:05:52,540 --> 00:05:54,140
Exactly.

65
00:05:57,060 --> 00:06:00,740
Yeah, I think the biggest issue with respect to personal data

66
00:06:00,740 --> 00:06:08,820
is that it's really, really difficult to define what exactly personal data is, or rather what

67
00:06:08,820 --> 00:06:11,700
does not constitute personal data anymore.

68
00:06:11,700 --> 00:06:14,600
Yeah, I totally agree.

69
00:06:14,600 --> 00:06:19,500
It's very difficult to anonymize data and we see this, you know, very often with people

70
00:06:19,500 --> 00:06:22,820
who come and ask for help that they think that they're not presenting personal data

71
00:06:22,820 --> 00:06:26,840
because whatever direct identifiers are missing.

72
00:06:26,840 --> 00:06:32,300
But if it's a data about an individual, then let's just agree that it's personal data rather

73
00:06:32,300 --> 00:06:37,400
than trying to mathematically demonstrate that the probability of re-identifying this

74
00:06:37,400 --> 00:06:41,600
person is whatever is your favorite threshold.

75
00:06:41,600 --> 00:06:46,860
But in general, again, here we don't have time and maybe we don't even have the energy

76
00:06:46,860 --> 00:06:51,680
to go through legislations, but at least this list that is mentioned here is something that

77
00:06:51,680 --> 00:06:59,440
you should be aware if your research kind of you know applies in this with this type of data.

78
00:07:00,320 --> 00:07:07,920
And then of course from the core ethical principle to the legislation and regulations we have the

79
00:07:07,920 --> 00:07:13,680
kind of national policies or even university level policies. So in the page you find some

80
00:07:13,680 --> 00:07:18,240
links related to Aalto University but if you're from other university most likely you have the

81
00:07:18,240 --> 00:07:23,680
similar policy whether it's about the ethical review if that's what you need in your research

82
00:07:23,680 --> 00:07:31,040
or open science policies, open data policies, code of conduct and things like that.

83
00:07:32,240 --> 00:07:41,440
So then at the end it's you the researchers who will depending on the case need to consider

84
00:07:41,440 --> 00:07:49,200
these policies and this legislation and maybe these core ethical principles and trying to do

85
00:07:49,200 --> 00:07:54,480
the best you can. I want to stress here that sometimes the best practices are some,

86
00:07:56,000 --> 00:08:02,000
how can I say, state that it will never be reached and I really like that in this paper

87
00:08:02,000 --> 00:08:08,800
by Wilson 2017 that is linked here, they mention good enough computational practices in this case

88
00:08:08,800 --> 00:08:16,400
because sometimes it's trying to avoid that type of sloppy practices so that by adopting good

89
00:08:16,400 --> 00:08:23,920
enough practices you can, you know, do the best you can and maybe slowly adopt more good enough

90
00:08:23,920 --> 00:08:31,680
practices until you reach the best practices. I would like to add that one problem with

91
00:08:31,680 --> 00:08:37,680
best practices is that very often to actually achieve best practices you need multiple people

92
00:08:37,680 --> 00:08:46,720
working on the same thing and one person doesn't have all the knowledge and very often also not the

93
00:08:47,440 --> 00:08:53,920
capabilities to do all things that would be best practice because it's just getting too much.

94
00:08:54,720 --> 00:09:01,840
So this concept of good enough practices is something that is quite important in my opinion.

95
00:09:01,840 --> 00:09:08,320
I totally agree. In this figure here, I tried to represent this is not exhaustive. I'm sure

96
00:09:08,320 --> 00:09:14,880
there's more best practices in computational science and there's more, how should we call

97
00:09:14,880 --> 00:09:21,680
them, worst practices. But in this continuum, so basically in the green side, we would have,

98
00:09:21,680 --> 00:09:28,000
you know, that all the data is archived with the DOI, there is data versioning, we have protocols,

99
00:09:28,000 --> 00:09:33,760
register protocols for data collection, version control for the code, so full reproducibility,

100
00:09:33,760 --> 00:09:39,920
unit tests, you can add you know all the things that also we have mentioned today earlier,

101
00:09:39,920 --> 00:09:44,560
and on the other end you really have this case that there are no backups, there's no documentation,

102
00:09:44,560 --> 00:09:51,440
no readme file, no idea how to rerun things, no dependencies listed, no history log on which

103
00:09:51,440 --> 00:09:58,480
script was from before the other. So I'm not asking you now, [name], to disclose something

104
00:09:58,480 --> 00:10:05,040
sensitive, but if you put yourself in this type of continuum, or maybe let's rephrase this. Have

105
00:10:05,040 --> 00:10:15,920
you had experience of some worst practices, actually? Okay, I have myself been in the

106
00:10:15,920 --> 00:10:25,520
situation that I had code that was not documented well enough and that where I then two years later

107
00:10:25,520 --> 00:10:31,440
was trying to think okay how did I do this, where did I put the configuration files, where did I do

108
00:10:31,440 --> 00:10:38,640
this and that and needed to go back and not completely reinvent but it took me quite some

109
00:10:38,640 --> 00:10:50,560
some time to set this up again. I have seen enough kind of bad things online from others

110
00:10:50,560 --> 00:11:02,720
where it's like, you're providing the data, but informants that are completely unusable

111
00:11:02,720 --> 00:11:07,720
and that essentially it would feel kind of more honest

112
00:11:08,680 --> 00:11:10,240
to just not provide the data.

113
00:11:12,600 --> 00:11:15,600
And I hope that I'm nowadays somewhere

114
00:11:15,600 --> 00:11:20,600
on the yellow to greenish border there,

115
00:11:21,680 --> 00:11:25,880
because yeah, I also won't be always able

116
00:11:25,880 --> 00:11:29,600
to go through all the different things

117
00:11:29,600 --> 00:11:33,440
that should be on the list.

118
00:11:34,940 --> 00:11:39,100
Because it's also quite a bit of things

119
00:11:39,100 --> 00:11:44,100
come with a lot of administrative work,

120
00:11:44,120 --> 00:11:46,020
which essentially means

121
00:11:46,020 --> 00:11:47,780
while you're doing the administrative work

122
00:11:47,780 --> 00:11:49,100
on a couple of things,

123
00:11:49,100 --> 00:11:51,580
you're not actually doing anything

124
00:11:51,580 --> 00:11:54,460
to progress the project that you have.

125
00:11:54,460 --> 00:11:59,460
And that's getting frustrating very quickly.

126
00:11:59,600 --> 00:12:07,080
I agree. And the administrative work is sometimes not even valued, meaning that, for example,

127
00:12:07,080 --> 00:12:12,560
I wrote this here, this single folder for all projects where all the researchers from

128
00:12:12,560 --> 00:12:17,200
the same group have read and write. And, you know, basically I've seen myself, I was part

129
00:12:17,200 --> 00:12:22,600
of this type of, you know, gigantic research folders where things, I don't know, more

130
00:12:22,600 --> 00:12:28,240
or less self-organized, but never really, you know, nobody had the time to do this type

131
00:12:28,240 --> 00:12:36,160
of administrative work, even though the policies, whether they come from EU horizon or whether

132
00:12:36,160 --> 00:12:40,920
they come from Academy of Finland, they kind of stress the importance of managing the data

133
00:12:40,920 --> 00:12:46,720
and managing the software. But, you know, maybe you get a very good point earlier that

134
00:12:46,720 --> 00:12:51,960
when it's not anymore this type of single person job that needs to do everything from

135
00:12:51,960 --> 00:12:58,080
inventing the idea, collecting the data, writing the code, if it becomes a teamwork that then

136
00:12:58,080 --> 00:13:05,920
you have the data steward, the research software engineer, and then it's the researchers,

137
00:13:05,920 --> 00:13:11,440
the expert can focus on their expertise. And then in the end, best practices and even

138
00:13:11,440 --> 00:13:20,640
administrative tasks can be taken care of. Just along those lines, I often see the problem that

139
00:13:20,640 --> 00:13:25,640
that I would like to follow best practices,

140
00:13:26,080 --> 00:13:31,080
but the best practices are written down so vaguely that,

141
00:13:33,680 --> 00:13:35,420
yeah, like I just said,

142
00:13:35,420 --> 00:13:38,080
they stress that you should care about data management,

143
00:13:38,080 --> 00:13:39,680
but how?

144
00:13:39,680 --> 00:13:41,140
What should you actually do?

145
00:13:44,100 --> 00:13:47,400
These kind of really concrete guidelines,

146
00:13:47,400 --> 00:13:50,520
they are, at least to me, very often lacking.

147
00:13:54,840 --> 00:13:55,800
Just to mention it again,

148
00:13:55,800 --> 00:13:58,440
with the administrative stuff that is undervalued,

149
00:14:00,000 --> 00:14:01,680
especially as a researcher

150
00:14:01,680 --> 00:14:04,120
that does something with personal data,

151
00:14:04,120 --> 00:14:08,640
you fill in the same data into 20 different forms

152
00:14:08,640 --> 00:14:12,440
to submit to 20 different places for approval.

153
00:14:12,440 --> 00:14:13,600
And it gets really frustrating.

154
00:14:13,600 --> 00:14:15,320
Why can't you just do this once?

155
00:14:15,320 --> 00:14:20,320
and everyone who wants to know something can extract from what you just submitted.

156
00:14:20,320 --> 00:14:28,320
So these are the kind of things that then lead to people not following protocol, not, yeah.

157
00:14:28,320 --> 00:14:30,320
Yeah.

158
00:14:30,320 --> 00:14:35,320
But I would say in general, I feel that things are much better.

159
00:14:35,320 --> 00:14:39,320
I don't know if it's because I've been involved in dealing with these issues,

160
00:14:39,320 --> 00:14:44,320
but maybe there is more awareness in general among researchers.

161
00:14:44,320 --> 00:14:51,120
It is getting better. Also because higher levels are enforcing it more.

162
00:14:52,320 --> 00:14:59,840
That's too good. All right, but basically to conclude, this page also has this little mention

163
00:14:59,840 --> 00:15:04,880
of cyber security and again the point here, you know, it's not to give a course on cyber

164
00:15:04,880 --> 00:15:09,680
security and most likely your organization has a mandatory course on cyber security.

165
00:15:09,680 --> 00:15:15,200
What is very important that I often try to remind to our researchers is that they need to understand

166
00:15:15,200 --> 00:15:20,400
the classification of information, meaning that they need to understand whether it's a script,

167
00:15:20,400 --> 00:15:26,000
whether it's a piece of data, whether it's your internal note or the picture for the summer trip

168
00:15:26,000 --> 00:15:31,840
of the department. You need to understand is it public data, is it internal data,

169
00:15:31,840 --> 00:15:39,120
is it confidential or is it secret. In general you could assume that most of the work we do

170
00:15:39,680 --> 00:15:45,960
goes between the confidential or internal if you are for example drafting a paper or

171
00:15:45,960 --> 00:15:51,440
still working on something that you're not ready to fully make public.

172
00:15:51,440 --> 00:15:56,320
But then of course there's also lots of public work that we do whether it's our open source

173
00:15:56,320 --> 00:16:00,680
code projects web pages and things like that.

174
00:16:00,680 --> 00:16:05,600
Maybe I would say that a minority of us need to work with secret data but of course there

175
00:16:05,600 --> 00:16:08,160
There are legislations, for example,

176
00:16:08,160 --> 00:16:10,320
when you need to reuse health data,

177
00:16:10,320 --> 00:16:12,960
so data that was collected for medical purposes.

178
00:16:12,960 --> 00:16:16,680
There are legislation that forces to be secret.

179
00:16:16,680 --> 00:16:20,680
Then in this last three minutes, we

180
00:16:20,680 --> 00:16:23,640
were discussing a few days ago that it's

181
00:16:23,640 --> 00:16:27,440
important to maybe mention what could be the issues,

182
00:16:27,440 --> 00:16:29,240
whether they are ethical or legal.

183
00:16:29,240 --> 00:16:34,760
It's a blurt of using generative AI in your daily work.

184
00:16:34,760 --> 00:16:39,600
Now here, I try to think of many types of issues,

185
00:16:39,600 --> 00:16:41,280
but to keep it short without,

186
00:16:41,280 --> 00:16:43,940
you can read the whole thing yourself.

187
00:16:43,940 --> 00:16:47,200
In general, I see when it comes to computational science

188
00:16:47,200 --> 00:16:50,000
as to write code, that there could be

189
00:16:50,000 --> 00:16:53,000
this type of unwanted plagiarism issues

190
00:16:53,000 --> 00:16:55,720
that I might be, that the generative AI

191
00:16:55,720 --> 00:16:59,280
might synthesize some code that is verbatim,

192
00:16:59,280 --> 00:17:01,900
some excerpt from a library,

193
00:17:01,900 --> 00:17:04,520
but then I might not realize this,

194
00:17:04,520 --> 00:17:06,520
most likely I would not start searching

195
00:17:06,520 --> 00:17:09,240
if that code is verbatim from a library.

196
00:17:09,240 --> 00:17:11,560
And then the issues becomes that maybe that library

197
00:17:11,560 --> 00:17:13,640
had a specific type of software license

198
00:17:13,640 --> 00:17:16,080
that wouldn't allow me to reuse it

199
00:17:16,080 --> 00:17:19,160
unless I also adopt the same license.

200
00:17:19,160 --> 00:17:22,560
There's no point here to go down to these details,

201
00:17:22,560 --> 00:17:25,560
but you can understand that there are multiple risks

202
00:17:25,560 --> 00:17:27,920
of using this type of generative AI

203
00:17:27,920 --> 00:17:31,440
without carefully considering basically the output.

204
00:17:31,440 --> 00:17:36,240
So again, if you are so-called vibe coding afternoon project

205
00:17:36,240 --> 00:17:39,840
because you just want to make some funny cat pictures

206
00:17:39,840 --> 00:17:42,560
through some code, maybe it's OK.

207
00:17:42,560 --> 00:17:46,120
But if you're writing the core functions for your research

208
00:17:46,120 --> 00:17:50,160
analysis, and you are not sure if the output

209
00:17:50,160 --> 00:17:53,000
from this generative AI system is the actual output

210
00:17:53,000 --> 00:17:57,000
that you would need, you need to reconsider or maybe

211
00:17:57,000 --> 00:18:00,320
ask for help from other experts for doing code review

212
00:18:00,320 --> 00:18:06,880
things like that. Maybe one that I like to mention is recently they noticed that

213
00:18:08,480 --> 00:18:14,160
when you work for example with Python you might need to import some NumPy or Pandas or whatever

214
00:18:14,160 --> 00:18:18,400
and if you don't have them in your system you need to pip install NumPy, Pandas and so on.

215
00:18:19,120 --> 00:18:25,120
And recently there are some consistent hallucinations of packages and then in the Python

216
00:18:25,120 --> 00:18:29,840
ecosystem but also in many other languages there is this type of typosquatting so that

217
00:18:29,840 --> 00:18:35,840
malicious people actually have created packages with a similar name and then you do a typo when

218
00:18:35,840 --> 00:18:43,200
you or the generative AI does a typo and then suddenly you have basically you know some

219
00:18:43,200 --> 00:18:50,240
malware installed in your system. I would like to add something that also recently came up

220
00:18:50,240 --> 00:19:01,280
when it comes to tools allowing the LLM to collect and process additional data or additional

221
00:19:01,280 --> 00:19:10,400
public data, and particularly if that same thing also allows the model to access some

222
00:19:10,400 --> 00:19:16,600
private information, because there have been recent attacks where essentially the public

223
00:19:16,600 --> 00:19:23,440
data that was consumed by the LLM contained instructions to the LLM, at which point it

224
00:19:23,440 --> 00:19:32,460
generated content that, when you looked at it, had image links or things in there that

225
00:19:32,460 --> 00:19:38,780
essentially were leaking private information.

226
00:19:38,780 --> 00:19:50,860
So essentially my recommendation is assume that the LLM is just another human, just another

227
00:19:50,860 --> 00:19:56,700
random human and think about whether you want to give it access to whatever you are currently

228
00:19:56,700 --> 00:20:00,700
giving it access to and that includes the whole conversation that you're currently having with it.

229
00:20:01,980 --> 00:20:07,500
I had to quote this, it's not [name]'s quote but [name] often says this that there is no cloud

230
00:20:07,500 --> 00:20:12,860
computing it's just someone else's computer and in my opinion it's the whole point because unless

231
00:20:12,860 --> 00:20:18,940
you run these LLMs locally and truly locally in your machine you're just sending data to

232
00:20:18,940 --> 00:20:24,860
somebody else's computer and even if they promise don't worry we will never give this data to anyone

233
00:20:24,860 --> 00:20:33,100
you know it's exactly yeah all right but hopefully this was motivational enough to think if what

234
00:20:33,100 --> 00:20:39,660
you're doing is good, is right, is moral, is ethical, is legal and so on. Feel free to

235
00:20:40,300 --> 00:20:47,980
write something in the chat of this type of philosophical thoughts but I guess it's time

236
00:20:47,980 --> 00:20:53,340
for a break and then later we can go back to code and computing. Thank you [name] for

237
00:20:53,340 --> 00:21:01,820
the nice conversation and I guess see you in 10 minutes

238
00:21:01,820 --> 00:21:07,580
I don't see any clock now so 12 p.m. bye

