1
00:00:00,000 --> 00:00:07,920
Hello again, hello everyone, we're back, I guess we can skip

2
00:00:07,920 --> 00:00:18,000
introduction. You have seen me and [name] before. This section will be about the LMs and generative

3
00:00:18,000 --> 00:00:26,160
AI tools. Please let me shortly overview the generative AI services available in Aalto.

4
00:00:27,600 --> 00:00:35,440
So we have a public service as a web service. It's similar as ChatGPT, but it's under

5
00:00:35,440 --> 00:00:40,120
agreement between Aalto and the company.

6
00:00:40,120 --> 00:00:43,400
We call it Ardor GPT or Aalto AI Assistant.

7
00:00:43,400 --> 00:00:49,040
It's similar as Chat GPT and based on the same models.

8
00:00:49,040 --> 00:00:55,520
Secondly, you can use some remote APIs to run models.

9
00:00:55,520 --> 00:01:04,160
Aalto also has institutional agreement with Azure OpenAI API,

10
00:01:04,160 --> 00:01:06,160
so you can request that.

11
00:01:06,160 --> 00:01:09,200
It has a better data security.

12
00:01:09,200 --> 00:01:10,980
In addition to that,

13
00:01:10,980 --> 00:01:14,780
we have hosted some local models

14
00:01:14,780 --> 00:01:19,680
on entirely on our own infrastructure.

15
00:01:19,680 --> 00:01:24,280
Those APIs are based on open source models,

16
00:01:24,280 --> 00:01:28,640
so they have very good data privacy.

17
00:01:28,640 --> 00:01:31,320
So basically, you can use them and

18
00:01:31,320 --> 00:01:34,600
maybe request for different models.

19
00:01:34,600 --> 00:01:38,640
This job is in developing.

20
00:01:38,640 --> 00:01:42,400
Of course, if you want to run models locally on

21
00:01:42,400 --> 00:01:48,520
your computer or on a cluster to get higher performance,

22
00:01:48,520 --> 00:01:53,320
it's also good. Actually, that will be our main focus today.

23
00:01:53,320 --> 00:01:58,520
But usually, your computer doesn't have enough resource.

24
00:01:58,520 --> 00:02:03,240
so Triton or the other cluster you're working on will be a good option.

25
00:02:05,000 --> 00:02:13,720
But in addition to these general AI tools, we also have a dedicated tool for speech-to-text.

26
00:02:14,680 --> 00:02:22,360
[name] knows better about this. Yeah, but just to add something about the API that is provided by

27
00:02:22,360 --> 00:02:30,280
ITS. If I'm not mistaken you can use it with confidential data as well but it has this cluster

28
00:02:30,280 --> 00:02:35,000
structure and you have to go and like fill a form and everything but if you host your model locally

29
00:02:35,000 --> 00:02:39,800
you don't have to be worried about the cost and everything all except for the GPU time that you

30
00:02:39,800 --> 00:02:48,200
are you are getting for that for the cluster. So yeah in the project. But yeah but we have also

31
00:02:48,200 --> 00:02:53,480
another service called Speech-to-Text. It's based on the OpenAI Whisper and it's very good for

32
00:02:53,480 --> 00:03:00,360
transcribing your videos or audios or interviews that you have. We would show how to run it on,

33
00:03:00,360 --> 00:03:04,440
or maybe we don't have time, I don't know, but if you go to the docs and click on the link

34
00:03:04,440 --> 00:03:09,400
there is a very good documentation that you can follow both on on-demand on the web service and

35
00:03:09,400 --> 00:03:16,520
also on the terminal if you prefer that. All of the processing is done on Triton so you can process

36
00:03:16,520 --> 00:03:21,960
confidential data as well, and it supports multiple languages. It's at the moment very

37
00:03:21,960 --> 00:03:30,040
fast because it utilizes on the GPUs. I guess that's it.

38
00:03:31,000 --> 00:03:37,560
Is there anything else that you want to add? I think that's it. Let's go to our main focus

39
00:03:37,560 --> 00:03:49,720
today, which is running LLMs on the cluster. So we know that LLMs, LLMs models, they are very big

40
00:03:49,720 --> 00:03:56,760
and they usually need quite a lot of disk space to store the models. So we pre-downloaded

41
00:03:57,960 --> 00:04:06,840
the popular models on Triton. So you can access them and you can use them. It's in a shared

42
00:04:06,840 --> 00:04:12,400
the folder so it won't occupy your own disk space.

43
00:04:12,400 --> 00:04:17,760
But there's some tricks to use the models.

44
00:04:17,760 --> 00:04:22,380
We will talk about the coding part later.

45
00:04:22,380 --> 00:04:27,460
But firstly, maybe you want to know which models are available.

46
00:04:27,460 --> 00:04:34,100
So simply run this command on Triton, of course.

47
00:04:34,100 --> 00:04:37,580
you will see all the models listed there.

48
00:04:37,580 --> 00:04:40,820
You can request more models if you need,

49
00:04:40,820 --> 00:04:43,140
and we will also be keeping

50
00:04:43,140 --> 00:04:47,500
our eyes on the latest models and download them for you.

51
00:04:47,500 --> 00:04:50,660
The most common way to use the models

52
00:04:50,660 --> 00:04:54,900
is by using Transformers Python library.

53
00:04:54,900 --> 00:04:57,460
It's developed by Hugging Face,

54
00:04:57,460 --> 00:05:02,740
and it also provides a lot of tools and models,

55
00:05:02,740 --> 00:05:11,700
of course, to use it has a very good interfaces to use the models. We will demonstrate some

56
00:05:11,700 --> 00:05:21,140
examples later. But after you know that where to find the model, the next thing to consider is to

57
00:05:22,100 --> 00:05:31,540
how much resources to request to use the model. You have just listened to the wonderful talk from

58
00:05:31,540 --> 00:05:33,940
similar and the whole thing about GPU computing.

59
00:05:33,940 --> 00:05:37,020
So when it comes to LLMs,

60
00:05:37,020 --> 00:05:40,360
GPU is where most of the time needed.

61
00:05:40,360 --> 00:05:43,740
So you needed to request the GPUs.

62
00:05:43,740 --> 00:05:48,540
The thing is, what kind of GPUs to request?

63
00:05:48,540 --> 00:05:51,180
That depends on the model,

64
00:05:51,180 --> 00:05:53,260
which model you're using.

65
00:05:53,260 --> 00:05:55,700
Meanwhile, you needed to request

66
00:05:55,700 --> 00:06:02,700
some number of CPUs and system memories.

67
00:06:02,700 --> 00:06:06,220
You have just listened to this,

68
00:06:06,220 --> 00:06:08,740
that the system memory is

69
00:06:08,740 --> 00:06:11,580
something separated from the GPU memory.

70
00:06:11,580 --> 00:06:16,140
Let's first talk a bit about the GPU memory.

71
00:06:16,140 --> 00:06:18,740
When you request a GPU,

72
00:06:18,740 --> 00:06:20,660
you get the whole GPU,

73
00:06:20,660 --> 00:06:23,900
so all the memory belongs to you.

74
00:06:23,900 --> 00:06:30,700
but different GPUs have different amount of memory.

75
00:06:30,940 --> 00:06:35,300
On Triton, we can use it as infer.

76
00:06:35,300 --> 00:06:37,500
I think [name] also showed that

77
00:06:37,500 --> 00:06:41,780
to get a list of partitions available.

78
00:06:41,780 --> 00:06:48,340
Partition names can reflect the GPU type.

79
00:06:48,340 --> 00:06:52,820
For example, GPU V100 with 32 gigabytes memory,

80
00:06:52,820 --> 00:07:01,660
There are also some other newer GPUs like A100 or H100 and H200.

81
00:07:01,660 --> 00:07:04,140
But the thing is,

82
00:07:04,140 --> 00:07:06,780
how much memory do you need?

83
00:07:06,780 --> 00:07:09,980
It depends on the model size.

84
00:07:09,980 --> 00:07:18,620
We have a table here as a reference that usually,

85
00:07:18,620 --> 00:07:22,140
when you find a model on Hugging Face,

86
00:07:22,140 --> 00:07:24,020
there will be a model card.

87
00:07:24,020 --> 00:07:29,820
The model card will tell you how many parameters the model has,

88
00:07:29,820 --> 00:07:34,480
and also from the model name,

89
00:07:34,480 --> 00:07:37,500
you can also know the number of parameters.

90
00:07:37,500 --> 00:07:40,820
For example, it's a Mistral 7-billion model,

91
00:07:40,820 --> 00:07:43,700
then it has seven billion parameters.

92
00:07:43,700 --> 00:07:48,220
But it depends on the data type used by the model.

93
00:07:48,220 --> 00:07:53,220
it could be like 28 gigabytes or 14 gigabytes,

94
00:07:53,380 --> 00:07:55,540
or if you quantize the model,

95
00:07:55,540 --> 00:08:00,540
then it will be even like a half the memory.

96
00:08:00,980 --> 00:08:05,980
So this table can be roughly telling you

97
00:08:06,220 --> 00:08:10,100
that how much GPU memory you need

98
00:08:10,100 --> 00:08:13,040
and then which kind of GPU you can request.

99
00:08:14,620 --> 00:08:16,980
But there's some other tricky things here

100
00:08:16,980 --> 00:08:21,260
Because AI is developing very fast,

101
00:08:21,260 --> 00:08:25,940
some models, they use different operators.

102
00:08:25,940 --> 00:08:29,340
And some operators or model layers

103
00:08:29,340 --> 00:08:35,660
may need a different GPU computing capacity.

104
00:08:35,660 --> 00:08:43,900
So it's a bit tricky that maybe the model can fit in the GPU,

105
00:08:43,900 --> 00:08:48,340
but the GPU doesn't support the model

106
00:08:48,340 --> 00:08:52,620
or the software you use to run the model.

107
00:08:52,620 --> 00:08:56,420
So you may also face this kind of errors.

108
00:08:56,420 --> 00:09:01,940
Don't be very worried.

109
00:09:01,940 --> 00:09:02,940
You can come to us.

110
00:09:02,940 --> 00:09:06,260
We can help you to do the troubleshooting.

111
00:09:06,260 --> 00:09:10,020
And you can also try out, generally speaking,

112
00:09:10,020 --> 00:09:12,660
try out newer GPUs.

113
00:09:12,660 --> 00:09:16,220
So that's about the GPU memory.

114
00:09:16,220 --> 00:09:19,780
Let's go back to this again.

115
00:09:21,220 --> 00:09:24,500
And there's the system memory.

116
00:09:24,500 --> 00:09:29,500
The system memory, well, like [name] mentioned before,

117
00:09:30,080 --> 00:09:34,460
it depends on how your program was written

118
00:09:34,460 --> 00:09:37,920
and how the data will be loaded.

119
00:09:37,920 --> 00:09:41,660
So usually we will request a bit more memory,

120
00:09:41,660 --> 00:09:44,940
system memory than GPU memory.

121
00:09:44,940 --> 00:09:49,460
For example, here, the GPU memory is 32 gigabytes.

122
00:09:49,460 --> 00:09:51,700
I put 80 gigabytes here,

123
00:09:51,700 --> 00:09:56,460
but it's probably a bit too much, I would say.

124
00:09:56,460 --> 00:09:59,640
But roughly speaking,

125
00:09:59,640 --> 00:10:02,380
the system memory is less expensive,

126
00:10:02,380 --> 00:10:06,720
so I would use a slightly higher number.

127
00:10:06,720 --> 00:10:10,500
For example, maybe 40 gigabytes is enough here.

128
00:10:10,500 --> 00:10:17,140
And then the CPUs, a number of CPUs.

129
00:10:17,140 --> 00:10:22,240
Sometimes you need multiple CPUs to run the models.

130
00:10:22,240 --> 00:10:27,420
Some framework can use multi-processes to do

131
00:10:27,420 --> 00:10:33,420
some data pre-processing or even some intermediate steps,

132
00:10:33,420 --> 00:10:35,500
many more CPUs.

133
00:10:35,500 --> 00:10:40,340
So in general, I would put a few more CPUs here

134
00:10:40,340 --> 00:10:45,260
And again, it's less expensive than GPUs.

135
00:10:45,260 --> 00:10:47,820
So what do you think, [name]?

136
00:10:47,820 --> 00:10:51,420
Do you have anything to add here?

137
00:10:51,420 --> 00:10:53,700
I was trying to answer a question.

138
00:10:53,700 --> 00:10:55,620
But no, I think it's fine.

139
00:10:55,620 --> 00:10:57,820
But the thing that you were mentioning

140
00:10:57,820 --> 00:11:00,020
about the architecture of the model

141
00:11:00,020 --> 00:11:01,980
is also an important thing.

142
00:11:01,980 --> 00:11:04,700
And again, if you don't know what is happening,

143
00:11:04,700 --> 00:11:06,300
I guess you don't have to worry about.

144
00:11:06,300 --> 00:11:09,660
But sometimes, some part of the model

145
00:11:09,660 --> 00:11:12,020
or like some part of the calculation needs

146
00:11:12,020 --> 00:11:15,300
and a specific kind of architecture like flash attention

147
00:11:15,300 --> 00:11:17,200
that it's like very popular nowadays.

148
00:11:18,100 --> 00:11:21,220
But if you run it on a normal GPU

149
00:11:21,220 --> 00:11:22,740
that doesn't support the flash attention,

150
00:11:22,740 --> 00:11:24,500
it would just raise an error.

151
00:11:24,500 --> 00:11:28,760
So it's like very likely, especially at the beginning

152
00:11:28,760 --> 00:11:30,540
when you're working with these huge models

153
00:11:30,540 --> 00:11:33,500
to care to face a lot of issues

154
00:11:33,500 --> 00:11:36,860
and then don't get worried, come and ask us

155
00:11:36,860 --> 00:11:38,900
and we would try to help you.

156
00:11:38,900 --> 00:11:43,340
Or ask GPT in that case, but we prefer human interaction.

157
00:11:43,340 --> 00:11:44,940
Yes.

158
00:11:44,940 --> 00:11:46,020
OK.

159
00:11:46,020 --> 00:11:49,460
Yeah, I mentioned that we have downloaded a lot of models

160
00:11:49,460 --> 00:11:53,720
for you and put them in a shared folder.

161
00:11:53,720 --> 00:11:56,540
The thing is Hugging Face Transformers

162
00:11:56,540 --> 00:12:00,540
doesn't look for the models by default.

163
00:12:00,540 --> 00:12:05,820
There is an environment variable called hifhome.

164
00:12:05,820 --> 00:12:08,940
By default, it's pointing to your home folder.

165
00:12:08,940 --> 00:12:11,620
As you have learned on the first day,

166
00:12:11,620 --> 00:12:14,420
I guess, your home folder is kind of small.

167
00:12:14,420 --> 00:12:20,460
It has only 20 gigabytes disk space.

168
00:12:20,460 --> 00:12:22,860
So what you should do essentially

169
00:12:22,860 --> 00:12:29,820
is to change the Hugging Face home to the shared folder

170
00:12:29,820 --> 00:12:31,060
we have.

171
00:12:31,060 --> 00:12:35,260
But again, as you have learned before on Triton,

172
00:12:35,260 --> 00:12:37,380
or I would say most of the clusters,

173
00:12:38,700 --> 00:12:42,660
a lot of softwares and tools will be managed by modules.

174
00:12:42,660 --> 00:12:45,420
So here we also use a module to manage

175
00:12:45,420 --> 00:12:49,380
the access to the models.

176
00:12:49,380 --> 00:12:53,380
So you need to module load the model hacking face.

177
00:12:53,380 --> 00:12:58,380
What this does is basically setting the hacking face home

178
00:13:00,020 --> 00:13:04,540
to the correct location, the correct directory,

179
00:13:04,540 --> 00:13:09,300
which is this one here, yeah.

180
00:13:09,300 --> 00:13:11,900
And by doing this,

181
00:13:11,900 --> 00:13:15,560
transformers will know where to find the models.

182
00:13:15,560 --> 00:13:20,560
But after that, you need to have a proper conda environment

183
00:13:20,580 --> 00:13:22,860
to use the models.

184
00:13:22,860 --> 00:13:26,580
We have created a model, sorry,

185
00:13:26,580 --> 00:13:29,420
we have created a conda environment,

186
00:13:29,420 --> 00:13:33,900
which is called scicomp-llm-env.

187
00:13:33,900 --> 00:13:36,180
This is managed by us.

188
00:13:36,180 --> 00:13:40,300
It has most of the popular frameworks,

189
00:13:40,300 --> 00:13:45,300
like Transformers, like llama.cpp and the stuff like that.

190
00:13:46,900 --> 00:13:49,260
But if you need your own ones,

191
00:13:49,260 --> 00:13:52,740
you can make a customized conda info

192
00:13:52,740 --> 00:13:57,740
by using the knowledge you learned from yesterday, I think.

193
00:13:57,980 --> 00:14:01,500
But anyway, for this demo, we can use this one.

194
00:14:01,500 --> 00:14:07,380
And most of the times, this one would work, I would say.

195
00:14:07,380 --> 00:14:11,260
So that's the Python environment.

196
00:14:11,260 --> 00:14:14,020
But after that, we needed to do something else

197
00:14:14,020 --> 00:14:20,740
to let Transformer to use the local model instead

198
00:14:20,740 --> 00:14:25,540
of downloading models from HuggingFace.

199
00:14:25,540 --> 00:14:29,620
So we needed to set up two environment variables.

200
00:14:29,620 --> 00:14:34,620
Here is transformers offline, kind of set it to true,

201
00:14:34,660 --> 00:14:37,860
and then export another one.

202
00:14:37,860 --> 00:14:42,460
It's also for transformers to load the local models,

203
00:14:42,460 --> 00:14:46,060
and then run your Python script.

204
00:14:46,060 --> 00:14:49,620
We will show the example later.

205
00:14:50,780 --> 00:14:53,180
I will skip this Python script.

206
00:14:53,180 --> 00:14:56,020
We will see it in the demo.

207
00:14:59,620 --> 00:15:03,460
Yeah. I think that's all from here.

208
00:15:03,460 --> 00:15:06,280
We can start to the demo now.

209
00:15:06,280 --> 00:15:09,680
Maybe we can show a real example.

210
00:15:09,680 --> 00:15:10,800
Sorry?

211
00:15:10,800 --> 00:15:12,360
Yeah, we can show a real example.

212
00:15:12,360 --> 00:15:17,280
It can be a bit overwhelming to hear all of this in a 30-minute session,

213
00:15:17,280 --> 00:15:22,280
but once you understand a bit of it and then get your hands dirty,

214
00:15:22,280 --> 00:15:24,720
you would, don't worry, you will get it.

215
00:15:24,720 --> 00:15:27,560
But at the beginning, it can be really overwhelming with

216
00:15:27,560 --> 00:15:30,080
all of the environments and different parameters

217
00:15:30,080 --> 00:15:31,620
that you have to set.

218
00:15:31,620 --> 00:15:32,900
Yeah.

219
00:15:32,900 --> 00:15:34,900
Okay, let me go to the,

220
00:15:35,940 --> 00:15:40,440
I'll use the on-demand Jupyter Notebook to do the demo.

221
00:15:42,860 --> 00:15:45,140
This is my work directory.

222
00:15:45,140 --> 00:15:49,140
I haven't downloaded the examples.

223
00:15:50,860 --> 00:15:52,960
I will go to the repo.

224
00:15:54,700 --> 00:15:55,540
Okay.

225
00:15:57,560 --> 00:16:00,160
The LLM examples report, yeah.

226
00:16:00,160 --> 00:16:02,680
Yes, it's in the note.

227
00:16:07,320 --> 00:16:12,000
You can see we have a few different examples here.

228
00:16:12,000 --> 00:16:15,040
Today we will focus on the hacking face models.

229
00:16:15,040 --> 00:16:19,520
We also have models for VLLM and

230
00:16:19,520 --> 00:16:26,200
also a small example for like a chat with your PDF.

231
00:16:26,200 --> 00:16:32,240
It's a kind of AI application example,

232
00:16:32,240 --> 00:16:35,800
but we will focus to HuggingFace.

233
00:16:36,160 --> 00:16:41,040
So basically, we have a minimal example here.

234
00:16:41,040 --> 00:16:44,960
I have the Summator script.

235
00:16:47,640 --> 00:16:50,040
So this is the one.

236
00:16:50,040 --> 00:16:55,600
This is a batch script you have seen from the previous days.

237
00:16:55,600 --> 00:17:02,880
Basically, I have the time here and the number of CPUs and the memory.

238
00:17:02,880 --> 00:17:04,920
Okay, I have 40 gigabytes here.

239
00:17:04,920 --> 00:17:07,540
It's more reasonable.

240
00:17:07,540 --> 00:17:09,860
I request one GPU.

241
00:17:09,860 --> 00:17:12,820
Of course, you can request the two GPUs if

242
00:17:12,820 --> 00:17:17,540
the model is large and couldn't fit in one GPU.

243
00:17:17,540 --> 00:17:20,420
But that's a little bit advanced.

244
00:17:20,420 --> 00:17:25,420
Maybe we can look at that later.

245
00:17:25,420 --> 00:17:30,500
If you need help, come to our help session.

246
00:17:30,500 --> 00:17:34,260
Then the partition, I will use this V100.

247
00:17:34,260 --> 00:17:36,980
I would say this is a bit old GPU,

248
00:17:36,980 --> 00:17:42,900
but the model I'm going to run is also not a very new one.

249
00:17:42,900 --> 00:17:45,860
As I mentioned before,

250
00:17:45,860 --> 00:17:50,000
load the necessary modules and run the script.

251
00:17:50,000 --> 00:17:54,140
I will submit the job firstly because it will take a while.

252
00:17:54,140 --> 00:17:58,140
Yes. But what does it do, the script?

253
00:17:58,140 --> 00:18:00,660
Can you also show the script and we can see?

254
00:18:00,660 --> 00:18:02,660
The Python script.

255
00:18:02,660 --> 00:18:07,460
I will show that later after I submit the job.

256
00:18:07,460 --> 00:18:09,980
Now it's submitted and let's

257
00:18:09,980 --> 00:18:13,220
check out if it can start immediately.

258
00:18:13,220 --> 00:18:17,740
It's pending. Let's go to the script.

259
00:18:18,180 --> 00:18:21,820
This is the script I submitted by using

260
00:18:21,820 --> 00:18:24,620
the shell script.

261
00:18:24,620 --> 00:18:28,820
It's an example to use the model.

262
00:18:28,820 --> 00:18:31,420
This is the model I'm using here.

263
00:18:31,420 --> 00:18:35,700
HuggingFace provides basically

264
00:18:35,700 --> 00:18:39,660
two different ways to run the model.

265
00:18:39,660 --> 00:18:43,140
The first one is a little bit complex.

266
00:18:43,140 --> 00:18:47,280
You can see it has multiple steps.

267
00:18:47,280 --> 00:18:50,960
Firstly, you need to load the model,

268
00:18:50,960 --> 00:18:56,080
like initialize the model and specify the data type.

269
00:18:56,080 --> 00:18:58,260
I use auto here.

270
00:18:58,260 --> 00:19:01,060
By using this, it will automatically choose

271
00:19:01,060 --> 00:19:04,760
the best data type or

272
00:19:04,760 --> 00:19:09,960
the default data type when the model was released.

273
00:19:09,960 --> 00:19:18,240
I also did some memory usage reduction by quantization.

274
00:19:18,240 --> 00:19:27,120
I will load the model with 8-bit instead of float 32 or float 16.

275
00:19:27,120 --> 00:19:33,200
I set the device map to auto to let it automatically find the GPUs available.

276
00:19:35,520 --> 00:19:38,320
Maybe I shouldn't go to that detailed stuff.

277
00:19:40,880 --> 00:19:47,920
We have the tokenizer and the tokenizer will be initialized under the messages.

278
00:19:48,240 --> 00:19:59,240
You want to pass to the model and then use the tokenizer to do some preprocessing for the text, like apply some chat template.

279
00:19:59,240 --> 00:20:06,240
But again, we won't go to the details. This is necessary for some models.

280
00:20:06,240 --> 00:20:14,240
And then the inputs after the preprocessing, the inputs will be passed to the model.

281
00:20:14,240 --> 00:20:22,240
The model has a generate method and then we will see the generated stuff.

282
00:20:22,240 --> 00:20:29,240
But this is a slightly complex procedure to use HuggingFace models.

283
00:20:29,240 --> 00:20:37,240
There's a higher level help API from Transformers or from HuggingFace.

284
00:20:37,240 --> 00:20:43,240
It's easier, but before that, maybe let's go back to the terminal to see if it's running.

285
00:20:43,240 --> 00:20:47,420
Okay, it's done, I think.

286
00:20:49,240 --> 00:20:50,840
Okay, it's done.

287
00:20:50,840 --> 00:20:53,840
Actually, I want to show you the GPU usage.

288
00:20:55,720 --> 00:20:59,560
Maybe I should submit it again.

289
00:21:00,640 --> 00:21:03,720
But we can also look at the seff data as well.

290
00:21:03,720 --> 00:21:04,960
Yeah, yeah, of course.

291
00:21:06,360 --> 00:21:11,220
And let me see if it's still running.

292
00:21:11,220 --> 00:21:12,680
Okay, it's running.

293
00:21:12,680 --> 00:21:14,800
Anyway, let's go back to this one.

294
00:21:15,760 --> 00:21:19,240
So a easier way to use the transformers

295
00:21:19,240 --> 00:21:24,240
is by using the pipeline API provided by Hugging Face.

296
00:21:25,160 --> 00:21:27,960
You don't need to specify the tokenizers

297
00:21:27,960 --> 00:21:32,840
and like initialize the model separately.

298
00:21:32,840 --> 00:21:35,800
You just put all the stuff to the pipeline,

299
00:21:35,800 --> 00:21:39,600
the name of the model, sorry, the task,

300
00:21:39,600 --> 00:21:50,160
name of the model and the device, the GPU, and some other parameters, some generation parameters

301
00:21:50,160 --> 00:21:57,120
or model arguments and all the configuration stuff here. Then the message, then it's...

302
00:21:58,720 --> 00:22:02,800
The job is running, so if you want to show that... Yes, let's see.

303
00:22:02,800 --> 00:22:15,800
It's quite, I hope it won't be stop.

304
00:22:15,800 --> 00:22:22,800
Okay, now we can see the usage of GPU.

305
00:22:22,800 --> 00:22:27,800
So, I gave it 32 gigabytes of memory GPU,

306
00:22:27,800 --> 00:22:32,040
GPU, but it used only half of the memory.

307
00:22:32,040 --> 00:22:39,080
GPU utilization is also lower than 50%.

308
00:22:39,080 --> 00:22:43,640
Well, this is something you can see yearly.

309
00:22:43,640 --> 00:22:47,320
OK, now I think the calculation has started.

310
00:22:47,320 --> 00:22:50,920
The usage of the memory is increasing to 24.

311
00:22:50,920 --> 00:23:05,240
Okay, yeah, so basically, you can have a look at the GPU memory usage and the computation

312
00:23:05,240 --> 00:23:13,040
usage in real time, but as [name] mentioned that after the job is done, we have the other

313
00:23:13,040 --> 00:23:18,680
tools to show the resource usage.

314
00:23:18,680 --> 00:23:23,680
Okay, now, maybe let's have a look at the output.

315
00:23:25,880 --> 00:23:28,160
We have two, the first one.

316
00:23:29,560 --> 00:23:34,560
Yeah, the output, well, it's not something very exciting.

317
00:23:36,000 --> 00:23:41,000
I just give it a message and it will apply a chat template,

318
00:23:41,000 --> 00:23:44,680
it will apply a chat template,

319
00:23:45,540 --> 00:23:49,660
kind of give the model some reminders.

320
00:23:49,660 --> 00:23:53,540
Okay, this is the starting point of the prompt,

321
00:23:53,540 --> 00:23:54,380
this is the end,

322
00:23:54,380 --> 00:23:58,220
and this is the starting point of your message.

323
00:23:58,220 --> 00:24:03,220
Sorry, so firstly is the system prompt

324
00:24:03,580 --> 00:24:06,100
like you are helpful assistant thing,

325
00:24:06,100 --> 00:24:09,500
and then it's the starting point of your message,

326
00:24:09,500 --> 00:24:10,660
your question.

327
00:24:10,660 --> 00:24:15,020
And then it will also add the starting point

328
00:24:15,020 --> 00:24:17,580
for the assistant, kind of telling it,

329
00:24:17,580 --> 00:24:22,180
now it's your turn to continue the generation

330
00:24:22,180 --> 00:24:23,920
to answer the questions.

331
00:24:25,380 --> 00:24:26,420
Yeah, we can see.

332
00:24:27,520 --> 00:24:32,520
Yeah, then the pipeline, by using the pipeline,

333
00:24:32,660 --> 00:24:36,820
we kind of skip the intermediate steps

334
00:24:36,820 --> 00:24:39,620
and get the response.

335
00:24:40,660 --> 00:24:48,660
By default, it will give you the prompts and the answers.

336
00:24:48,660 --> 00:24:52,860
You can do post-processing for this.

337
00:24:54,540 --> 00:24:58,940
Do we have any questions?

338
00:24:59,820 --> 00:25:04,140
I was trying to answer the questions.

339
00:25:04,140 --> 00:25:09,180
We have a user who faced an error.

340
00:25:09,180 --> 00:25:17,180
with the CUDA, I guess, but I'm not sure. But I just want to ask a question. When we're

341
00:25:17,180 --> 00:25:23,180
looking at the utilization, the utilization was like around 20 or 30 percent.

342
00:25:24,940 --> 00:25:29,020
How can we try to increase that or why is the utilization low, I would say?

343
00:25:30,540 --> 00:25:38,700
I think the utilization is low mainly telling us that the GPU is waiting for CPU doing stuff.

344
00:25:39,180 --> 00:25:43,860
or sometimes it's the input to output things going on,

345
00:25:43,860 --> 00:25:45,540
and the GPU is waiting for that.

346
00:25:45,540 --> 00:25:51,780
But in our case, I think it's mainly GPU waiting for CPU to do stuff.

347
00:25:51,780 --> 00:25:54,420
But our example is quite small.

348
00:25:54,420 --> 00:25:58,780
We don't have very large input data,

349
00:25:58,780 --> 00:26:01,460
and also we requested,

350
00:26:01,460 --> 00:26:05,500
I think we do some quantization of the model.

351
00:26:05,500 --> 00:26:12,180
Yes, we did. We used this to do some quantization.

352
00:26:12,180 --> 00:26:16,180
It will also make the computation lower,

353
00:26:16,180 --> 00:26:18,060
and the usage lower.

354
00:26:18,060 --> 00:26:21,020
But if you are running something very big,

355
00:26:21,020 --> 00:26:25,540
you would definitely need to optimize the usage of GPUs.

356
00:26:25,540 --> 00:26:27,220
But don't worry about that.

357
00:26:27,220 --> 00:26:30,780
It's a bit complex or tricky sometimes,

358
00:26:30,780 --> 00:26:35,740
so don't hesitate to talk to us.

359
00:26:35,740 --> 00:26:39,740
What essentially it means that load utilization,

360
00:26:39,740 --> 00:26:45,140
it means that your GPU can do much more computation,

361
00:26:45,140 --> 00:26:51,500
and because we are only answering simple questions here in this example,

362
00:26:51,500 --> 00:26:53,500
it means there are not enough data

363
00:26:53,500 --> 00:26:56,180
or not enough computation for the GPU to be done.

364
00:26:56,180 --> 00:26:57,920
If you have load utilization,

365
00:26:57,920 --> 00:27:03,120
And one of the things that you can try is like feed it more data or more complex data to a more complex

366
00:27:05,120 --> 00:27:10,000
model in this case. And as you mentioned, because the model is not complex, it only

367
00:27:10,000 --> 00:27:17,120
has eight bits of each parameter. It means every calculation is like done very fast.

368
00:27:18,160 --> 00:27:24,480
So yeah, there are like some techniques to make that GPU more useful or like make it more work.

369
00:27:24,480 --> 00:27:32,160
But yeah, it can be very complicated, so come and ask us and we will try to help you with that.

370
00:27:35,600 --> 00:27:40,560
Yes. Yeah, so anything else that you want to add here?

371
00:27:43,600 --> 00:27:49,440
I don't have too much from my side. I mean, we only give a very simple demo. There are

372
00:27:49,440 --> 00:27:58,960
too many the other details. Yeah, unfortunately we don't have enough time for the exercise today,

373
00:28:00,080 --> 00:28:11,120
but we will be there. So what I would recommend is to try to run the LLM example by yourself.

374
00:28:11,120 --> 00:28:16,160
I would look at the example again and see if we can find the error that the user was

375
00:28:16,160 --> 00:28:21,560
facing and would reply in the in the notes but try to rerun the example and

376
00:28:21,560 --> 00:28:26,020
see if you are facing an error and if you can do that of course you can do

377
00:28:26,020 --> 00:28:30,920
like much more you can like request multiple GPUs with like larger amount of

378
00:28:30,920 --> 00:28:35,400
like larger model so to divide the model and everything but it's more complicated

379
00:28:35,400 --> 00:28:40,560
it doesn't fit in that 30 minute session so yeah if you want to learn more or if

380
00:28:40,560 --> 00:28:46,000
you have any questions come and ask us and yeah I think it was good and we are

381
00:28:46,000 --> 00:28:48,920
are all right on time.

382
00:28:48,920 --> 00:28:50,280
So what is next?

383
00:28:50,280 --> 00:28:52,760
I guess we have a, no, we don't have a short break.

384
00:28:52,760 --> 00:28:56,000
So we have the wrap-up and summary.

