1
00:00:00,000 --> 00:00:02,000
CodeRefinery.org

2
00:00:30,000 --> 00:00:32,060
you

3
00:01:00,000 --> 00:01:02,060
you

4
00:01:30,000 --> 00:01:32,060
you

5
00:02:00,000 --> 00:02:02,060
you

6
00:02:30,000 --> 00:02:32,060
you

7
00:03:00,000 --> 00:03:02,060
you

8
00:03:30,000 --> 00:03:32,060
you

9
00:04:00,000 --> 00:04:02,060
you

10
00:04:30,000 --> 00:04:32,060
you

11
00:05:00,000 --> 00:05:02,060
you

12
00:05:30,000 --> 00:05:32,060
you

13
00:06:00,000 --> 00:06:02,060
you

14
00:06:30,000 --> 00:06:32,060
you

15
00:07:00,000 --> 00:07:02,060
you

16
00:07:30,000 --> 00:07:32,060
you

17
00:08:00,000 --> 00:08:02,060
you

18
00:08:30,000 --> 00:08:32,060
you

19
00:09:00,000 --> 00:09:02,060
you

20
00:09:30,000 --> 00:09:32,060
you

21
00:10:00,000 --> 00:10:02,060
you

22
00:10:30,000 --> 00:10:32,060
you

23
00:11:00,000 --> 00:11:16,360
Hello.

24
00:11:16,360 --> 00:11:19,320
Can anyone hear us?

25
00:11:19,320 --> 00:11:25,560
Yeah, I think they can hear us, at least.

26
00:11:25,560 --> 00:11:27,120
I can hear us on the stream.

27
00:11:27,120 --> 00:11:28,120
You hear us?

28
00:11:28,120 --> 00:11:29,120
Okay, good.

29
00:11:29,120 --> 00:11:34,560
That's a good start.

30
00:11:34,560 --> 00:11:52,960
Let's get all these controls going.

31
00:11:52,960 --> 00:12:13,960
Yeah, we have icebreakers, like, unless you like icy weather in the chat.

32
00:12:13,960 --> 00:12:16,560
What is your favorite weather?

33
00:12:16,560 --> 00:12:19,060
Yeah.

34
00:12:19,060 --> 00:12:25,440
OK, so yeah, so [name], what's your favorite weather?

35
00:12:25,440 --> 00:12:30,840
I would say that it's like, it's a weather that changes

36
00:12:30,840 --> 00:12:33,000
every two weeks or something.

37
00:12:33,000 --> 00:12:35,280
Like, I don't usually like it to be

38
00:12:35,280 --> 00:12:37,360
the same too long at a time.

39
00:12:37,360 --> 00:12:39,280
But I do like Finnish winters.

40
00:12:39,280 --> 00:12:40,320
It's great.

41
00:12:40,320 --> 00:12:43,280
Yeah, yeah.

42
00:12:43,280 --> 00:12:47,040
I mean, a cold, dry winter is good.

43
00:12:48,240 --> 00:12:49,640
Yeah, dry, yeah.

44
00:12:49,960 --> 00:12:50,160
Yeah.

45
00:12:52,600 --> 00:12:57,360
And you know, like once it gets to 20, it can get too hot sometimes, but maybe

46
00:12:58,440 --> 00:13:07,600
15 to 20 and sunny, occasional rain every few days to keep the fire warnings away.

47
00:13:07,600 --> 00:13:17,600
It's often said that, especially in the cold weather, it's a question of what are you wearing.

48
00:13:17,600 --> 00:13:22,600
But I think for different kinds of weather, it's the question of what are you planning on doing.

49
00:13:22,600 --> 00:13:28,600
Summer weather is very nice if you're planning on going to the beach or something like that.

50
00:13:28,600 --> 00:13:34,600
But it's not very nice if you're inside and have to work.

51
00:13:34,600 --> 00:13:43,600
If you say you happen to be inside with a bunch of computers running a streaming setup in a insulated building.

52
00:13:43,600 --> 00:13:50,600
Yeah. Not necessarily the best weather for that sort of an activity. I don't know who would do that.

53
00:13:50,600 --> 00:13:53,600
Yeah. Yeah.

54
00:13:53,600 --> 00:13:58,600
Okay. The next question is which technologies have you used?

55
00:13:58,600 --> 00:14:03,600
And there are some things there like Python, multi-processing, R parallel and so on.

56
00:14:03,600 --> 00:14:15,100
These are all basically ways to use multiple processors at once, and maybe other people can add more of them there.

57
00:14:15,100 --> 00:14:23,600
If you don't know the terminology, it's okay, you'll learn later, but if you do know it and you've used it, then mark it down.

58
00:14:23,600 --> 00:14:30,600
And this is really important, because this is basically going to be... well, this is what we'll go through here.

59
00:14:30,600 --> 00:14:39,040
Yeah, and don't worry about the word salad again like this, there's going to be a lot

60
00:14:39,040 --> 00:14:45,200
of these terms and we'll try to explain them when we go, like when we encounter them.

61
00:14:45,200 --> 00:14:51,720
But yeah, there's going to be some of these words that especially like we already encountered

62
00:14:51,720 --> 00:14:59,520
yesterday the bash batch kind of a thing where you basically have to have a keen ear to differentiate

63
00:14:59,520 --> 00:15:03,520
the different technologies, like which is a Bash shell and which is a Bash script,

64
00:15:04,400 --> 00:15:13,200
especially depending on pronunciation, they can sound pretty much the same. We have some

65
00:15:14,000 --> 00:15:23,040
similar kind of stuff happening today with MPI and MP. Don't get worried about it. If

66
00:15:23,040 --> 00:15:28,080
something is unclear, just ask in the notes and we'll try to clarify the thoughts.

67
00:15:29,520 --> 00:15:32,480
Okay, and the last question, which task,

68
00:15:32,480 --> 00:15:34,440
not computing related,

69
00:15:34,440 --> 00:15:37,600
has involved the most number of synchronized people

70
00:15:37,600 --> 00:15:39,560
working on the same things?

71
00:15:39,560 --> 00:15:41,400
So it could be, for example,

72
00:15:42,360 --> 00:15:43,640
like what's the typical thing?

73
00:15:43,640 --> 00:15:46,680
These Olympic opening ceremonies or something

74
00:15:46,680 --> 00:15:50,280
where the countries try to get the most number of dancers

75
00:15:50,280 --> 00:15:52,740
just to put on a show,

76
00:15:52,740 --> 00:15:56,240
or it could be a sporting event,

77
00:15:56,240 --> 00:16:00,400
or what else involves a lot of synchronized people?

78
00:16:02,080 --> 00:16:05,160
There have been these shows, I think in Italy at least,

79
00:16:05,160 --> 00:16:07,960
where people are playing, like hundreds of people

80
00:16:07,960 --> 00:16:09,600
are playing like instruments together,

81
00:16:09,600 --> 00:16:14,560
like guitars or drums and rocking together,

82
00:16:14,560 --> 00:16:17,520
like a thousand people playing electric guitar

83
00:16:17,520 --> 00:16:18,440
at the same time.

84
00:16:22,440 --> 00:16:25,760
I think this summer there's going to be like a bass

85
00:16:25,760 --> 00:16:32,560
playing this kind of like show in Helsinki center that will involve like hundreds of

86
00:16:32,560 --> 00:16:38,560
bassists playing together with the same song so. Yeah there's an interesting comment there

87
00:16:38,560 --> 00:16:49,120
street fighting with the police I hope that was well I hope that's a joke and not actual protest

88
00:16:49,120 --> 00:16:51,840
in some country where that happens.

89
00:16:54,800 --> 00:16:59,680
Or maybe it's like a football activity or something like that.

90
00:16:59,680 --> 00:17:05,920
Ah, yeah. Is football hooliganism considered synchronized though?

91
00:17:09,440 --> 00:17:10,640
That's a good question, yeah.

92
00:17:10,640 --> 00:17:21,200
But anyway, yeah, protests are probably the most synchronized things I would imagine.

93
00:17:21,200 --> 00:17:28,520
I mean, I guess there's different kinds of synchronized, for example, a protest is people

94
00:17:28,520 --> 00:17:38,680
are working towards the same thing, but there's usually not a central leadership managing

95
00:17:38,680 --> 00:17:45,560
every little operation. Maybe getting people in one place, but it's separate. While some

96
00:17:45,560 --> 00:18:01,600
things like these Olympic Opening Ceremony would be... what's it called? Olympic Opening

97
00:18:01,600 --> 00:18:08,000
Ceremony. This is very strongly synchronized probably with a single leader that is guiding

98
00:18:08,000 --> 00:18:12,320
everyone like an orchestra so there's clearly one leader and everyone sees

99
00:18:12,320 --> 00:18:20,560
this person and is following their cues. Oh army is a good one yeah some big

100
00:18:20,560 --> 00:18:27,240
military exercises involve huge numbers of people and all kinds of complex

101
00:18:27,240 --> 00:18:36,500
logistics. Okay now the hard part try to make a metaphor of these tasks to the

102
00:18:36,500 --> 00:18:38,420
different parallel programming styles.

103
00:18:42,780 --> 00:18:44,220
I didn't think that far ahead.

104
00:18:45,820 --> 00:18:54,620
Like, I think, yeah, like, all of these are, like, some of them

105
00:18:54,620 --> 00:18:57,460
are more in sync than others.

106
00:18:57,500 --> 00:19:00,020
For example, like dancing, I would say is probably the most

107
00:19:00,020 --> 00:19:01,940
in sync out of these.

108
00:19:02,340 --> 00:19:05,780
And some of them are like just individuals doing stuff, but at

109
00:19:05,780 --> 00:19:09,380
same time. So some of these, like we're going to be talking

110
00:19:09,380 --> 00:19:14,220
about different methods of parallelization in the cluster

111
00:19:14,220 --> 00:19:17,740
environment. And some of these are more synchronous, and some

112
00:19:17,740 --> 00:19:22,220
of them aren't. And in many cases, probably throughout your

113
00:19:23,220 --> 00:19:25,620
your work where you will encounter situations where

114
00:19:25,620 --> 00:19:29,540
you're basically like middle manager for computer programs.

115
00:19:30,180 --> 00:19:34,660
So instead of like, like, you need to manage all of these

116
00:19:34,660 --> 00:19:41,300
different workers that try to do stuff for you. So basically you have your own small

117
00:19:42,260 --> 00:19:48,340
virtual office of workers and you try to like keep them in check and then there's always one

118
00:19:48,340 --> 00:19:53,860
of these programs that doesn't work correctly or something. You need to waste a lot of time

119
00:19:53,860 --> 00:20:01,940
trying to get it working. So we'll talk about many of these things throughout the day.

120
00:20:04,660 --> 00:20:20,020
I wish I could think of this metaphor to what we're going to talk about, but it's okay.

121
00:20:20,020 --> 00:20:27,980
Technoparty, yeah, there you've got some music playing which everyone's going for, or like

122
00:20:27,980 --> 00:20:29,620
dancing to it at the same time.

123
00:20:29,620 --> 00:20:32,620
Yeah.

124
00:20:32,620 --> 00:20:34,580
Well, should we begin?

125
00:20:34,580 --> 00:20:37,340
Yeah, sure.

126
00:20:37,340 --> 00:20:44,220
So are you going to play it to my screen or?

127
00:20:44,220 --> 00:20:44,940
Sure.

128
00:20:44,940 --> 00:20:52,060
Do you give a start of, do you start off with the day's intro?

129
00:20:52,060 --> 00:20:53,260
It's going to your screen now.

130
00:20:53,260 --> 00:20:54,700
You can do it.

131
00:20:54,700 --> 00:20:56,540
OK.

132
00:20:56,540 --> 00:21:00,020
Maybe I'll, I'm coming to my screen then.

133
00:21:00,020 --> 00:21:02,060
OK.

134
00:21:02,060 --> 00:21:03,540
Hey, it worked.

135
00:21:03,540 --> 00:21:05,780
OK.

136
00:21:05,780 --> 00:21:08,540
So yeah, we are beginning day three.

137
00:21:08,540 --> 00:21:10,700
And as you see from the schedule,

138
00:21:10,700 --> 00:21:12,260
it's all about Parallel.

139
00:21:12,260 --> 00:21:15,780
We start with a big picture here.

140
00:21:15,780 --> 00:21:19,660
And then we go to the different forms of Parallelism.

141
00:21:19,660 --> 00:21:23,660
We have a brief intermission where

142
00:21:23,660 --> 00:21:31,880
The CSC comes and gives a talk about their services.

143
00:21:31,880 --> 00:21:34,320
And then we talk about GPUs.

144
00:21:34,320 --> 00:21:47,160
And as usual, there's an Ask Us Anything session where we expect lots of good questions.

145
00:21:47,160 --> 00:21:52,360
So the hardest part about today isn't so much the commands we learn, which are similar,

146
00:21:52,360 --> 00:21:56,040
knowing what to use and how they fit with the programs.

147
00:21:56,040 --> 00:22:00,120
So one thing I've often seen there's some program it says it runs parallel

148
00:22:00,120 --> 00:22:04,760
but it's not clear about how it runs parallel and thus isn't clear how to

149
00:22:04,760 --> 00:22:09,160
run a parallel on our cluster. And that's the most important thing

150
00:22:09,160 --> 00:22:13,480
we can say and what we'll focus on. And of course as every day

151
00:22:13,480 --> 00:22:17,000
there's a lot of different...

152
00:22:17,160 --> 00:22:20,360
well we give a summary but you need to read yourself.

153
00:22:20,360 --> 00:22:27,800
So with that said, I guess we go to what does parallel mean? And I push it to [name]'s screen.

154
00:22:28,440 --> 00:22:40,280
There we go. So, hello. [name], parallel computing, what does it mean?

155
00:22:41,800 --> 00:22:50,280
Yeah, so like [name] said just previously, in many cases you might see a paper or something

156
00:22:50,280 --> 00:22:56,960
where somebody says that, okay, we executed, or especially in a talk where people don't

157
00:22:56,960 --> 00:23:03,080
have time to explain how they did a thing, they will say that, okay, we did a thing with

158
00:23:03,080 --> 00:23:07,720
a massive parallel simulation and we got these results.

159
00:23:07,720 --> 00:23:15,200
And then you're left wondering like, okay, how is it done actually behind it?

160
00:23:15,200 --> 00:23:23,120
And in many cases, there are these different ways that you can do different things, but

161
00:23:23,120 --> 00:23:29,760
they are not differentiated enough so that you can actually say simply that, okay, they

162
00:23:29,760 --> 00:23:32,680
use these kinds of ways.

163
00:23:32,680 --> 00:23:37,020
And there are multiple ways that you can do parallel computing.

164
00:23:37,020 --> 00:23:42,920
And parallel computing in a nutshell is basically just like you run multiple processes at the

165
00:23:42,920 --> 00:23:52,360
same time in some way that you can then get the results done faster. There are multiple ways of

166
00:23:52,360 --> 00:24:01,800
doing this, like parallel calculations. You can either do them completely independently,

167
00:24:02,440 --> 00:24:10,520
somewhat together, or you can do them completely in sync. We are going to be talking about these

168
00:24:10,520 --> 00:24:14,600
different methods of parallelization throughout the session.

169
00:24:17,880 --> 00:24:23,000
Right now, does anyone need to know what the methods are? Are we expecting any initial knowledge?

170
00:24:25,720 --> 00:24:33,880
Not really. Not really. I would just say as a historical footnote,

171
00:24:33,880 --> 00:24:43,800
If you were alive back in early 2000s or something around that time, there weren't really that

172
00:24:43,800 --> 00:24:47,160
many parallel processors in any of the computers.

173
00:24:47,160 --> 00:24:53,320
Most of the computing that was done, they were parallel clusters, of course, and vast

174
00:24:53,320 --> 00:24:59,480
majority of those were using one of the parallelism methods that we are going to be talking about,

175
00:24:59,480 --> 00:25:01,200
this MPI parallelism.

176
00:25:01,200 --> 00:25:10,060
But around the year 2000, you might have heard about Moore's law that tells about the number

177
00:25:10,060 --> 00:25:16,040
of transistors in a processor is going to double every two years.

178
00:25:16,040 --> 00:25:21,600
Well, as the processors went smaller and smaller, suddenly at some point there was a situation

179
00:25:21,600 --> 00:25:25,840
where there's not that much space anymore for the transistors.

180
00:25:25,840 --> 00:25:32,640
What many processor manufacturers started to do is to add multiple processors into their computers,

181
00:25:33,280 --> 00:25:40,560
like into the CPUs. So your phone might nowadays have like maybe eight processors,

182
00:25:40,560 --> 00:25:43,840
and your laptop might have like eight or something like that as well.

183
00:25:44,880 --> 00:25:52,160
So this change happened, but the programs didn't necessarily change. So this is kind of like

184
00:25:52,160 --> 00:26:00,020
Like, lots of modern programs already are parallel, but lots of older ones might not

185
00:26:00,020 --> 00:26:01,020
yet be.

186
00:26:01,020 --> 00:26:06,340
And we'll see how these different methods happen.

187
00:26:06,340 --> 00:26:15,900
So basically, you're saying that processors haven't gotten much more faster than 20 years

188
00:26:15,900 --> 00:26:18,020
ago, but there's more of them.

189
00:26:18,020 --> 00:26:20,460
So we have to be able to use more.

190
00:26:20,460 --> 00:26:21,460
Yeah.

191
00:26:21,460 --> 00:26:27,780
They have gotten faster, but at the same time, they haven't gotten as fast as, let's say,

192
00:26:27,780 --> 00:26:31,780
from 60s to 80s or something like that.

193
00:26:31,780 --> 00:26:35,620
But there are more of them, a lot more of them nowadays.

194
00:26:35,620 --> 00:26:41,940
So if we look at the picture in the screen, we can see that basically in the compute cluster,

195
00:26:41,940 --> 00:26:44,940
we of course have lots of processors.

196
00:26:44,940 --> 00:26:49,700
So the Triton cluster has about 10,000 processors.

197
00:26:49,700 --> 00:26:56,860
So yesterday we were talking what is a core, what is a CPU, well in the case of the computing

198
00:26:56,860 --> 00:27:04,020
cluster, there are cores of individual CPUs, but it doesn't really matter.

199
00:27:04,020 --> 00:27:07,900
You can just think of it like something that can do computations.

200
00:27:07,900 --> 00:27:12,100
We have 10,000 things that can do computations.

201
00:27:12,100 --> 00:27:18,180
And if you want to utilize them, we have to try to utilize them in parallel usually to

202
00:27:18,180 --> 00:27:21,620
get the most out of the system.

203
00:27:21,620 --> 00:27:24,740
So should we go through the different methods quickly?

204
00:27:24,740 --> 00:27:25,620
OK, let's see.

205
00:27:25,620 --> 00:27:28,620
Can you explain them to me?

206
00:27:28,620 --> 00:27:32,260
Pretend I'm a new undergrad student

207
00:27:32,260 --> 00:27:34,420
and don't know much about computers.

208
00:27:34,420 --> 00:27:36,780
So in various linking parallel, what does that mean?

209
00:27:36,780 --> 00:27:37,780
Yeah.

210
00:27:37,780 --> 00:27:42,780
Yeah, let's look at the pretty picture about this.

211
00:27:42,780 --> 00:27:48,780
So, in this picture, is each red box a single thing that's running?

212
00:27:49,980 --> 00:27:57,340
Yes. So, if we think about the cluster that is compromised of different computers and

213
00:27:57,340 --> 00:28:03,020
different CPUs, you can think of each of these blocks as, let's say, one core that can take one

214
00:28:03,020 --> 00:28:15,260
job each. In the case of an array job, we have this kind of parallelism called an array job.

215
00:28:15,260 --> 00:28:22,860
And what that means is that it's basically like a situation where if you want to do something

216
00:28:22,860 --> 00:28:51,860
So if you want to do a thing, like let's say I want to cook spaghetti and then I want to cook a different kind of a pasta at the same time, I can do them separately in two different burners, like I don't necessarily have to, you know, I don't have to do the same thing over and over again.

217
00:28:51,860 --> 00:28:58,220
I don't necessarily have to put them into the same pot or something like that.

218
00:28:58,220 --> 00:29:02,680
So I have, so it's like I want to make five pots of pasta.

219
00:29:02,680 --> 00:29:07,180
I can get five people and do it separately and that works perfectly.

220
00:29:07,180 --> 00:29:08,180
Yes.

221
00:29:08,180 --> 00:29:14,420
So if you have a thing that you can like split up in some way, let's say you have a simulation

222
00:29:14,420 --> 00:29:19,980
and you have different parameters for that, you can embarrassingly parallel that or run

223
00:29:19,980 --> 00:29:25,660
in an embarrassingly parallel way. There was another term that somebody used that was

224
00:29:27,020 --> 00:29:33,660
easily parallelizable or something like that. That basically means that you can split it

225
00:29:34,700 --> 00:29:41,900
in some natural way, the computation. You run a different experiment in a different calculation.

226
00:29:41,900 --> 00:29:47,180
You can split it among different data sets, different models, something like that. Something

227
00:29:47,180 --> 00:29:53,660
that is very easy to split up and what in the embarrassingly parallel way that utilizes these

228
00:29:53,660 --> 00:30:00,220
array jobs you can like spread them out throughout the cluster or in different places and you don't

229
00:30:00,220 --> 00:30:06,060
necessarily have to worry where they end up they just get done and the question is more about okay

230
00:30:06,060 --> 00:30:13,100
how can you manage these jobs so that they can get to the queue how can you put all of your

231
00:30:13,100 --> 00:30:22,300
different parameters into the queue. So in the picture here in below, you can see that,

232
00:30:22,300 --> 00:30:28,380
let's say we have one of these array jobs that we would submit, some of them might get into

233
00:30:28,380 --> 00:30:35,500
one computer and one could get into a different computer and they would be completely independent

234
00:30:35,500 --> 00:30:42,940
of each other. And there was a great question in the notes that do they all have their own

235
00:30:42,940 --> 00:30:49,740
copies of variables. And yes, they would all be independent. So, basically, there would be

236
00:30:49,740 --> 00:30:55,820
something to differentiate them, like some sort of different task that they are supposed to do,

237
00:30:55,820 --> 00:30:58,940
but they would all run completely independent of each other.

238
00:30:58,940 --> 00:31:06,540
Yeah. Okay. I think I understand this. So, what's the next version?

239
00:31:06,540 --> 00:31:11,540
Yeah. So the next version is this so-called shared memory parallelism.

240
00:31:11,540 --> 00:31:15,540
So this is basically what your laptop is doing, like if you're running something.

241
00:31:15,540 --> 00:31:22,540
So, for example, my Zoom that I'm talking to [name] and to you.

242
00:31:22,540 --> 00:31:23,540
Right.

243
00:31:23,540 --> 00:31:30,540
Utilizing most likely multiple processors on my computer to encode the video and do all kinds of stuff.

244
00:31:30,540 --> 00:31:40,540
And what in the shared memory parallelism, what we do is that we have a process that runs on a computer.

245
00:31:40,540 --> 00:31:49,540
And that computer, that process can like spawn multiple other processes and they can work together in the same computer.

246
00:31:49,540 --> 00:31:52,540
And why is it called shared memory parallelism?

247
00:31:52,540 --> 00:31:58,360
parallelism is that, well, they are physically in the same machine, like they are working

248
00:31:58,360 --> 00:32:03,840
like the processes are in the same machine, so they can interact with each other through

249
00:32:03,840 --> 00:32:08,000
the shared memory, the shared random access memory they have.

250
00:32:08,000 --> 00:32:15,320
So in the image, in the top, you can see that in the shared memory parallelism, we want

251
00:32:15,320 --> 00:32:24,360
all of the processes in the same physical machine so that they have access to that shared memory.

252
00:32:24,360 --> 00:32:29,960
So that's like when someone in the note says do parallel runs all have their same copy of

253
00:32:29,960 --> 00:32:36,120
the variables. So here they do all have the same copy because they access the same memory space.

254
00:32:37,160 --> 00:32:42,120
Yes, of course they can have their own local variables. Each process can have their own

255
00:32:42,120 --> 00:32:49,480
local variables. But the performance comes in the idea that, okay, they can share information

256
00:32:49,480 --> 00:32:56,760
with each other and they can do a calculation together. So if you look at the picture at the

257
00:32:56,760 --> 00:33:03,320
bottom, you might have a program, like a main program, and that main program might start

258
00:33:03,320 --> 00:33:11,400
multiple processes that each work in a different CPU, and they communicate through the shared

259
00:33:11,400 --> 00:33:19,720
memory. This is commonly used, for example, with Python multiprocessing. In R, if you're using

260
00:33:21,640 --> 00:33:29,160
parallel. In MATLAB, if you're using parallel pool, those all utilize this sort of a parallelism

261
00:33:30,120 --> 00:33:36,360
and various other technologies. One technology is this technology called OpenMP,

262
00:33:36,360 --> 00:33:45,080
So open multiprocessing. And it's used in many kinds of libraries underneath many languages.

263
00:33:45,720 --> 00:33:53,320
So for example, if you're using Python, there's this NumPy library that utilizes these OpenMP

264
00:33:56,120 --> 00:34:02,040
parallelism to do these calculations automatically with multiple CPUs.

265
00:34:02,040 --> 00:34:05,960
So I'm trying to think of the metaphor here. So now I have five people cooking pasta,

266
00:34:05,960 --> 00:34:12,280
but they're in the same kitchen. And if we want to share the salt or something, I can put it down

267
00:34:12,280 --> 00:34:18,040
on the counter and someone else can just pick it up and use it. So we all have the same workspace

268
00:34:18,760 --> 00:34:26,200
or something like that. Yeah, that's a great analogy. Okay, so what's the next one?

269
00:34:27,240 --> 00:34:32,360
Yeah, so the next one is this, what I already mentioned, this historically

270
00:34:32,360 --> 00:34:40,200
used parallelism that is used especially in like the supercomputer kind of a thing,

271
00:34:41,160 --> 00:34:48,120
supercomputer systems. And this is called message passing interface parallelism or MPI parallelism.

272
00:34:48,120 --> 00:34:55,640
So if you have heard about MPI, this is the thing. So in the message passing interface

273
00:34:55,640 --> 00:35:04,640
or MPI parallelism, you have a program that consists of multiple programs.

274
00:35:04,640 --> 00:35:10,640
So basically, you start, let's say, a thousand copies of the same program on different computers.

275
00:35:10,640 --> 00:35:17,640
And each of those programs knows that, okay, I'm a part of some greater whole.

276
00:35:17,640 --> 00:35:23,640
I'm a part of this great MPI program, and I need to work together with my…

277
00:35:23,640 --> 00:35:34,920
It's basically like an anthill, like everybody's working together for the common good.

278
00:35:34,920 --> 00:35:42,000
In my pasta analogy, so now we can't directly share things, we have to communicate.

279
00:35:42,000 --> 00:35:47,000
So is it like we have multiple cooks in different apartments, and if I make something that someone

280
00:35:47,000 --> 00:35:51,620
else needs, someone has to get up there, go to the other place, knock on the door, say

281
00:35:51,620 --> 00:35:57,940
say, okay, I made this for you, and they answer and receive it, and then we go back or something.

282
00:35:57,940 --> 00:35:58,940
Yeah.

283
00:35:58,940 --> 00:35:59,940
Yeah.

284
00:35:59,940 --> 00:36:01,480
That would be a good analogy.

285
00:36:01,480 --> 00:36:06,480
So what the MPI programs do is that when you start them up, they create this communication

286
00:36:06,480 --> 00:36:11,420
layer so that they can like, they can share information through each other.

287
00:36:11,420 --> 00:36:16,280
And they all like, usually what you would have is that, let's say one process would

288
00:36:16,280 --> 00:36:24,680
have its own, let's say, simulation range or certain area of the simulation that they

289
00:36:24,680 --> 00:36:26,160
would handle.

290
00:36:26,160 --> 00:36:32,360
And what they would communicate to the other processes are the edge regions of that simulation

291
00:36:32,360 --> 00:36:33,360
area.

292
00:36:33,360 --> 00:36:38,680
So that they would do minimal amount of communication with each other, but they would do stuff collectively

293
00:36:38,680 --> 00:36:43,160
to simulate, let's say, a weather model or something like that.

294
00:36:43,160 --> 00:36:48,920
Okay yeah so like you divide it up into different domains like physical domains and each edge

295
00:36:48,920 --> 00:36:56,040
condition can do things. Yeah. So what is MPI most often used for? It seems a bit harder to use or

296
00:36:56,040 --> 00:37:04,760
program. Yes so it is much more complicated because the program usually needs to be designed

297
00:37:04,760 --> 00:37:10,840
around the communication also like the algorithms that are used. It's quite often used in like

298
00:37:10,840 --> 00:37:19,320
physics simulations. So, for example, like CP2K, GPAW, LAMMPS, OpenFOAM, many weather models,

299
00:37:19,320 --> 00:37:27,080
like all weather models usually do that sort of stuff. And you usually need to build the program

300
00:37:27,080 --> 00:37:34,680
around the communication layer. So, basically, you need to design your program to be an MPI program.

301
00:37:34,680 --> 00:37:41,560
So compared to like the multi-processing thing or the shared memory parallelism

302
00:37:41,560 --> 00:37:47,720
where the programs are usually like you can just like quite trivially usually

303
00:37:48,360 --> 00:37:52,920
use them in different kinds of contexts, the MPI programs are usually like you need to

304
00:37:52,920 --> 00:37:58,600
from ground up design the program to be an MPI program. But it's very powerful when you

305
00:37:58,600 --> 00:38:03,000
design a program like that because then you can utilize these supercomputers.

306
00:38:03,000 --> 00:38:07,240
Yeah. Okay. Are there any more methods?

307
00:38:08,680 --> 00:38:09,480
Well, the last one...

308
00:38:09,480 --> 00:38:11,080
Oh, yeah, GPUs. I see.

309
00:38:11,080 --> 00:38:13,240
Yeah, you might have heard is GPU

310
00:38:14,760 --> 00:38:19,880
parallelism. And it's a different kind of like a parallelism, basically, because

311
00:38:19,880 --> 00:38:28,440
like in a GPU, you have like internal parallelism in the GPU card itself. So this graphics processing

312
00:38:28,440 --> 00:38:37,000
unit. They are basically like big vector calculators that can do a lot of calculations

313
00:38:37,000 --> 00:38:46,280
in parallel. Let's say they can multiply a matrix really fast, but they are in the GPU.

314
00:38:47,960 --> 00:38:55,400
Usually, when you have a GPU program, it's parallel in a sense that you have a CPU part

315
00:38:55,400 --> 00:39:02,000
And then usually that CPU part tells the GPU program, okay, here's some, let's say, matrix

316
00:39:02,000 --> 00:39:07,400
you need to multiply, and here's another matrix that you need to multiply with.

317
00:39:07,400 --> 00:39:14,500
And then they are transferred into the GPUs, this VRAM, so this video RAM.

318
00:39:14,500 --> 00:39:17,500
The GPUs have their own fast memory that they can access.

319
00:39:17,500 --> 00:39:25,840
And then these individual calculating units, they're often called like CUDA cores or compute

320
00:39:25,840 --> 00:39:27,960
units or something like that.

321
00:39:27,960 --> 00:39:30,920
They do the calculation in parallel.

322
00:39:30,920 --> 00:39:38,360
So they work basically like this kind of like a fast calculator for these kinds of programs

323
00:39:38,360 --> 00:39:45,280
that can do a lot of stuff in parallel.

324
00:39:45,280 --> 00:39:51,480
But they don't, and of course, there's cases where you also have programs where the calculations

325
00:39:51,480 --> 00:39:57,480
are done using, let's say, some communication so that you can have multiple GPUs working

326
00:39:57,480 --> 00:39:58,480
together.

327
00:39:58,480 --> 00:40:04,120
But usually, the GPU parallelism is internal to the program structure.

328
00:40:04,120 --> 00:40:09,600
And that also means that your program needs to be designed to use GPUs or, well, it can't

329
00:40:09,600 --> 00:40:10,600
use the GPUs.

330
00:40:10,600 --> 00:40:15,600
So, what's a good metaphor here with my pasta analogy?

331
00:40:15,600 --> 00:40:28,480
Is it like, you have one of these, well, if you want to, let's say you want to make the

332
00:40:28,480 --> 00:40:34,460
pasta sauce and you have like an onion slicer or something like that, you just can slice

333
00:40:34,460 --> 00:40:39,680
the whole onion with one go, but you cannot really cook pasta with that really, like you

334
00:40:39,680 --> 00:40:42,880
You can, it does that one thing in parallel.

335
00:40:42,880 --> 00:40:44,560
Instead of chopping with the knife,

336
00:40:44,560 --> 00:40:49,040
you can just chop a full onion at one time.

337
00:40:49,040 --> 00:40:52,200
Actually, I could show my slide with the GPU metaphor

338
00:40:52,200 --> 00:40:53,840
for the pasta thing.

339
00:40:53,840 --> 00:40:54,520
OK.

340
00:40:54,520 --> 00:40:57,040
I'll switch to my screen here.

341
00:41:01,440 --> 00:41:02,680
There.

342
00:41:02,680 --> 00:41:05,280
So this is part of some other thing.

343
00:41:05,280 --> 00:41:08,600
But in the cooking metaphor, so what we see here

344
00:41:08,600 --> 00:41:16,040
is from some restaurant in Switzerland where they're cooking a bunch of chickens on a fire.

345
00:41:17,080 --> 00:41:24,200
And here it's designed where you could cook probably 20 chickens at once. And sorry for

346
00:41:24,200 --> 00:41:30,760
vegetarians, I am too, but this is, well, the metaphor I found. So with this, with very little

347
00:41:30,760 --> 00:41:38,440
effort and quite good efficiency, we can cook so much food at once. But how much pasta can we cook

348
00:41:38,600 --> 00:41:39,600
None.

349
00:41:39,600 --> 00:41:40,600
Not that many.

350
00:41:40,600 --> 00:41:41,600
Yeah.

351
00:41:41,600 --> 00:41:51,480
So the GPU is intrinsically suitable for some types of problems, but not all types.

352
00:41:51,480 --> 00:41:56,400
So if it matches the GPU model, you can do a whole lot.

353
00:41:56,400 --> 00:42:01,160
But if it doesn't, well, it just doesn't run there.

354
00:42:01,160 --> 00:42:12,040
With the GPUs, because why the GPUs are powerful, why they are like that, they originate, of

355
00:42:12,040 --> 00:42:15,680
course, for historical reasons, designed for computer graphics.

356
00:42:15,680 --> 00:42:21,880
And if you do 3D computer graphics, you need to do a lot of rotations and that sort of

357
00:42:21,880 --> 00:42:27,720
stuff in order to calculate vector graphics in the computer programs.

358
00:42:27,720 --> 00:42:33,320
And that's why the computers or the chips of the GPUs were designed to do lots of these

359
00:42:33,320 --> 00:42:35,240
matrix calculations.

360
00:42:35,240 --> 00:42:43,600
And then afterwards, people realized that, hey, what if we use these really good matrix

361
00:42:43,600 --> 00:42:47,560
calculation units to do all kinds of other computations?

362
00:42:47,560 --> 00:42:52,520
And then when, let's say, the deep learning boom started, people realized, okay, we can

363
00:42:52,520 --> 00:42:56,480
run these models in GPUs very efficiently.

364
00:42:56,480 --> 00:43:04,040
And just today I saw news that NVIDIA has surpassed Apple in the stock price.

365
00:43:04,040 --> 00:43:09,960
So they are basically like the GPU trend is ongoing and it's going really fast because

366
00:43:09,960 --> 00:43:17,400
these can be utilized as long as you can formulate your program in this kind of like a matrix

367
00:43:17,400 --> 00:43:22,520
kind of a way, you can usually utilize GPUs to do these fast calculations.

368
00:43:22,520 --> 00:43:28,840
and that's why you said it has to be completely designed around the GPU basically from the start.

369
00:43:31,480 --> 00:43:37,160
So before we move forward to the actual first examples and first exercise,

370
00:43:40,120 --> 00:43:46,760
I want to talk a bit about what is serial and what is parallel in your code. So not every

371
00:43:46,760 --> 00:43:55,020
Every program parallelizes with the GPU parallelization or the shared memory parallelization, because

372
00:43:55,020 --> 00:44:04,280
every program by design or by maths, basically, needs to have a serial part or it has some

373
00:44:04,280 --> 00:44:06,080
part that is serial part.

374
00:44:06,080 --> 00:44:08,120
And then you have some parallel part.

375
00:44:08,120 --> 00:44:13,840
And if you parallelize the parallel part, you can get different amounts of speed up

376
00:44:13,840 --> 00:44:19,760
based on well how much of the program execution was done in parallel. So for example in the first

377
00:44:19,760 --> 00:44:27,360
picture you can see that there's only a small parallel part and if you parallelize that with

378
00:44:27,360 --> 00:44:32,400
two CPUs so you basically divide it by half and then you run it with two CPUs you only save a

379
00:44:32,400 --> 00:44:39,920
small amount of time but if you have like a larger parallel part and you parallelize it you get a

380
00:44:39,920 --> 00:44:46,640
bigger time saver. So it depends on your program. And this is why the embarrassingly parallel is

381
00:44:46,640 --> 00:44:54,800
often the best case. Parallelism in many cases. Because not every problem is easy to parallelize

382
00:44:54,800 --> 00:45:05,440
in these more advanced ways. And it's not a problem. Not everything goes faster by adding

383
00:45:05,440 --> 00:45:11,680
more power to it. So, for example, like Ferrari doesn't go faster in Finnish roads because

384
00:45:11,680 --> 00:45:16,880
there's speed limits. Like, you can use an older car as well.

385
00:45:16,880 --> 00:45:22,640
What about this metaphor? So, with the pasta thing again. So, if I have two people making

386
00:45:22,640 --> 00:45:28,960
pasta in my house, it's probably still okay. Actually, this metaphor isn't perfect. But

387
00:45:28,960 --> 00:45:35,440
Let's say I have 50 people making pasta in my home, or in one home, and it has one sink.

388
00:45:35,440 --> 00:45:40,960
Eventually, everyone's going to need to use the sink, and there'll be a queue there.

389
00:45:40,960 --> 00:45:42,600
That's actually not correct.

390
00:45:42,600 --> 00:45:48,680
Yeah, but if you, like, let's say, like, if you break the pasta in half, like, if you

391
00:45:48,680 --> 00:45:52,320
break the pasta in half and cook it in two pots, does it cook any faster?

392
00:45:52,320 --> 00:45:54,800
Like, do you get it done any faster?

393
00:45:54,800 --> 00:46:00,280
Most likely not, because the pasta has a set cooking time that will always, like pasta

394
00:46:00,280 --> 00:46:07,000
in a sense is always, it will only cook for certain minutes.

395
00:46:07,000 --> 00:46:15,840
You cannot make it faster by spreading, putting it into multiple pots, usually at least.

396
00:46:15,840 --> 00:46:21,760
So that's good to keep in mind when doing parallel computations.

397
00:46:21,760 --> 00:46:24,080
So should we start doing them?

398
00:46:24,080 --> 00:46:25,600
There's one more question, though.

399
00:46:25,600 --> 00:46:31,920
So it's a question, running same simulation,

400
00:46:31,920 --> 00:46:34,880
different random sets, inside each simulation,

401
00:46:34,880 --> 00:46:38,920
I use a GPU for large matrix multiplications.

402
00:46:38,920 --> 00:46:41,400
Each simulation is in a different CPU,

403
00:46:41,400 --> 00:46:44,360
but how to handle the GPU part?

404
00:46:44,360 --> 00:46:51,000
So this is somehow related to the combining

405
00:46:51,000 --> 00:46:52,520
different forms of parallelism.

406
00:46:52,520 --> 00:46:53,020
Yes.

407
00:46:53,020 --> 00:47:06,780
it's not exactly what's there, but yeah. Yeah, so one thing that you can remember is that you can

408
00:47:06,780 --> 00:47:12,060
combine these different methods of parallelization. You can combine, like you can run, if you have

409
00:47:12,060 --> 00:47:17,420
multiple different simulations, you can run this kind of an array job, this embarrassingly parallel

410
00:47:17,420 --> 00:47:28,060
simulation with each job so that each job has a GPU, if your code uses GPUs.

411
00:47:28,060 --> 00:47:35,060
In that case, if you have a simulation and then you need to use a GPU to do some part

412
00:47:35,060 --> 00:47:41,700
of that simulation, there are methods for that, but it requires a bit more finesse so

413
00:47:41,700 --> 00:47:46,420
that the different CPUs can communicate with the GPU.

414
00:47:46,420 --> 00:47:51,220
In many cases, you might have a situation where you have one processor that discusses

415
00:47:51,220 --> 00:47:58,820
with the GPU, and the other processors are doing the simulation.

416
00:47:58,820 --> 00:48:04,700
So you have basically a shared memory part of the simulations, and then you have one

417
00:48:04,700 --> 00:48:08,540
part that does the matrix multiplication with the GPU.

418
00:48:08,540 --> 00:48:15,980
So you have basically these two linked by one CPU, but this is a special case.

419
00:48:15,980 --> 00:48:22,860
But do remember that it's possible to combine all of these methods together.

420
00:48:22,860 --> 00:48:27,980
And if you scroll down the page more, it finally says the research software engineers can help

421
00:48:27,980 --> 00:48:30,020
with parallel computing.

422
00:48:30,020 --> 00:48:31,700
And this is something I'd really recommend.

423
00:48:31,700 --> 00:48:35,100
I mean, these things are confusing.

424
00:48:35,100 --> 00:48:40,220
Even when I'm doing things, I'll often ask [name] just to make sure that I'm running it

425
00:48:40,220 --> 00:48:42,860
correctly or to save me some time.

426
00:48:42,860 --> 00:48:47,220
So if you get to the point where you wonder how it works,

427
00:48:47,220 --> 00:48:49,820
and you're at Aalto University, we're here.

428
00:48:49,820 --> 00:48:53,540
And our job is to save you time figuring out this stuff.

429
00:48:53,540 --> 00:48:56,420
So come by and ask.

430
00:48:56,420 --> 00:48:59,300
I will also mention that it's always a good idea

431
00:48:59,300 --> 00:49:01,340
before creating your own, let's say,

432
00:49:01,340 --> 00:49:03,300
computing framework or something.

433
00:49:03,300 --> 00:49:05,660
It's always good to check if somebody else has done it

434
00:49:05,660 --> 00:49:12,500
before you, because many of these parallel things

435
00:49:12,500 --> 00:49:16,220
are complicated to write, so you don't usually

436
00:49:16,220 --> 00:49:17,380
want to write your own.

437
00:49:17,380 --> 00:49:20,660
There's usually, for example, for R,

438
00:49:20,660 --> 00:49:22,020
there's the parallel package.

439
00:49:22,020 --> 00:49:24,260
And for Python, there's multiple,

440
00:49:24,260 --> 00:49:28,860
like joblib, and there's parallel.

441
00:49:28,860 --> 00:49:30,660
My many parallel processing libraries

442
00:49:30,660 --> 00:49:33,340
that already do this kind of stuff

443
00:49:33,340 --> 00:49:34,740
so that you don't necessarily have

444
00:49:34,740 --> 00:49:37,540
to worry about how it's done underneath it.

445
00:49:37,540 --> 00:49:41,060
So keep that in mind.

446
00:49:41,060 --> 00:49:46,980
whenever using, it's a good idea to use code because like writing the actual like communication

447
00:49:46,980 --> 00:49:53,300
and that sort of stuff is super complicated. So most people that are listening to this

448
00:49:53,300 --> 00:49:58,820
won't be writing their own parallel code but they'll be using existing libraries or frameworks

449
00:49:58,820 --> 00:50:06,900
and yeah I would recommend at least that. You can always write your own but

450
00:50:06,900 --> 00:50:12,860
But yeah, so should we move forward to the array jobs

451
00:50:12,860 --> 00:50:15,500
and actually get to do something?

452
00:50:15,500 --> 00:50:19,220
So I'll flip back to my screen.

453
00:50:19,220 --> 00:50:22,060
Yes.

454
00:50:22,060 --> 00:50:29,300
OK, so here we are, to array jobs.

455
00:50:32,780 --> 00:50:35,420
So I guess we go like before.

456
00:50:35,420 --> 00:50:44,780
So let's look at the abstract here.

457
00:50:44,780 --> 00:50:46,420
So what's an array job?

458
00:50:46,420 --> 00:50:50,140
So array makes me think a bunch of things in parallel.

459
00:50:50,140 --> 00:50:54,820
And from before, I know it's somehow related to embarrassing parallel.

460
00:50:54,820 --> 00:50:57,500
So how would you explain it?

461
00:50:57,500 --> 00:50:58,500
Yeah.

462
00:50:58,500 --> 00:51:02,620
So an array is basically like what the name says.

463
00:51:02,620 --> 00:51:06,060
It's like a list of numbers, basically.

464
00:51:06,060 --> 00:51:10,220
And in this case, let's say we want

465
00:51:10,220 --> 00:51:13,860
to do a simulation with multiple different data sets.

466
00:51:13,860 --> 00:51:15,060
We have data set one.

467
00:51:15,060 --> 00:51:18,020
We have data set two, data set three, and so forth.

468
00:51:18,020 --> 00:51:20,500
We want to do the same simulation with we

469
00:51:20,500 --> 00:51:24,300
have something that we want to iterate over, basically.

470
00:51:24,300 --> 00:51:26,140
And all of these simulations would

471
00:51:26,140 --> 00:51:28,480
be independent of each other.

472
00:51:28,480 --> 00:51:34,240
So we have independent simulations that we can do, but we just want to do them all.

473
00:51:34,240 --> 00:51:41,320
And what we could do is we could write our Sbatch script for each simulation at a time.

474
00:51:41,320 --> 00:51:48,480
So we would have like n Sbatch scripts to run like serial jobs.

475
00:51:48,480 --> 00:51:56,240
But what we can do in Slurm, there's this feature called array, where you can give this

476
00:51:56,240 --> 00:52:02,400
slurm comment that we previously already see called array and give it a list of numbers or

477
00:52:02,400 --> 00:52:10,800
different kinds of numbers and then it will launch basically one job that consists of

478
00:52:10,800 --> 00:52:14,960
multiple of these independent jobs. So basically it launches copies of the same job

479
00:52:16,320 --> 00:52:25,600
but with the key difference that each job gets one number different. So each job gets its own

480
00:52:25,600 --> 00:52:31,920
Slurm_Array_Task_Id. So, each one gets its own environment variable, which is the number

481
00:52:31,920 --> 00:52:40,960
of that job's number in that array. So, that job's number in that list of jobs.

482
00:52:40,960 --> 00:52:49,680
Yeah. So, this variable right here, this runs 10 times with input underscore one,

483
00:52:49,680 --> 00:52:57,440
input underscore 2 and so on up to 10. Okay. So when the job is running, if we look at the

484
00:52:57,440 --> 00:53:04,400
minimal example script over here, when the job is running, that number, the slurm array task ID,

485
00:53:04,400 --> 00:53:10,880
will go from numbers 1 to 10 because we have specified there in the comment that we want

486
00:53:10,880 --> 00:53:18,880
them to run an array from 1 to 10. That number will change. That number then we can, for example,

487
00:53:18,880 --> 00:53:24,800
In that code, we can run input one, input two, input three, and they all run independently

488
00:53:24,800 --> 00:53:32,560
at the same time. The Slurm will try to allocate all of those into the queue, and they will run

489
00:53:32,560 --> 00:53:37,920
independent of each other in different computers, different nodes, and whatever.

490
00:53:37,920 --> 00:53:43,440
All of the other requirements that we have specified, if we require a certain amount of

491
00:53:43,440 --> 00:53:47,920
of memory, certain amount of time, whatever, they are copied as well.

492
00:53:47,920 --> 00:53:57,160
The rest of the script is basically copied between all of the jobs, but the array index

493
00:53:57,160 --> 00:53:58,160
is just different.

494
00:53:58,160 --> 00:53:59,160
Yeah.

495
00:53:59,160 --> 00:54:05,960
So I run sbatch once, but 10 things come out the other end.

496
00:54:05,960 --> 00:54:06,960
Yes.

497
00:54:06,960 --> 00:54:07,960
Yes, basically.

498
00:54:07,960 --> 00:54:10,320
So we can do this.

499
00:54:10,320 --> 00:54:16,960
What you can do then, or what is the question usually, is that, okay, you have a number,

500
00:54:16,960 --> 00:54:24,000
like the array is a number, and how can you convert that number into something else, let's

501
00:54:24,000 --> 00:54:25,880
say a parameter or something.

502
00:54:25,880 --> 00:54:27,960
We'll go through a few examples of this.

503
00:54:27,960 --> 00:54:28,960
Yeah.

504
00:54:28,960 --> 00:54:29,960
Yeah.

505
00:54:29,960 --> 00:54:30,960
Maybe we could-

506
00:54:30,960 --> 00:54:31,960
Should we do the example?

507
00:54:31,960 --> 00:54:32,960
Yeah.

508
00:54:32,960 --> 00:54:33,960
Let's do an example.

509
00:54:33,960 --> 00:54:34,960
Okay.

510
00:54:34,960 --> 00:54:36,960
And run that.

511
00:54:36,960 --> 00:54:39,160
There's some good questions in the notes.

512
00:54:39,160 --> 00:54:47,000
we can answer those first. Simulation is relatively fast. So yeah, if the array jobs take

513
00:54:47,880 --> 00:54:55,080
a few minutes or less, then it's better not to have thousands of them. Try to group them together.

514
00:54:55,080 --> 00:55:00,840
And there's some hints about this later in the page. It's exactly like an overall for loop

515
00:55:00,840 --> 00:55:08,360
kind of thing. The number assigned to the array randomly done. I'm not sure exactly what that

516
00:55:08,360 --> 00:55:14,760
means. So the Slurm_Array_Task_IDs come exactly from this here.

517
00:55:17,240 --> 00:55:24,680
And you can skip values. You can choose different number ranges. You can do it like

518
00:55:24,680 --> 00:55:30,120
you can do like every fifth number or something like that. There's different

519
00:55:30,120 --> 00:55:37,480
syntaxes you can use to which numbers you want to use. But you will get like each of these jobs

520
00:55:37,480 --> 00:55:43,720
will get one of those numbers that you have specified in the dash dash array comment.

521
00:55:44,920 --> 00:55:51,960
The array jobs result in multiple sessions. Yes, so they can run at the same time,

522
00:55:51,960 --> 00:55:56,120
but they might run at different times, and they're all completely independent

523
00:55:56,120 --> 00:55:58,360
and have no knowledge of each other.

524
00:55:58,360 --> 00:55:59,360
Yes.

525
00:55:59,360 --> 00:56:00,360
Okay.

526
00:56:00,360 --> 00:56:01,360
Let's go on.

527
00:56:01,360 --> 00:56:02,360
So, if I scroll down.

528
00:56:02,360 --> 00:56:03,360
Yeah.

529
00:56:03,360 --> 00:56:06,360
Let's look at that example.

530
00:56:06,360 --> 00:56:08,360
Where's the first example we get?

531
00:56:08,360 --> 00:56:09,360
My first array job.

532
00:56:09,360 --> 00:56:10,360
Yeah.

533
00:56:10,360 --> 00:56:12,360
This seems like the place to start, right?

534
00:56:12,360 --> 00:56:13,360
Yes.

535
00:56:13,360 --> 00:56:14,360
Okay.

536
00:56:14,360 --> 00:56:15,360
Yeah.

537
00:56:15,360 --> 00:56:16,360
If we look at the script.

538
00:56:16,360 --> 00:56:22,360
So, if we look at the script over here, we have, like, we have the usual suspects.

539
00:56:22,360 --> 00:56:23,360
So, we have the first line.

540
00:56:23,360 --> 00:56:27,360
We have the shebang, shebang, how it's pronounced.

541
00:56:27,360 --> 00:56:37,360
And then we have a few of these comments, so if [name], you want to open our editor and copy the script there.

542
00:56:37,360 --> 00:56:42,360
I've just realized I haven't cloned the HPC Examples repository or gotten set up.

543
00:56:42,360 --> 00:56:43,360
Yeah.

544
00:56:43,360 --> 00:56:44,360
Okay.

545
00:56:44,360 --> 00:56:45,360
Yeah.

546
00:56:45,360 --> 00:56:58,360
Well, in the meantime, I can, while [name] is copying the repository, I can, oh, yeah,

547
00:56:58,360 --> 00:56:59,360
yeah.

548
00:56:59,360 --> 00:57:03,600
Actually, for this, you don't need that, yeah.

549
00:57:03,600 --> 00:57:05,360
The next ones, maybe, yes.

550
00:57:18,680 --> 00:57:19,180
OK.

551
00:57:23,680 --> 00:57:26,440
What?

552
00:57:26,440 --> 00:57:26,940
Yes.

553
00:57:26,940 --> 00:57:38,820
So, in the script, we have the time and the memory requirements at the start.

554
00:57:38,820 --> 00:57:46,460
So these are the usual suspects that we always want to give anyways.

555
00:57:46,460 --> 00:57:48,420
And after that, we have the output.

556
00:57:48,420 --> 00:57:55,060
And this time in the output comment, we have a directive for the Sbatch.

557
00:57:55,060 --> 00:57:57,620
We have a few different characters there.

558
00:57:57,620 --> 00:58:06,620
We have this %A and %a, and what these do is that because we now launch multiple jobs

559
00:58:06,620 --> 00:58:11,620
at the same time, we don't want them all to write in the same output file because it would

560
00:58:11,620 --> 00:58:16,860
get really cluttered and it would be really hard to see what each of these would do.

561
00:58:16,860 --> 00:58:25,780
What we can do is that we can divert each output into a separate file where the person's

562
00:58:25,780 --> 00:58:36,020
capital A is like a shorthand for the job ID as a whole, like the whole array will get

563
00:58:36,020 --> 00:58:37,900
one job ID.

564
00:58:37,900 --> 00:58:45,220
So that will be the job ID for the array job, for the whole SLURM submission basically.

565
00:58:45,220 --> 00:58:48,540
So basically, they all have the same capital A.

566
00:58:48,540 --> 00:58:49,040
Yes.

567
00:58:49,040 --> 00:58:52,460
The lowercase a is the number from it?

568
00:58:52,460 --> 00:58:55,100
Yeah, that's the Slurm array task ID.

569
00:58:55,100 --> 00:58:59,020
So each we will get, like if we submit in this example,

570
00:58:59,020 --> 00:59:02,780
we submit from 0 to 15, so we get 16 outputs

571
00:59:02,780 --> 00:59:07,220
when we calculate them together.

572
00:59:07,220 --> 00:59:11,380
So we get 16 output files in this case.

573
00:59:11,380 --> 00:59:15,780
And then, in the example, we run this echo.

574
00:59:20,020 --> 00:59:23,740
And here, I'm just printing it out.

575
00:59:23,740 --> 00:59:24,740
Yes.

576
00:59:24,740 --> 00:59:27,580
OK, does this look correct?

577
00:59:27,580 --> 00:59:29,380
Yeah, let's try and run it.

578
00:59:29,380 --> 00:59:35,900
OK, I Control-X and Y, and Enter to save.

579
00:59:35,900 --> 00:59:38,460
If I list, I see array example.

580
00:59:38,460 --> 00:59:41,300
So I run with sbatch?

581
00:59:41,300 --> 00:59:41,800
Yes.

582
00:59:45,460 --> 00:59:48,460
So we do one submission.

583
00:59:48,460 --> 00:59:53,460
And now if you run slurmq, we can see, hopefully.

584
00:59:53,460 --> 00:59:54,780
It already ran.

585
00:59:57,740 --> 01:00:00,820
The thing you saw pending was something I have.

586
01:00:00,820 --> 01:00:03,980
Yeah, we need to put maybe a sleep there.

587
01:00:03,980 --> 01:00:05,460
But OK, it already ran.

588
01:00:05,460 --> 01:00:07,860
And now we already see the output.

589
01:00:07,860 --> 01:00:19,300
So we see there that we have generated the overall job ID was 476680, and then we have

590
01:00:19,300 --> 01:00:23,140
from 0 to 50 in these output files.

591
01:00:23,140 --> 01:00:32,100
If [name] now looks at the output 0, you can see that the task ID is 0.

592
01:00:32,100 --> 01:00:37,220
And if you pick, let's say, 1 is 1 and so forth.

593
01:00:37,220 --> 01:00:42,460
You can notice that each process, when they were running, or each array task, when they

594
01:00:42,460 --> 01:00:50,660
were running, they all got a different number for the slow array task ID.

595
01:00:50,660 --> 01:00:55,820
There was a question also in the chat, but can you map this number into two parameters?

596
01:00:55,820 --> 01:01:01,940
Yes, you can.

597
01:01:01,940 --> 01:01:08,740
You can basically, whatever countable set you have, you can map into that, like whatever,

598
01:01:08,740 --> 01:01:11,180
like two dimensions, three dimensions.

599
01:01:11,180 --> 01:01:17,620
The question is just like, how complicated does the thing start to be?

600
01:01:17,620 --> 01:01:23,100
And usually, we will look at a few examples on what you can do with this or what sort

601
01:01:23,100 --> 01:01:25,420
of things you can do with this.

602
01:01:25,420 --> 01:01:31,260
But in many cases, if it starts to get really complicated, the mapping, it's easier to just

603
01:01:31,260 --> 01:01:35,660
the number to your program and then write it in your program. Okay, what should it do

604
01:01:35,660 --> 01:01:43,820
with numbers? So, quite often, let's say it loads a line from a configuration file and

605
01:01:43,820 --> 01:01:48,380
then loads the parameters from there or something like that.

606
01:01:48,380 --> 01:01:56,780
Yeah. Okay. So, is that basically what there is? Do we, I think...

607
01:01:56,780 --> 01:02:03,900
Maybe we could run the same example, but with a sleep in there and just demo, just to verify

608
01:02:03,900 --> 01:02:09,100
that it actually runs on separate jobs.

609
01:02:09,100 --> 01:02:14,420
I'm pushing the up arrow key to go up in history to find my nano command.

610
01:02:14,420 --> 01:02:15,420
Yeah.

611
01:02:15,420 --> 01:02:18,220
I push enter.

612
01:02:18,220 --> 01:02:29,680
So what we get when we run this, we get 15 different individual jobs that all run the

613
01:02:29,680 --> 01:02:33,440
same except the task ID is different.

614
01:02:33,440 --> 01:02:41,520
And if we, based on that Slurm task ID, we do something different, or use a different

615
01:02:41,520 --> 01:02:46,800
data set or different parameters or something, we will get 15 different results.

616
01:02:46,800 --> 01:02:47,800
Yeah.

617
01:02:47,800 --> 01:02:48,760
Okay. Should I do it?

618
01:02:48,760 --> 01:02:49,280
Yeah.

619
01:02:49,280 --> 01:02:50,520
Yes. Let's do it.

620
01:02:50,520 --> 01:02:52,320
Press Enter.

621
01:02:54,640 --> 01:02:59,280
Okay. I push Enter and then I run Slurm q immediately.

622
01:02:59,280 --> 01:03:00,760
Yeah.

623
01:03:02,760 --> 01:03:06,280
So here it looks like.

624
01:03:06,280 --> 01:03:09,200
Yeah. They all got running immediately.

625
01:03:09,200 --> 01:03:11,160
They're all running very quickly.

626
01:03:11,160 --> 01:03:13,800
You can notice on the left,

627
01:03:13,800 --> 01:03:17,680
you can see that they have this underscore there,

628
01:03:17,680 --> 01:03:24,240
where you have the different array indices separated. So you have the same thing running

629
01:03:24,240 --> 01:03:31,280
with different indices. And on the right, you can see where the node list is. Some are running in

630
01:03:31,280 --> 01:03:37,040
PE40, some are running in PE41, and some are running in PE43, I think, or something.

631
01:03:37,920 --> 01:03:44,000
So the computers where they are running might be completely different because they are independent.

632
01:03:44,000 --> 01:03:53,600
But using this array thing makes it possible for you to run a lot of simulations at the

633
01:03:53,600 --> 01:03:59,120
same time and it also is very nice for the queue manager because the queue manager doesn't

634
01:03:59,120 --> 01:04:03,240
now have to consider each job independently.

635
01:04:03,240 --> 01:04:10,320
It can consider the array job as a whole and it's much better for the queue manager to

636
01:04:10,320 --> 01:04:18,480
do this for you to do this this way you don't get you don't accidentally like cause like a denial

637
01:04:18,480 --> 01:04:23,120
of service and this could be the denial of service by submitting like thousands of jobs

638
01:04:23,120 --> 01:04:30,320
so many of our users might run arrays that have like hundreds or maybe even maybe thousands of

639
01:04:30,320 --> 01:04:37,120
jobs in the array so if they want to do like really big lots of simulations like different

640
01:04:37,120 --> 01:04:42,120
random numbers or different parameters or something like that.

641
01:04:42,120 --> 01:04:48,120
So there's a question, does it run the same program ten times?

642
01:04:48,120 --> 01:04:55,120
Yes, and it has, but your program has to be able to use this variable to do different things each time.

643
01:04:55,120 --> 01:05:00,120
If that's what you want. And below, I think we have a bunch of examples of this.

644
01:05:00,120 --> 01:05:06,120
So we won't really go through the examples, but maybe we can show the spirit of them.

645
01:05:06,120 --> 01:05:07,120
Yeah.

646
01:05:07,120 --> 01:05:15,260
Maybe we can show the first two as this kind of like, yeah.

647
01:05:15,260 --> 01:05:19,240
So here we see it's the array job.

648
01:05:19,240 --> 01:05:22,420
There's 30 things in the array.

649
01:05:22,420 --> 01:05:27,180
And I see there's some application that says input.

650
01:05:27,180 --> 01:05:33,980
And here every program will take a different piece of input data.

651
01:05:33,980 --> 01:05:40,060
if you have your input data all numbered, then that's relatively easy.

652
01:05:42,060 --> 01:05:48,460
Yeah, and if you scroll a bit down, there's two examples of how can you map those numbers

653
01:05:48,460 --> 01:05:57,340
into some parameters. One example is that you can use this case structure in Bash.

654
01:05:57,340 --> 01:06:09,460
If the Slurm array task ID is something, set the seed in this case to be some other value.

655
01:06:09,460 --> 01:06:18,400
And there is also below, there's another example where you can use, if you don't want to write,

656
01:06:18,400 --> 01:06:21,860
you can write it a bit more concise.

657
01:06:21,860 --> 01:06:28,020
you can use this bash array. This is a different array than the slurm array we are talking

658
01:06:28,020 --> 01:06:35,080
about. Basically, bash has this list or all this kind of array structure. So, you can

659
01:06:35,080 --> 01:06:43,940
list the numbers there and then pick from this array that specific number based on the

660
01:06:43,940 --> 01:06:47,660
index of the, like, there.

661
01:06:47,660 --> 01:06:53,420
And I guess the whole thing could be done in Python, basically, where you give a single

662
01:06:53,420 --> 01:06:58,700
number to your Python or R MATLAB program, whatever, and internally it maps to whatever

663
01:06:58,700 --> 01:07:00,860
parameters it's supposed to use.

664
01:07:00,860 --> 01:07:01,860
Yes.

665
01:07:01,860 --> 01:07:02,860
Okay.

666
01:07:02,860 --> 01:07:03,860
Yeah.

667
01:07:03,860 --> 01:07:11,340
You can also, like, read a line from a file, like, below this example also, like, if you

668
01:07:11,340 --> 01:07:16,500
want to write your different parameters in a file and then read it from there.

669
01:07:16,500 --> 01:07:20,500
But maybe we should go into exercises and let people try them out themselves.

670
01:07:20,500 --> 01:07:21,500
Yeah, I agree.

671
01:07:21,500 --> 01:07:25,500
There's a little bit more here you can read yourself.

672
01:07:25,500 --> 01:07:32,500
So for exercises, what should we recommend people to work on?

673
01:07:32,500 --> 01:07:41,500
I would probably say that the exercise 1 would be probably best.

674
01:07:41,500 --> 01:07:46,140
So you can choose whichever of these previous examples

675
01:07:46,140 --> 01:07:48,660
that we provide in the page and try

676
01:07:48,660 --> 01:07:55,620
to give this pi.py different seed numbers.

677
01:07:55,620 --> 01:07:57,700
So you can try out one of these exercises.

678
01:07:57,700 --> 01:08:02,740
I would say that run one of these, at least,

679
01:08:02,740 --> 01:08:03,660
and try it out.

680
01:08:07,220 --> 01:08:08,940
Also, there was a good question, what

681
01:08:08,940 --> 01:08:18,780
a difference between job and a program. Job, in this case, often refers to Slurm job. Basically,

682
01:08:18,780 --> 01:08:27,260
like an allocation that Slurm needs to fulfill for you. When you submit with sbatch some script,

683
01:08:27,260 --> 01:08:31,980
then Slurm thinks of it as a job that it needs to do. I need to do this thing.

684
01:08:31,980 --> 01:08:39,860
thing. I need to execute this thing. If you give it an array job, it knows that, okay,

685
01:08:39,860 --> 01:08:48,380
I need to execute this like 100 times, but with different array index each time. That's

686
01:08:48,380 --> 01:08:58,060
a slurm job. Inside the script in that job, it can have multiple programs that you run

687
01:08:58,060 --> 01:09:02,020
and then one after the other.

688
01:09:02,020 --> 01:09:04,620
Yeah.

689
01:09:04,620 --> 01:09:05,500
OK.

690
01:09:05,500 --> 01:09:07,020
And how long should we give?

691
01:09:07,020 --> 01:09:09,700
I guess this has to be combined with the break

692
01:09:09,700 --> 01:09:13,340
because it will be break time.

693
01:09:13,340 --> 01:09:16,620
So assuming people start 10 minutes from now

694
01:09:16,620 --> 01:09:20,020
after the break, should we give 20 minutes maybe?

695
01:09:20,020 --> 01:09:21,940
Would you say this is the most important one,

696
01:09:21,940 --> 01:09:23,780
so we should give the most time here?

697
01:09:23,780 --> 01:09:27,860
I think it would be good to have enough time to really test

698
01:09:27,860 --> 01:09:34,900
these examples, because this is something that can give you a lot of power with minimal changes

699
01:09:34,900 --> 01:09:42,180
to your code. And that's why this is very good to learn, because you can then utilize it in

700
01:09:42,180 --> 01:09:49,220
multiple different ways. So yeah, I would say 20 minutes at least. Yeah. Okay. Sounds good.

701
01:09:49,220 --> 01:09:57,860
So I wrote down until 20.

702
01:09:57,860 --> 01:09:58,620
20 pounds.

703
01:09:58,620 --> 01:09:59,120
So that's.

704
01:09:59,120 --> 01:10:00,660
And do ask the questions.

705
01:10:00,660 --> 01:10:03,860
It's very good to have the questions in the notes.

706
01:10:03,860 --> 01:10:05,300
Yeah.

707
01:10:05,300 --> 01:10:08,260
This is the most important interactive lesson

708
01:10:08,260 --> 01:10:09,220
we're doing today.

709
01:10:09,220 --> 01:10:13,700
So OK, see you in half an hour.

710
01:10:13,700 --> 01:10:14,420
Yeah.

711
01:10:14,420 --> 01:10:15,860
Thanks.

712
01:10:15,860 --> 01:10:16,380
Thanks.

713
01:10:16,380 --> 01:10:17,940
Bye.

714
01:10:19,220 --> 01:10:21,280
you

715
01:10:49,220 --> 01:10:51,280
you

716
01:11:19,220 --> 01:11:21,280
you

717
01:11:49,220 --> 01:11:51,280
you

718
01:12:19,220 --> 01:12:21,280
you

719
01:12:49,220 --> 01:12:51,280
you

720
01:13:19,220 --> 01:13:21,280
you

721
01:13:49,220 --> 01:13:51,280
you

722
01:14:19,220 --> 01:14:21,280
you

723
01:14:49,220 --> 01:14:51,280
you

724
01:15:19,220 --> 01:15:21,280
you

725
01:15:49,220 --> 01:15:51,280
you

726
01:16:19,220 --> 01:16:21,280
you

727
01:16:49,220 --> 01:16:51,280
you

728
01:17:19,220 --> 01:17:21,280
you

729
01:17:49,220 --> 01:17:51,280
you

730
01:18:19,220 --> 01:18:21,280
you

731
01:18:49,220 --> 01:18:51,280
you

732
01:19:19,220 --> 01:19:21,280
you

733
01:19:49,220 --> 01:19:51,280
you

734
01:20:19,220 --> 01:20:21,280
you

735
01:20:49,220 --> 01:20:51,280
you

736
01:21:19,220 --> 01:21:21,280
you

737
01:21:49,220 --> 01:21:51,280
you

738
01:22:19,220 --> 01:22:21,280
you

739
01:22:49,220 --> 01:22:51,280
you

740
01:23:19,220 --> 01:23:21,280
you

741
01:23:49,220 --> 01:23:51,280
you

742
01:24:19,220 --> 01:24:21,280
you

743
01:24:49,220 --> 01:24:51,280
you

744
01:25:19,220 --> 01:25:21,280
you

745
01:25:49,220 --> 01:25:51,280
you

746
01:26:19,220 --> 01:26:21,280
you

747
01:26:49,220 --> 01:26:51,280
you

748
01:27:19,220 --> 01:27:21,280
you

749
01:27:49,220 --> 01:27:51,280
you

750
01:28:19,220 --> 01:28:21,280
you

751
01:28:49,220 --> 01:28:51,280
you

752
01:29:19,220 --> 01:29:21,280
you

753
01:29:49,220 --> 01:29:51,280
you

754
01:30:19,220 --> 01:30:21,280
you

755
01:30:49,220 --> 01:30:51,280
you

756
01:31:19,220 --> 01:31:21,280
you

757
01:31:49,220 --> 01:31:51,280
you

758
01:32:19,220 --> 01:32:21,280
you

759
01:32:49,220 --> 01:32:51,280
you

760
01:33:19,220 --> 01:33:21,280
you

761
01:33:49,220 --> 01:33:51,280
you

762
01:34:19,220 --> 01:34:21,280
you

763
01:34:49,220 --> 01:34:51,280
you

764
01:35:19,220 --> 01:35:21,280
you

765
01:35:49,220 --> 01:35:51,280
you

766
01:36:19,220 --> 01:36:21,280
you

767
01:36:49,220 --> 01:36:51,280
you

768
01:37:19,220 --> 01:37:21,280
you

769
01:37:49,220 --> 01:37:51,280
you

770
01:38:19,220 --> 01:38:21,280
you

771
01:38:49,220 --> 01:38:51,280
you

772
01:39:19,220 --> 01:39:21,280
you

773
01:39:49,220 --> 01:39:51,280
you

774
01:40:19,220 --> 01:40:26,700
Hello, we're back.

775
01:40:26,700 --> 01:40:31,900
And there have been really many very good questions here, and I think most have been

776
01:40:31,900 --> 01:40:33,900
answered.

777
01:40:33,900 --> 01:40:45,540
There's a good question about how does the program understand Slurm_Array_Task_ID.

778
01:40:45,540 --> 01:40:50,660
is called an environment variable and it's not specific to Slurm. Environment variable is a thing

779
01:40:50,660 --> 01:41:00,660
which all Unix and probably Windows programs have a concept of and many languages can read

780
01:41:00,660 --> 01:41:05,300
them and get the values. We're showing how to do it with Bash but there's other ways too.

781
01:41:05,300 --> 01:41:16,100
Yeah, we will quickly mention when in the next session that we start soon, how you can like

782
01:41:16,740 --> 01:41:21,700
calculate or get the CPU number, how many CPUs you have allocated in the script,

783
01:41:23,300 --> 01:41:27,780
like in your code, how you can get that number and you can use the same kind of structure for the

784
01:41:27,780 --> 01:41:32,580
array ID as well. Yeah.

785
01:41:33,380 --> 01:41:36,740
There's a good thing here about confusing terminology.

786
01:41:36,740 --> 01:41:40,180
And yes, it's confusing. My recommendation is to

787
01:41:40,180 --> 01:41:43,620
run it several times and see what happens.

788
01:41:43,620 --> 01:41:47,860
And also, I would say that

789
01:41:47,860 --> 01:41:52,100
try to look beyond the terminology and look at the

790
01:41:52,100 --> 01:41:55,780
functionality. All of these have so

791
01:41:55,780 --> 01:42:01,700
many conflicting terms that it's very hard to keep it in check, even in speech. We might

792
01:42:02,260 --> 01:42:08,900
use confusing terminologies all the time that are not correct because there's so many different

793
01:42:08,900 --> 01:42:13,700
terms. But what I think the most important thing is to keep in mind that, okay, you have

794
01:42:14,340 --> 01:42:21,220
this workflow that you might write your serial job. You might do an interactive, first interactive

795
01:42:21,220 --> 01:42:27,460
job on your program, then you write it into a serial script. And then when you have run it

796
01:42:27,460 --> 01:42:33,620
through the serial script, like we did yesterday, after that, you want to do it with, let's say,

797
01:42:33,620 --> 01:42:39,860
multiple parameters or something. So, then you add the array constructor. And then in your code,

798
01:42:39,860 --> 01:42:46,020
you write some way that, okay, it understands that there's different things that it needs to do.

799
01:42:46,020 --> 01:42:49,300
Yeah, like there's multiple ways of doing that.

800
01:42:49,300 --> 01:42:54,780
And usually it's a good idea to recommend a way is that first you launch like one array

801
01:42:54,780 --> 01:42:55,780
job.

802
01:42:55,780 --> 01:42:58,600
Like I mean, like one array index.

803
01:42:58,600 --> 01:43:03,300
So let's say array equals, that's just array equals zero.

804
01:43:03,300 --> 01:43:06,680
So you basically launch, it would be basically the serial job, right?

805
01:43:06,680 --> 01:43:07,680
Okay.

806
01:43:07,680 --> 01:43:08,680
Yeah.

807
01:43:08,680 --> 01:43:11,580
Like because you're launching only one of them and you see that, okay, did it work correctly?

808
01:43:11,580 --> 01:43:17,420
And then maybe you launch the array, and that's just array equals one, and you launch the

809
01:43:17,420 --> 01:43:18,420
next one.

810
01:43:18,420 --> 01:43:19,420
Okay.

811
01:43:19,420 --> 01:43:20,420
Yeah.

812
01:43:20,420 --> 01:43:22,220
And you see that, okay, did it do the different thing that you expected?

813
01:43:22,220 --> 01:43:26,460
And then you launch array from zero to thousand or something.

814
01:43:26,460 --> 01:43:32,300
Because like, so you basically like copy paste the same code without doing the copy paste.

815
01:43:32,300 --> 01:43:36,380
And that's this kind of workflow, once you get hang of that, that is the most important

816
01:43:36,380 --> 01:43:37,380
thing.

817
01:43:37,380 --> 01:43:43,620
call it is not the most important issue, I would say.

818
01:43:43,620 --> 01:43:52,260
Okay, yeah. So what's coming up next?

819
01:43:52,260 --> 01:44:01,060
Do we do some examples of the array jobs? Or should we go on?

820
01:44:01,060 --> 01:44:05,700
I think we should move on because we have a lot of

821
01:44:05,700 --> 01:44:11,780
ground to cover. But, yeah, so maybe we should...

822
01:44:11,780 --> 01:44:15,460
My screen. So it's a shared memory parallelism.

823
01:44:15,460 --> 01:44:20,900
Yeah. Okay. Yeah. And for the next two, do we give a

824
01:44:20,900 --> 01:44:25,060
very high-level overview, but

825
01:44:26,740 --> 01:44:34,180
we don't do as many examples? Yeah, yeah. Like, we will run

826
01:44:34,180 --> 01:44:41,220
through some examples, but yeah, so we won't necessarily, because these are very dependent

827
01:44:41,220 --> 01:44:48,420
on your program, so we don't necessarily like, it's very hard to do like a good example because

828
01:44:48,420 --> 01:44:53,540
it depends on what program you're using, but we can demonstrate how the queue thinks about

829
01:44:53,540 --> 01:44:59,300
these things. So the array job, like if we think about the array job that we just had,

830
01:44:59,300 --> 01:45:06,860
But we are running basically the same serial job, but we're just running with more of those.

831
01:45:06,860 --> 01:45:12,960
We are running more of those simultaneously and separately of each other.

832
01:45:12,960 --> 01:45:18,140
But all of those will, like, let's say if we run one of those and that takes like an

833
01:45:18,140 --> 01:45:24,200
hour to run, the other ones will also most likely take an hour to run as well if they're

834
01:45:24,200 --> 01:45:26,240
quite similar jobs.

835
01:45:26,240 --> 01:45:30,120
But in many cases, you would want it to run faster.

836
01:45:30,120 --> 01:45:32,920
You want it to actually finish faster.

837
01:45:32,920 --> 01:45:40,920
And in that case, you might want to try to use multiple processors to make it run faster.

838
01:45:40,920 --> 01:45:45,800
And the first question here is that, can your program actually do it?

839
01:45:45,800 --> 01:45:53,600
Does your program support multiple processors so that it can internally run faster?

840
01:45:53,600 --> 01:46:02,800
If we look at the picture below, again, we see that in the shared memory jobs,

841
01:46:05,360 --> 01:46:14,480
we get for the one single job, and I mean, again, let's focus, let's put the array idea that we had

842
01:46:14,480 --> 01:46:20,080
and put that in the back of our mind, but let's think about the serial job. We start one,

843
01:46:20,080 --> 01:46:27,760
we submit one job first to the queue and in the shared memory job, we would want to give this one

844
01:46:28,480 --> 01:46:34,400
job more resources. We wanted to give that to have access to multiple CPUs.

845
01:46:35,520 --> 01:46:40,880
So in the array job, we had individual jobs that are completely independent and all of those were

846
01:46:40,880 --> 01:46:49,920
running with one CPU. But in this case, we want to give like, okay, let's have one of these scripts

847
01:46:50,080 --> 01:46:54,720
and let this script should be able to use multiple CPUs.

848
01:46:56,240 --> 01:46:59,920
And it's very easy to ask for these resources.

849
01:46:59,920 --> 01:47:04,320
So you can just give this dash dash CPUs per task

850
01:47:04,320 --> 01:47:08,560
and then a number in this Sbatch comment.

851
01:47:11,360 --> 01:47:15,680
Yes, and you can then reserve multiple CPUs for a job.

852
01:47:15,680 --> 01:47:23,860
The question is that does your program actually use those, or can it use those?

853
01:47:23,860 --> 01:47:30,060
In many cases, the program might recognize that there's multiple processes there, and

854
01:47:30,060 --> 01:47:32,940
in some cases it doesn't.

855
01:47:32,940 --> 01:47:41,020
What is usually important in your program, if you see something like nTasks or nProcs

856
01:47:41,020 --> 01:47:49,300
or nCPUs, or CPUs, or processors, something like this,

857
01:47:49,300 --> 01:47:54,620
like a flag that tells that, OK, this can be run in parallel.

858
01:47:54,620 --> 01:47:56,220
What you usually want to do is you

859
01:47:56,220 --> 01:47:58,460
want this number that the process,

860
01:47:58,460 --> 01:48:02,540
like what the program takes in, to be

861
01:48:02,540 --> 01:48:05,020
equal to the number of processors

862
01:48:05,020 --> 01:48:07,620
you have requested from the queue.

863
01:48:07,620 --> 01:48:10,340
And this doesn't happen automatically all the time.

864
01:48:11,220 --> 01:48:18,420
Yes, it doesn't because the program doesn't know where it's being run. The program doesn't

865
01:48:18,420 --> 01:48:24,660
understand that it's in a computer cluster. It's being run in this allocation of, let's say,

866
01:48:24,660 --> 01:48:31,140
four CPUs. The program doesn't necessarily know that, okay, I've been given four CPUs,

867
01:48:31,140 --> 01:48:37,140
And it might see that there's like 40 CPUs or 100 CPUs in the machine.

868
01:48:37,140 --> 01:48:43,140
And it might think that, hey, I can go full ham and I can take all of these CPUs.

869
01:48:43,140 --> 01:48:47,140
But in reality, it's only been given four.

870
01:48:47,140 --> 01:49:00,140
So you might get completely overloaded by having the program at start like 100 processes on those four CPUs.

871
01:49:00,140 --> 01:49:05,000
So, what you usually want to do is you want to check your program, where your program

872
01:49:05,000 --> 01:49:12,520
takes the value of how much parallelism I should use, how much parallelism I should do, and

873
01:49:12,520 --> 01:49:17,520
then make certain that that number matches the request that's in there.

874
01:49:17,520 --> 01:49:20,640
I think I remember some mention of this at the bottom of the page.

875
01:49:20,640 --> 01:49:25,680
Should we look there now, or maybe come back to it after we've seen it?

876
01:49:25,680 --> 01:49:29,800
We can maybe come back to it, but maybe we should just, like, this is too theoretical,

877
01:49:29,800 --> 01:49:32,520
So run something and see if it's faster.

878
01:49:32,520 --> 01:49:33,360
OK.

879
01:49:33,360 --> 01:49:35,000
So I'll scroll down some.

880
01:49:38,040 --> 01:49:39,760
Where's an example?

881
01:49:39,760 --> 01:49:41,440
Here's an example.

882
01:49:41,440 --> 01:49:45,280
Running an example shared memory program.

883
01:49:45,280 --> 01:49:46,320
Yeah.

884
01:49:46,320 --> 01:49:50,880
So the program here is the same Py Script

885
01:49:50,880 --> 01:49:53,120
that we have used as an example.

886
01:49:53,120 --> 01:49:57,800
And it has this flag called dash dash nprocs.

887
01:49:57,800 --> 01:49:59,200
So how many procs?

888
01:49:59,200 --> 01:50:02,240
like it uses Python multiprocessing to start

889
01:50:02,240 --> 01:50:08,560
do the calculation in parallel and any if you give it the number then

890
01:50:08,560 --> 01:50:12,240
it will use multiple processors yeah okay so

891
01:50:12,240 --> 01:50:17,200
let's try running this example over here

892
01:50:17,760 --> 01:50:23,120
so cpus per task equals two so i tell it two processors

893
01:50:23,120 --> 01:50:36,000
Yes. And then, yeah, and the usual suspects like the memory and time and then we call it

894
01:50:37,040 --> 01:50:44,480
and then the number of processes. And do note that now [name] is putting the numbers to be

895
01:50:44,480 --> 01:50:50,560
like the same on like the number of CPUs per task and the number of processors.

896
01:50:50,560 --> 01:50:54,320
Because it's the same over here.

897
01:50:54,320 --> 01:50:57,000
So let's try running it.

898
01:50:57,000 --> 01:50:58,000
Okay.

899
01:50:58,000 --> 01:50:59,000
Okay.

900
01:50:59,000 --> 01:51:05,680
Oh, here it says using two processes.

901
01:51:05,680 --> 01:51:08,800
I guess that's the program printing that out.

902
01:51:08,800 --> 01:51:09,800
Yeah.

903
01:51:09,800 --> 01:51:10,800
Yeah.

904
01:51:10,800 --> 01:51:13,240
It knows that it now needs to use multiple processes.

905
01:51:13,240 --> 01:51:14,720
So maybe let's try it out.

906
01:51:14,720 --> 01:51:17,960
Can you put time at the start of the call?

907
01:51:17,960 --> 01:51:18,960
Okay.

908
01:51:18,960 --> 01:51:24,800
In Linux, there's this program called time that you can put, if you put it before the Srun.

909
01:51:24,800 --> 01:51:26,400
Should I do it before Python?

910
01:51:26,400 --> 01:51:27,200
Yeah, before that.

911
01:51:27,200 --> 01:51:28,240
Yeah, that's good.

912
01:51:28,240 --> 01:51:28,800
Yeah, that's good.

913
01:51:29,600 --> 01:51:34,240
So what the time does is that it will give some like timing information,

914
01:51:34,240 --> 01:51:37,040
like how long did the program execution take.

915
01:51:37,040 --> 01:51:40,560
Let's try it out and see how long it took.

916
01:51:42,240 --> 01:51:44,640
0.91 seconds, user.

917
01:51:44,640 --> 01:51:46,040
or?

918
01:51:46,040 --> 01:51:49,240
Maybe we should add a 0 there into the number of.

919
01:51:49,240 --> 01:51:50,800
OK.

920
01:51:50,800 --> 01:51:53,760
So now we're running 10 times more.

921
01:51:53,760 --> 01:51:54,260
Yeah.

922
01:51:58,080 --> 01:51:59,480
Processes.

923
01:51:59,480 --> 01:52:07,120
So it says 8.7 seconds user time and 4.4 seconds elapsed.

924
01:52:07,120 --> 01:52:11,040
So it did go about twice as fast.

925
01:52:11,040 --> 01:52:11,800
Yes.

926
01:52:11,800 --> 01:52:17,440
We also note that time also produces this 198% CPU.

927
01:52:17,440 --> 01:52:19,120
OK, yeah.

928
01:52:19,120 --> 01:52:22,840
So we can also use the seff that we previously yesterday

929
01:52:22,840 --> 01:52:25,680
talked about to see the utilization.

930
01:52:25,680 --> 01:52:29,440
So if you use the seff and the job ID,

931
01:52:29,440 --> 01:52:34,120
we can quickly check that the utilization, what was it?

932
01:52:34,120 --> 01:52:42,440
So nine seconds.

933
01:52:42,440 --> 01:52:45,000
Here it says only 112.

934
01:52:45,000 --> 01:52:45,520
Yeah.

935
01:52:45,520 --> 01:52:49,920
It might be that maybe it didn't have time to,

936
01:52:49,920 --> 01:52:52,040
because it's so short, maybe it didn't have time

937
01:52:52,040 --> 01:52:53,800
to measure it correctly.

938
01:52:53,800 --> 01:52:55,400
Yeah, probably.

939
01:52:55,400 --> 01:52:56,960
Probably, if that is the case.

940
01:52:56,960 --> 01:52:58,160
Yeah.

941
01:52:58,160 --> 01:53:00,600
So it wasn't that bad, right?

942
01:53:00,600 --> 01:53:03,600
Yeah, I mean, that's not easy.

943
01:53:03,600 --> 01:53:08,360
What happens if I don't give the nprocs here?

944
01:53:08,360 --> 01:53:09,200
Let's try it out.

945
01:53:09,200 --> 01:53:14,960
So if you run it like that without the nprocs.

946
01:53:14,960 --> 01:53:16,040
I remove nprocs.

947
01:53:22,560 --> 01:53:25,520
So it most likely will take about 16 seconds.

948
01:53:25,520 --> 01:53:28,640
So I would assume if the previous one was eight.

949
01:53:28,640 --> 01:53:32,520
So it took, it used 8.4 seconds of CPU

950
01:53:32,520 --> 01:53:36,920
and took 8.5.

951
01:53:36,920 --> 01:53:40,040
So basically, it only ran on one CPU.

952
01:53:40,040 --> 01:53:46,960
And everything, we wasted one CPU.

953
01:53:46,960 --> 01:53:53,600
What happens if I use four CPUs, or I request four CPUs?

954
01:53:53,600 --> 01:53:57,520
Or I go like this.

955
01:53:57,520 --> 01:54:00,600
So I'm telling the program to use more CPUs.

956
01:54:00,600 --> 01:54:04,760
put something bigger there, put something like 32 or something

957
01:54:04,760 --> 01:54:10,040
so that we see an actual thing there.

958
01:54:10,040 --> 01:54:13,360
So now we are overbooking, basically, the CPUs.

959
01:54:13,360 --> 01:54:15,960
We have only requested two CPUs, but we

960
01:54:15,960 --> 01:54:18,800
are running 32 processors there.

961
01:54:18,800 --> 01:54:28,680
So you notice that it didn't run any faster or any slower here.

962
01:54:28,680 --> 01:54:37,080
Yeah. So you notice that it runs at the same speed basically as if it would have two CPUs.

963
01:54:38,840 --> 01:54:43,480
But the difference is, like this is a toy program. If the program would do something

964
01:54:43,480 --> 01:54:48,520
like really complicated, it would slow down the program because it's oversubscribing.

965
01:54:48,520 --> 01:54:50,360
It definitely can slow down.

966
01:54:51,000 --> 01:54:53,880
Yeah. There's a good question in the notes that,

967
01:54:53,880 --> 01:54:59,440
But what is the difference and benefit of allocating multiple CPUs for a single program

968
01:54:59,440 --> 01:55:06,120
against allocating multiple arrays or like I assume like multiple jobs in an array?

969
01:55:06,120 --> 01:55:11,640
And that's an excellent question because like it raises the question that which is better?

970
01:55:11,640 --> 01:55:19,580
Like is it better to run one program twice as fast, but run it like multiple?

971
01:55:19,580 --> 01:55:28,140
Like is it better to run one program twice as fast for double the parameters or is it

972
01:55:28,140 --> 01:55:33,840
better to run two individual programs with different parameters?

973
01:55:33,840 --> 01:55:40,960
So basically, is it better to divide the program among parameters or just run it as fast as

974
01:55:40,960 --> 01:55:44,520
possible but then go through multiple parameters?

975
01:55:44,520 --> 01:55:52,400
And usually the case is that it's usually easier to do the embarrassingly parallel route.

976
01:55:52,400 --> 01:55:58,360
If you want to do maximum throughput or you want to do maximum amount of jobs, maximum

977
01:55:58,360 --> 01:56:04,680
amount of parameters and whatever, it's usually better to use small jobs than big jobs.

978
01:56:04,680 --> 01:56:06,760
So does it depend on the scaling?

979
01:56:06,760 --> 01:56:13,840
So in here, this program is actually under the hood,

980
01:56:13,840 --> 01:56:16,320
basically embarrassingly parallel.

981
01:56:16,320 --> 01:56:18,560
It divides it up.

982
01:56:18,560 --> 01:56:21,440
Each one runs separately.

983
01:56:21,440 --> 01:56:25,200
But some jobs get slower the more processors you add to it.

984
01:56:25,200 --> 01:56:28,120
So in that case, it's better to not use too many

985
01:56:28,120 --> 01:56:30,120
and use arrays instead.

986
01:56:30,120 --> 01:56:33,120
And I guess you can combine both of them at the same time.

987
01:56:33,120 --> 01:56:33,880
Yeah.

988
01:56:33,880 --> 01:56:40,560
The thing that you get with running with multiple processors is that if your program takes a

989
01:56:40,560 --> 01:56:48,640
long time to run, let's say it takes 10 hours to run, you want the results today, not in

990
01:56:48,640 --> 01:56:49,640
10 hours.

991
01:56:49,640 --> 01:56:56,400
When you submit a job, you want them to be done so that you can look at them earlier.

992
01:56:56,400 --> 01:57:03,600
And in that case, if your program can be parallelized, it's possible to maybe get them faster.

993
01:57:03,600 --> 01:57:10,800
results and you can view them faster. And also if the program is big enough that it just

994
01:57:10,800 --> 01:57:18,080
would take way too much time to run. It's good to parallelize it then. It's like the whole point

995
01:57:18,080 --> 01:57:25,520
of the cluster to get stuff done faster, I guess. Yeah. Okay, yeah. So, should we continue

996
01:57:25,520 --> 01:57:30,520
you scrolling down, or is there anything else here?

997
01:57:32,000 --> 01:57:33,240
Tell me when to stop.

998
01:57:33,240 --> 01:57:35,760
Common special cases.

999
01:57:35,760 --> 01:57:39,080
Let's look at the example of the submission script,

1000
01:57:39,080 --> 01:57:40,880
how we would write it as a submission script.

1001
01:57:40,880 --> 01:57:42,480
Okay, yeah.

1002
01:57:42,480 --> 01:57:47,480
So, previously [name] ran the example

1003
01:57:47,640 --> 01:57:50,760
with the, from the, like an interactive job,

1004
01:57:50,760 --> 01:57:53,680
but we can also run this in, of course,

1005
01:57:53,680 --> 01:57:55,100
in a submission script, and that's

1006
01:57:55,100 --> 01:57:58,820
a better way of running it to write it into a CLI script.

1007
01:57:58,820 --> 01:57:59,320
Yeah.

1008
01:57:59,320 --> 01:58:04,000
So in this case, if you copy the script,

1009
01:58:04,000 --> 01:58:06,640
or modify the existing, yeah.

1010
01:58:06,640 --> 01:58:09,320
Modify.

1011
01:58:09,320 --> 01:58:13,080
The previous script wasn't a Pi example, though, so I will.

1012
01:58:13,080 --> 01:58:16,240
Yeah, maybe you want to do a completely new one.

1013
01:58:20,400 --> 01:58:21,520
OK.

1014
01:58:21,520 --> 01:58:26,000
I'll copy most of this over.

1015
01:58:26,000 --> 01:58:27,760
Yeah.

1016
01:58:27,760 --> 01:58:28,680
So.

1017
01:58:28,680 --> 01:58:38,200
So sbatch, so two per task.

1018
01:58:38,200 --> 01:58:39,880
Yeah.

1019
01:58:39,880 --> 01:58:42,760
And now, because we are running this in the script,

1020
01:58:42,760 --> 01:58:46,400
we can utilize the environment variable that Slurm sets.

1021
01:58:46,400 --> 01:58:49,560
So we can utilize this environment variable

1022
01:58:49,560 --> 01:58:58,840
called slurm CPUs per task, so that like slurm will fit when the code is being run,

1023
01:58:58,840 --> 01:59:05,160
that environment variable will contain, similar to what the array ID was, it will contain the

1024
01:59:05,160 --> 01:59:14,120
number of CPUs reserved for this program, so it will automatically fit that. This is very helpful

1025
01:59:14,120 --> 01:59:18,040
if you're running something and first you run it with, and you want to test out, for example,

1026
01:59:18,040 --> 01:59:23,960
scaling of the program, how well it behaves. If you run it with one CPU, if you run it with two

1027
01:59:23,960 --> 01:59:33,880
CPUs, four, eight, 16, so forth, it's usually a good idea to use this environment variable because

1028
01:59:33,880 --> 01:59:39,240
then you can easily test out different CPU numbers and you don't have to go into your

1029
01:59:39,240 --> 01:59:46,360
program code or something to change the number to match the request number. Yeah, okay, so let's

1030
01:59:46,360 --> 01:59:49,360
Submit it.

1031
01:59:49,360 --> 01:59:50,560
Submit, yes.

1032
01:59:53,480 --> 01:59:54,600
Do you think it will work?

1033
01:59:58,200 --> 01:59:59,240
The command.

1034
01:59:59,240 --> 01:59:59,880
Let's see.

1035
02:00:03,680 --> 02:00:07,080
If you look at the Slurm queue, it's probably run already.

1036
02:00:07,080 --> 02:00:09,040
It's running.

1037
02:00:09,040 --> 02:00:12,320
I added an extra zero there, so it would run a bit slower.

1038
02:00:12,320 --> 02:00:13,600
Yeah.

1039
02:00:13,600 --> 02:00:15,960
OK, yeah, it finished.

1040
02:00:15,960 --> 02:00:17,280
Yeah.

1041
02:00:17,280 --> 02:00:18,440
OK.

1042
02:00:18,440 --> 02:00:27,120
If we look at the output, I'll cat it.

1043
02:00:27,120 --> 02:00:33,560
Using two processes, yeah.

1044
02:00:33,560 --> 02:00:36,400
OK, should we seff?

1045
02:00:36,400 --> 02:00:40,840
Yeah, and also in the chat or in the notes,

1046
02:00:40,840 --> 02:00:42,640
there was a good question of, like,

1047
02:00:42,640 --> 02:00:49,040
I used the term processors and processors and like all of this million of words, like

1048
02:00:49,040 --> 02:00:53,920
the CPU, there's core, there's process, there's processor.

1049
02:00:53,920 --> 02:00:59,960
Sometimes there's thread or some and like all of these have their own like specific

1050
02:00:59,960 --> 02:01:06,800
different meaning, but they're basically like, like used in places like a synonyms in many

1051
02:01:06,800 --> 02:01:07,800
cases.

1052
02:01:07,800 --> 02:01:14,440
There are technical differences to all of these words, and the CPU and all of this.

1053
02:01:14,440 --> 02:01:16,960
You shouldn't necessarily worry about it.

1054
02:01:16,960 --> 02:01:23,720
The most important thing to note is that in the program, there's something that it can

1055
02:01:23,720 --> 02:01:24,720
do.

1056
02:01:24,720 --> 02:01:33,920
It can execute two programs at the same time within the same program, basically.

1057
02:01:33,920 --> 02:01:42,960
do that, it should have a place to run it, basically. If you go back to the pasta analogy,

1058
02:01:42,960 --> 02:01:50,400
instead of just cooking pasta in one pot, we are now cooking pasta in two pots or we're doing

1059
02:01:51,440 --> 02:02:00,080
two pots in two burners or stovetops. We just add more stuff there.

1060
02:02:00,080 --> 02:02:01,080
Yeah.

1061
02:02:01,080 --> 02:02:07,820
So yeah, like whatever term is used, like if the program developers, they will also

1062
02:02:07,820 --> 02:02:11,640
use all kinds of terms interchangeably all the time.

1063
02:02:11,640 --> 02:02:13,640
So don't worry about it too much.

1064
02:02:13,640 --> 02:02:19,240
And that's why I was saying that you should look for these various terms because you will

1065
02:02:19,240 --> 02:02:22,280
never know what is the term used by the developer.

1066
02:02:22,280 --> 02:02:29,360
They might talk about the process, they might talk about number of CPUs, number of processors,

1067
02:02:29,360 --> 02:02:36,760
of tasks, like number of jobs, like the terms might be different, but the important thing

1068
02:02:36,760 --> 02:02:45,000
is to know that, okay, these refer to, okay, how many processors or CPUs you want to use

1069
02:02:45,000 --> 02:02:50,480
and to match that with the number of CPUs that you request from the queue.

1070
02:02:50,480 --> 02:02:57,600
And for the queue, the important thing is the CPUs per task.

1071
02:02:57,600 --> 02:03:09,040
I run seff on this thing we just did? Sure. And it looks pretty much like before, so not perfect

1072
02:03:09,040 --> 02:03:17,440
efficiency, but it's so short that's expected. Yeah, and do note that here the CPU efficiency

1073
02:03:17,440 --> 02:03:25,040
refers to the size of the request, like the reservation. So the CPU efficiency is like

1074
02:03:25,040 --> 02:03:32,080
like, with respect to the amount of processors requested.

1075
02:03:32,080 --> 02:03:39,120
So 100% would mean that you're using all of the requested CPUs.

1076
02:03:39,120 --> 02:03:39,960
Yeah.

1077
02:03:39,960 --> 02:03:42,800
And I think you've said this before,

1078
02:03:42,800 --> 02:03:45,960
but the most important thing to me

1079
02:03:45,960 --> 02:03:50,600
is that this nprocs gets automatically detected

1080
02:03:50,600 --> 02:03:51,200
from Slurm.

1081
02:03:51,200 --> 02:03:54,640
So I only have to keep it up to date in one place,

1082
02:03:54,640 --> 02:04:01,680
which seems like it would save me a lot of time and effort and doing stuff wrong.

1083
02:04:01,680 --> 02:04:11,120
Yeah. Below, if we scroll a bit below, there's like this using Slurm CPUs per task efficiently.

1084
02:04:11,120 --> 02:04:19,520
And you can use this basically also for the array task ID. So here's a few examples on how you can

1085
02:04:19,520 --> 02:04:22,880
use this parameter.

1086
02:04:22,880 --> 02:04:25,880
So for example, in Python, like in the middle there,

1087
02:04:25,880 --> 02:04:32,760
you can use the OS module to get that environment variable.

1088
02:04:32,760 --> 02:04:37,360
So in this case, if the variable is set,

1089
02:04:37,360 --> 02:04:40,960
the value of that is converted into an integer.

1090
02:04:40,960 --> 02:04:44,160
If it's not set, the default value is now one.

1091
02:04:44,160 --> 02:04:46,160
Is one, yeah, OK.

1092
02:04:46,160 --> 02:04:54,640
So you can use this when in your program to like you can write this there so that it automatically

1093
02:04:54,640 --> 02:05:00,960
detects how many CPUs you're using and then you can use the end CPUs in your like to give it to

1094
02:05:00,960 --> 02:05:06,880
a library or something. Yeah and same to MATLAB. Okay yeah so basically programs so if programs

1095
02:05:06,880 --> 02:05:12,880
were well designed they would all use something like this to automatically detect number of

1096
02:05:12,880 --> 02:05:16,880
processors available.

1097
02:05:16,880 --> 02:05:18,880
Okay.

1098
02:05:18,880 --> 02:05:20,880
And this is, there is one

1099
02:05:20,880 --> 02:05:22,880
extra, like

1100
02:05:22,880 --> 02:05:24,880
I will add one extra

1101
02:05:24,880 --> 02:05:26,880
technical thing, like there is no standard

1102
02:05:26,880 --> 02:05:28,880
on how many CPUs you're going to be using.

1103
02:05:28,880 --> 02:05:30,880
The closest thing to a standard

1104
02:05:30,880 --> 02:05:32,880
that is used by many programs

1105
02:05:32,880 --> 02:05:34,880
is this so-called OMP

1106
02:05:34,880 --> 02:05:36,880
num threads. It's mentioned

1107
02:05:36,880 --> 02:05:38,880
above

1108
02:05:38,880 --> 02:05:40,880
there in the multi-threaded

1109
02:05:40,880 --> 02:05:47,920
versus multiprocess and double-booking section, but you can read the whole thing if you want.

1110
02:05:49,280 --> 02:05:55,440
Here's the technical stuff if you're interested, but the important thing is that many programs,

1111
02:05:55,440 --> 02:06:02,800
for example, like NumPy libraries in Python, they utilize internal parallelism already,

1112
02:06:02,800 --> 02:06:08,480
and it's done using this technology called OpenMP, open multiprocessing.

1113
02:06:08,480 --> 02:06:20,920
And that basically takes this variable, this OMP non-threads that describes how many processors

1114
02:06:20,920 --> 02:06:24,280
it should use.

1115
02:06:24,280 --> 02:06:28,560
And this is used in many programs.

1116
02:06:28,560 --> 02:06:34,840
Many programs use this as this kind of shorthand for how many processors you should use for

1117
02:06:34,840 --> 02:06:36,080
the parallelism.

1118
02:06:36,080 --> 02:06:44,340
So this is quite common for programs that don't even use OpenMP to do the parallelism.

1119
02:06:44,340 --> 02:06:50,280
So this is the only standard that is basically defined, otherwise it's like a wild west that

1120
02:06:50,280 --> 02:06:55,800
everybody uses their different environment variables for how many processors to use.

1121
02:06:55,800 --> 02:06:56,800
Yeah.

1122
02:06:56,800 --> 02:06:57,800
Okay.

1123
02:06:57,800 --> 02:07:04,360
So, we have three minutes before we need to go to a break.

1124
02:07:04,360 --> 02:07:07,600
What should we do?

1125
02:07:07,600 --> 02:07:13,640
I propose we can see if there's any further questions,

1126
02:07:13,640 --> 02:07:15,160
go to the break.

1127
02:07:15,160 --> 02:07:20,120
We can quickly discuss the MPI parallelism along with GPUs.

1128
02:07:20,120 --> 02:07:26,360
So does that sound good?

1129
02:07:26,360 --> 02:07:28,480
Yeah.

1130
02:07:28,480 --> 02:07:29,200
OK.

1131
02:07:29,200 --> 02:07:31,120
Yeah.

1132
02:07:31,120 --> 02:07:36,840
Yeah, so let's see.

1133
02:07:36,840 --> 02:07:40,920
What else is there?

1134
02:07:40,920 --> 02:07:43,160
I think [name] already answered the processes

1135
02:07:43,160 --> 02:07:44,360
versus processors.

1136
02:07:47,080 --> 02:07:50,600
So if you want to go deep into the computer architecture,

1137
02:07:50,600 --> 02:07:52,720
you can learn all kinds of things

1138
02:07:52,720 --> 02:07:56,680
about how Unix systems handle processes and threads

1139
02:07:56,680 --> 02:07:58,440
and things like that.

1140
02:07:58,440 --> 02:08:00,360
But for the most part, if you know

1141
02:08:00,360 --> 02:08:02,720
the options that are present in this page,

1142
02:08:02,720 --> 02:08:05,460
you can figure out how to run something,

1143
02:08:05,460 --> 02:08:08,480
and then you don't need to know more.

1144
02:08:13,040 --> 02:08:16,000
You're probably here because you want to do

1145
02:08:16,000 --> 02:08:20,760
science stuff and not learn about computer architecture.

1146
02:08:20,760 --> 02:08:24,040
I will quickly mention that a common thing that

1147
02:08:24,040 --> 02:08:27,260
happens to many researchers is that,

1148
02:08:27,260 --> 02:08:29,640
let's say you run a code in your laptop,

1149
02:08:29,640 --> 02:08:34,840
And your laptop, like I mentioned previously, they nowadays have multiple processors there.

1150
02:08:35,400 --> 02:08:41,800
And in many cases, if you run this code that can do multiprocessing internally,

1151
02:08:41,800 --> 02:08:48,360
it can do something with parallel, without you noticing, basically. You don't even notice.

1152
02:08:48,360 --> 02:08:53,480
The Zoom that I'm using now, it's doing something multiprocessing on the background.

1153
02:08:53,480 --> 02:08:59,320
I don't even know about it, but it's doing it to make certain that the video stream is correct.

1154
02:08:59,320 --> 02:09:03,480
and audio is processed correctly. It's doing multi-processing in the background.

1155
02:09:04,760 --> 02:09:11,240
And in my laptop, it just assumes that, okay, I can use whatever resources are available there.

1156
02:09:11,800 --> 02:09:19,400
And when people and resources go to the cluster where you need to actually request the resources,

1157
02:09:19,400 --> 02:09:27,480
the programs usually just think that, okay, they will look at the hardware that is there.

1158
02:09:27,480 --> 02:09:36,520
they see a server with 128 CPUs. And maybe when in the job request,

1159
02:09:38,520 --> 02:09:45,800
you might forget to add the CPUs per task flag. So then when you run the code in the cluster,

1160
02:09:45,800 --> 02:09:52,920
it's suddenly much slower. It suddenly takes four times as much time or eight times as much time.

1161
02:09:52,920 --> 02:09:56,680
And then you might wonder, like, okay, what's going wrong?

1162
02:09:56,680 --> 02:09:58,040
Like, am I doing something wrong?

1163
02:09:58,040 --> 02:10:00,600
Because it's so much slower in the computing cluster.

1164
02:10:00,600 --> 02:10:04,840
Maybe the computing cluster is a pile of crap and I don't want to use that.

1165
02:10:05,480 --> 02:10:15,400
But the reality is that the program doesn't necessarily know about what number of processors

1166
02:10:15,400 --> 02:10:16,120
it should use.

1167
02:10:16,760 --> 02:10:21,800
And also, the program doesn't have access to all of those processors, because those

1168
02:10:21,800 --> 02:10:30,920
are being reserved by other jobs in the queue. So when you're running a program in the cluster,

1169
02:10:30,920 --> 02:10:37,400
it's usually a good idea to just run it with multiple CPUs per task and see how it goes.

1170
02:10:37,400 --> 02:10:43,000
And you can also look at your own task manager in your own laptop and see how many processes,

1171
02:10:43,000 --> 02:10:49,800
do you see the processes being used while I run my code? And just decipher, does it use some

1172
02:10:49,800 --> 02:10:51,000
Multiprocessing.

1173
02:10:51,000 --> 02:10:52,680
Yeah.

1174
02:10:52,680 --> 02:10:54,320
OK, so we need to go to the break

1175
02:10:54,320 --> 02:10:57,320
so we don't get too far behind.

1176
02:10:57,320 --> 02:11:10,240
So break until one minute past the hour.

1177
02:11:10,240 --> 02:11:13,960
And then we have to talk on CSC resources.

1178
02:11:13,960 --> 02:11:15,960
So see you soon.

1179
02:11:18,520 --> 02:11:19,640
Bye.

1180
02:11:19,640 --> 02:11:20,140
Bye.

1181
02:11:49,640 --> 02:11:51,700
you

1182
02:12:19,640 --> 02:12:21,700
you

1183
02:12:49,640 --> 02:12:51,700
you

1184
02:13:19,640 --> 02:13:21,700
you

1185
02:13:49,640 --> 02:13:51,700
you

1186
02:14:19,640 --> 02:14:21,700
you

1187
02:14:49,640 --> 02:14:51,700
you

1188
02:15:19,640 --> 02:15:21,700
you

1189
02:15:49,640 --> 02:15:51,700
you

1190
02:16:19,640 --> 02:16:21,700
you

1191
02:16:49,640 --> 02:16:51,700
you

1192
02:17:19,640 --> 02:17:21,700
you

1193
02:17:49,640 --> 02:17:51,700
you

1194
02:18:19,640 --> 02:18:21,700
you

1195
02:18:49,640 --> 02:18:51,700
you

1196
02:19:19,640 --> 02:19:21,700
you

1197
02:19:49,640 --> 02:19:51,700
you

1198
02:20:19,640 --> 02:20:21,700
you

1199
02:20:49,640 --> 02:20:51,700
you

1200
02:21:19,640 --> 02:21:28,280
Hello, welcome back.

1201
02:21:28,280 --> 02:21:36,560
So now with us, we have a visitor from CSC, [name].

1202
02:21:36,560 --> 02:21:38,640
I hope I got that correct.

1203
02:21:38,640 --> 02:21:42,400
Yeah, pretty, pretty good.

1204
02:21:42,400 --> 02:21:46,760
But [name] works at CSC, the IT Center for Science.

1205
02:21:46,760 --> 02:21:55,060
So CSC is the national high-performance computing center in Finland, but not just that.

1206
02:21:55,060 --> 02:22:00,260
It's actually a whole lot more than just the services you see here.

1207
02:22:00,260 --> 02:22:03,700
And even what you see here isn't just high-performance computing.

1208
02:22:03,700 --> 02:22:11,420
But anyway, CSC is paid by the Ministry of Culture and Education, I believe, to provide

1209
02:22:11,420 --> 02:22:20,220
services for free to academic researchers and others within Finland. So CSC, like all the

1210
02:22:20,220 --> 02:22:28,300
things we've been talking about today, CSC has similar but larger, a bigger scale. It might be

1211
02:22:28,300 --> 02:22:34,620
a little bit more work to use because you have to apply for the resources and migrate to there.

1212
02:22:34,620 --> 02:22:39,620
but if you need power or cross-university collaboration,

1213
02:22:39,620 --> 02:22:41,380
this is the place to go.

1214
02:22:43,580 --> 02:22:45,740
And was that a good intro?

1215
02:22:45,740 --> 02:22:46,580
Should you?

1216
02:22:46,580 --> 02:22:48,860
That was an excellent intro.

1217
02:22:48,860 --> 02:22:49,900
Okay, yeah.

1218
02:22:49,900 --> 02:22:51,460
So I will stay here.

1219
02:22:51,460 --> 02:22:54,900
I'll be looking for questions in the notes

1220
02:22:54,900 --> 02:22:57,180
and raise them at an appropriate time.

1221
02:22:57,180 --> 02:22:59,700
So go ahead.

1222
02:22:59,700 --> 02:23:03,340
Okay, thanks [name] for introduction.

1223
02:23:03,340 --> 02:23:13,020
So, I will tell, I mean, you already heard in brief what CSC is, I might tell a little bit more

1224
02:23:13,820 --> 02:23:21,500
and then I give a bit more details at what kind of computing services CSC provides and for what

1225
02:23:21,500 --> 02:23:30,700
kind of use cases you might want to use our computing services. Science for a long time,

1226
02:23:30,700 --> 02:23:36,020
even I mean if you're using computers hasn't been only about computing so data

1227
02:23:36,020 --> 02:23:44,540
is also a big aspect aspect and we have also some services more more related to

1228
02:23:44,540 --> 02:23:49,020
data I'll discuss them briefly and finally some something else like

1229
02:23:49,020 --> 02:23:55,820
training we provide and of course if you want or need to use CSC services what

1230
02:23:55,820 --> 02:24:05,580
you actually need to do for that. As said, we are owned by the Ministry of Education,

1231
02:24:05,580 --> 02:24:11,580
which owns about 80% of the shares and the rest 20% is actually owned by the Finnish

1232
02:24:11,580 --> 02:24:20,860
universities. We are a non-profit company and we provide a lot of different services

1233
02:24:20,860 --> 02:24:24,380
for the science and higher education.

1234
02:24:24,380 --> 02:24:26,180
I didn't actually check now

1235
02:24:26,180 --> 02:24:29,500
what's the latest number of staff we have,

1236
02:24:29,500 --> 02:24:33,180
but I think it's starting to be close to 700

1237
02:24:33,180 --> 02:24:34,860
or something like that.

1238
02:24:34,860 --> 02:24:39,860
And out from the 700, maybe 120 or 130

1239
02:24:40,300 --> 02:24:42,940
work actually with the scientific computing

1240
02:24:42,940 --> 02:24:44,520
and supercomputing.

1241
02:24:45,460 --> 02:24:49,060
And even though we are sort of discussing here

1242
02:24:49,060 --> 02:24:56,100
that CSC Computing Services and what you need to do in order to get to be used.

1243
02:24:56,100 --> 02:25:00,180
I actually can assure you that all of you already at this point,

1244
02:25:00,180 --> 02:25:07,380
you are CSC users. If you have ever used any network

1245
02:25:07,380 --> 02:25:11,220
within the university, the backbone that's provided by

1246
02:25:11,220 --> 02:25:17,780
FUNET, pretty much always when you log into your any university web

1247
02:25:17,780 --> 02:25:23,620
services, so you're using account authentication. If you're using Wi-Fi, you're most likely using

1248
02:25:23,620 --> 02:25:30,740
Eduroam. So these are sort of these other services and I think many of the

1249
02:25:32,500 --> 02:25:39,540
standard administration systems, probably if you look for your course results and things like that,

1250
02:25:39,540 --> 02:25:47,380
there is very probably some CSC mode also there. And coming back to the computing services,

1251
02:25:47,780 --> 02:25:59,780
And things I'm talking here, as I already said, what's probably of big interest for many of you is that you don't actually need to pay most of this any money.

1252
02:25:59,780 --> 02:26:05,780
So that's already paid by the Ministry of Education.

1253
02:26:05,780 --> 02:26:13,780
Okay, in what kind of situation you might need CSC services?

1254
02:26:13,780 --> 02:26:22,340
I think you can also, first question is that when you actually need to use a HPC cluster and even now

1255
02:26:23,860 --> 02:26:28,020
that's probably something you discussed already earlier today but just to remind you

1256
02:26:28,740 --> 02:26:36,580
supercomputer is not any machine which automatically makes everything faster.

1257
02:26:36,580 --> 02:26:42,580
So actually the supercomputer to be useful, you need to have some

1258
02:26:42,580 --> 02:26:46,580
parallelism in your scientific task.

1259
02:26:46,580 --> 02:26:54,580
Actually, if I take, I could say, pretty much any simulation or so

1260
02:26:54,580 --> 02:26:59,580
that runs on a single core and can fit to the memory of my laptop,

1261
02:26:59,580 --> 02:27:03,580
I'm pretty sure that my laptop would actually run faster

1262
02:27:03,580 --> 02:27:07,580
than any of the CSC supercomputers.

1263
02:27:07,580 --> 02:27:11,580
So you need to have something that runs parallelly.

1264
02:27:11,580 --> 02:27:14,580
Sometimes it also might just be that

1265
02:27:14,580 --> 02:27:18,580
you just need to run something for quite a long time

1266
02:27:18,580 --> 02:27:25,580
and because of that using your own workstation or laptop is not useful.

1267
02:27:25,580 --> 02:27:30,580
Second thing might be that actually the problem you are trying to solve

1268
02:27:30,580 --> 02:27:38,980
needs lots of memory and that's something that the supercomputers can probably have much more memory.

1269
02:27:39,540 --> 02:27:45,700
Same goes for storage space. You need to deal with the terabytes of data.

1270
02:27:46,660 --> 02:27:51,620
That's most likely something you don't want to deal with your local port space.

1271
02:27:51,620 --> 02:28:04,820
One more reason to use CSC or some other HPC cluster is that the application you want to use,

1272
02:28:04,820 --> 02:28:11,700
it might be a commercial application that the principle user pays something for that, but the

1273
02:28:12,340 --> 02:28:18,420
CSC has actually bought the license for that, and in terms of the license, academic users can use

1274
02:28:18,420 --> 02:28:24,500
that free of charge. Or even though it might be an open source application, it might be something

1275
02:28:24,500 --> 02:28:30,340
which is a tedious install, have a lot of dependencies, and having data set ready to use

1276
02:28:31,860 --> 02:28:38,100
in a supercomputing system might be might be benefit. Regarding the data,

1277
02:28:39,700 --> 02:28:46,020
if you need to share data for other researchers, your colleagues, some of the services that CSC

1278
02:28:46,020 --> 02:28:52,420
provide might turn out to be useful and especially if you actually want to publish the data that's

1279
02:28:53,060 --> 02:29:00,820
that's something that I'm pretty sure it's very difficult for you to do in them according

1280
02:29:00,820 --> 02:29:09,060
all the principles on your own. Yeah we basically don't have our own long-term repositories and

1281
02:29:09,060 --> 02:29:17,220
say that's CSC's job. Yeah and I mean to get some DOEs for your data and I mean

1282
02:29:17,220 --> 02:29:21,700
something where other people can actually also cite your data if you if you share it.

1283
02:29:25,700 --> 02:29:34,260
So yeah this is why you might need CSC. You have something parallel, you need lots of memory

1284
02:29:34,260 --> 02:29:45,620
or storage-based application or data. You've already learned quite a bit about the university

1285
02:29:45,620 --> 02:29:53,620
clusters, so this question is of course that how do our computers actually differ from those?

1286
02:29:54,500 --> 02:30:02,420
And the answer is, from a user point of view, actually very little. The main difference really is in scale.

1287
02:30:02,420 --> 02:30:12,420
how many GPU cores, how many GPUs you can use, how much storage space you have, etc.

1288
02:30:12,420 --> 02:30:22,420
As an example, in Mahti, or actually also in Lumi CPU Partition, you can, if you need,

1289
02:30:22,420 --> 02:30:28,420
you could be running something like 25,000 cores your simulation, or if you need to use

1290
02:30:28,420 --> 02:30:40,340
hundreds of GPUs that's in principle available in Lumi. The way you access

1291
02:30:40,340 --> 02:30:46,180
CSC supercomputers is pretty much the same as you would access for example Triton, I

1292
02:30:46,180 --> 02:30:51,700
mean you can use SSH from your terminal to get the command-line based

1293
02:30:51,700 --> 02:30:58,260
access and this is actually I realize a valid point I did not update I think

1294
02:30:58,260 --> 02:31:05,700
the web GUI, it's not really as such so new. So in some terms new service, but I mean that's

1295
02:31:05,700 --> 02:31:12,580
something we have had already for three years. And I think in Triton, how long you have actually had

1296
02:31:12,580 --> 02:31:19,860
that? A little bit, but it's only become a big thing in the last year. We've only really been

1297
02:31:19,860 --> 02:31:27,140
starting pushing it for basically the new Triton in the last month. So yeah. But once again,

1298
02:31:27,140 --> 02:31:30,740
then something that's also available there.

1299
02:31:30,740 --> 02:31:34,100
And I think especially for these web interfaces,

1300
02:31:34,100 --> 02:31:36,260
I mean, if you used in one system,

1301
02:31:36,260 --> 02:31:40,120
it's more or less same way to use that.

1302
02:31:41,500 --> 02:31:43,540
When using from command line,

1303
02:31:43,540 --> 02:31:48,380
we also have a module system to selecting compilers

1304
02:31:48,380 --> 02:31:49,540
and so on.

1305
02:31:49,540 --> 02:31:51,340
And in the batch queue system,

1306
02:31:51,340 --> 02:31:54,140
I think most of the supercomputing sites,

1307
02:31:54,140 --> 02:31:57,500
only CSC actually use Slurm.

1308
02:31:57,500 --> 02:32:01,420
So differences in usage are mainly

1309
02:32:02,580 --> 02:32:05,060
what are the partition names and et cetera.

1310
02:32:05,060 --> 02:32:05,880
So.

1311
02:32:07,460 --> 02:32:08,300
Yeah.

1312
02:32:09,740 --> 02:32:11,980
One big difference of course is that

1313
02:32:13,100 --> 02:32:18,100
when you want to use CSC computing services,

1314
02:32:18,260 --> 02:32:20,900
you actually need to apply project for that

1315
02:32:20,900 --> 02:32:23,780
and you need to apply for some billing units

1316
02:32:23,780 --> 02:32:27,580
which you could think as CSC money or something like that.

1317
02:32:27,580 --> 02:32:29,860
And when you do competitions,

1318
02:32:29,860 --> 02:32:32,380
so actually nowadays also for data,

1319
02:32:32,380 --> 02:32:34,100
you have a lot of data on the disk

1320
02:32:34,100 --> 02:32:37,220
that also consume these billing units.

1321
02:32:37,220 --> 02:32:40,900
And it's one way we try to ensure

1322
02:32:40,900 --> 02:32:45,900
that the users actually think what they are doing.

1323
02:32:46,500 --> 02:32:49,060
It's often too easy.

1324
02:32:49,060 --> 02:32:51,980
I mean, I've been there, I've done that,

1325
02:32:51,980 --> 02:32:58,700
do some stupid things with the computer and waste resources, so that tries to assure that

1326
02:33:00,300 --> 02:33:05,740
users use the resources wise. Yeah, that's probably the biggest difference between

1327
02:33:05,740 --> 02:33:11,420
CSC and us. So for us, you just apply for the account, but you don't apply for or account for

1328
02:33:11,420 --> 02:33:18,300
individual resources separately, but there's no guaranteed amount available to you. At CSC,

1329
02:33:18,300 --> 02:33:22,140
you apply for the resources but there's much more available and

1330
02:33:22,140 --> 02:33:27,660
you're much more likely to get what you apply for within a quick time.

1331
02:33:27,660 --> 02:33:31,340
I'll come in the end of the presentation what is actually

1332
02:33:31,340 --> 02:33:37,660
what is needed for applying this and by the way in the way you you need

1333
02:33:37,660 --> 02:33:42,380
to apply for resources comes also that you typically also need

1334
02:33:42,380 --> 02:33:46,860
to report something especially I mean when you pre-apply some resources you

1335
02:33:46,860 --> 02:33:50,900
you need to somehow report that, what you use them for,

1336
02:33:50,900 --> 02:33:53,140
what kind of publication you make, and so.

1337
02:33:53,140 --> 02:33:53,980
Yeah.

1338
02:33:55,700 --> 02:33:56,540
Okay.

1339
02:33:57,820 --> 02:34:00,620
Then a bit about what are the computing,

1340
02:34:00,620 --> 02:34:05,220
or what kind of computing resources or services we have.

1341
02:34:06,180 --> 02:34:10,820
At the moment, we have two national supercomputers

1342
02:34:10,820 --> 02:34:15,820
which are available for basically to the users

1343
02:34:15,820 --> 02:34:20,140
users coming from Finnish universities and higher education institutions.

1344
02:34:20,340 --> 02:34:24,060
So we have Puhti and we have Mahti.

1345
02:34:24,260 --> 02:34:29,820
They both are actually in a sort of end part of their life.

1346
02:34:30,020 --> 02:34:32,860
So we are currently

1347
02:34:33,060 --> 02:34:38,060
or we have started already the sort of process of buying the next national

1348
02:34:38,260 --> 02:34:43,780
supercomputer, probably by the end of next year,

1349
02:34:43,780 --> 02:34:50,900
2025 Puhti and Mahtti will retire and we'll have a new system. What that's going to be,

1350
02:34:50,900 --> 02:34:56,180
I do not know and something we'll see with the negotiation with the vendors.

1351
02:34:58,180 --> 02:35:03,700
Puhti and Mahtti, they have a little bit different user profiles, so Puhti's,

1352
02:35:03,700 --> 02:35:10,260
you can somehow think that it's a bit more general purpose for smaller parallel jobs,

1353
02:35:10,260 --> 02:35:18,260
having Intel CPUs and some NVIDIA bit older generation GPUs,

1354
02:35:18,260 --> 02:35:23,260
MAHDI, SmartWave, Kietow Arts, medium and large scale

1355
02:35:23,260 --> 02:35:29,260
parallel simulations. For example, for most of the partitions,

1356
02:35:29,260 --> 02:35:35,260
the minimum number of CPU cores you are using,

1357
02:35:35,260 --> 02:35:38,380
using that single node, which it's in that system

1358
02:35:38,380 --> 02:35:43,380
with the AMD CPUs, it's always 128.

1359
02:35:43,540 --> 02:35:47,020
Mahti has also a little bit smaller GPU partition

1360
02:35:48,020 --> 02:35:52,340
than Puhti with the newer GPUs.

1361
02:35:52,340 --> 02:35:56,540
And what might be useful for some of you

1362
02:35:56,540 --> 02:36:00,500
for interactive workloads, development, and so on,

1363
02:36:00,500 --> 02:36:04,460
is that some of these GPUs can also actually be

1364
02:36:05,260 --> 02:36:11,100
slice the smaller ones so you do not, as the resources is a bit more limited, there are I

1365
02:36:11,100 --> 02:36:16,700
think only 24 of these nodes but for purposes where you do not need the full computational

1366
02:36:16,700 --> 02:36:23,340
power of single GPU you can get only a subset of that and do some useful stuff with that.

1367
02:36:23,340 --> 02:36:38,460
Then, as a sort of flagship, there is also LUMi. LUMi is actually not really a CSC supercomputer,

1368
02:36:38,460 --> 02:36:47,340
so it's actually truly a pan-European supercomputer, funding coming from the European Union and 11

1369
02:36:47,340 --> 02:36:55,980
countries belonging to Lumi Consortium, but it's hosted in the CSC data center.

1370
02:36:57,580 --> 02:37:04,460
User support is not directly given by CSC, so it's distributed to these Lumi Consortium countries,

1371
02:37:05,420 --> 02:37:12,380
but if you want to use that for Finnish users, you can access it via CSC

1372
02:37:12,380 --> 02:37:16,860
in applied resources a similar way as to Puhti and Matti.

1373
02:37:19,500 --> 02:37:25,580
Lumi at the moment is the most powerful supercomputer in Europe and the fifth most

1374
02:37:25,580 --> 02:37:35,420
powerful in the world, having over 10 000 AMD GPUs. So it's really a massive compute resource.

1375
02:37:35,420 --> 02:37:44,460
There is also a CPU part of that, which still is a quad-signal count, it's around 200,000

1376
02:37:44,460 --> 02:37:50,140
CPU cores also there.

1377
02:37:50,140 --> 02:37:58,340
Now with all this multitude of different supercomputers CSC has, you might ask, okay, which one of

1378
02:37:58,340 --> 02:38:01,700
these I should actually use?

1379
02:38:01,700 --> 02:38:07,700
And as I said, it really depends a bit on what kind of satellite task they have, a bit different profiles.

1380
02:38:07,700 --> 02:38:14,700
As I said, Puhti is maybe more for small or medium scale parallel tasks.

1381
02:38:14,700 --> 02:38:20,700
It has the most extensive software selection available,

1382
02:38:20,700 --> 02:38:30,700
while Mahti, as I said, is more for the larger parallel calculations.

1383
02:38:30,700 --> 02:38:35,660
duration of your of your simulation in

1384
02:38:35,660 --> 02:38:37,340
market the maximum time you get random

1385
02:38:37,340 --> 02:38:38,060
queue

1386
02:38:38,060 --> 02:38:40,220
and that's also a bit shorter than in

1387
02:38:40,220 --> 02:38:42,380
than in Puhti.

1388
02:38:42,380 --> 02:38:45,580
And Lumi I would say that

1389
02:38:45,580 --> 02:38:47,100
Lumi is really if you have an

1390
02:38:47,100 --> 02:38:49,500
application that can really benefit from

1391
02:38:49,500 --> 02:38:51,340
AMD GPUs

1392
02:38:51,340 --> 02:38:52,860
that's that's when you really should

1393
02:38:52,860 --> 02:38:54,700
consider Lumi.

1394
02:38:54,700 --> 02:38:57,100
Some projects you might might want to

1395
02:38:57,100 --> 02:38:59,340
run also the CPUs but

1396
02:38:59,340 --> 02:39:07,340
they do not provide that much advantage over Mahti. Software availability in Lumi is actually

1397
02:39:07,340 --> 02:39:16,300
a bit more limited, and talking about the module systems, the Cray module system that is in Lumi,

1398
02:39:16,300 --> 02:39:21,740
it's in some ways a bit more, in some cases, a bit more complicated to use than

1399
02:39:22,780 --> 02:39:28,140
the ones that we have in Puhti and Mahti, which are, I would say, more similar to Triton.

1400
02:39:29,340 --> 02:39:44,220
About the web interface, I'd say this is something that nowadays you can use all the three supercomputers,

1401
02:39:44,220 --> 02:39:54,580
either Puhti, Matti or Lumi, and here you can see the sort of selection of applications

1402
02:39:54,580 --> 02:39:57,260
within this web interface that you can use, for example,

1403
02:39:57,260 --> 02:39:59,420
in Puhti.

1404
02:39:59,420 --> 02:40:02,300
I think, personally, I use the Jupyter Notebooks a lot.

1405
02:40:02,300 --> 02:40:04,540
That's quite nice.

1406
02:40:04,540 --> 02:40:07,260
You can use it in MATLAB or RStudio.

1407
02:40:07,260 --> 02:40:11,500
That also might be quite convenient.

1408
02:40:11,500 --> 02:40:15,620
And if you need to use some graphical applications using

1409
02:40:15,620 --> 02:40:18,940
the NVIDIA web desktop might be also a good option.

1410
02:40:18,940 --> 02:40:28,940
In addition to the kind of traditional HPC services,

1411
02:40:28,940 --> 02:40:33,940
CSC provides also some cloud-like resources.

1412
02:40:33,940 --> 02:40:40,940
They might become useful if you have the operating system,

1413
02:40:40,940 --> 02:40:45,940
the system libraries and so on, that are available in supercomputers,

1414
02:40:45,940 --> 02:40:51,540
not always the latest possible. I mean the main objective for the for the

1415
02:40:51,540 --> 02:40:56,940
system is that it needs to be stable and sometimes you might need a bit more

1416
02:40:56,940 --> 02:41:01,660
flexibility. You really would like to run your own operating system or

1417
02:41:01,660 --> 02:41:09,100
things like that and for this kind of usage you might want to consider some of

1418
02:41:09,100 --> 02:41:15,260
the cloud services. Basically we have three types of cloud services. We have a

1419
02:41:15,260 --> 02:41:21,820
a CPoL as a general computing cloud, then I don't know, are

1420
02:41:21,820 --> 02:41:29,180
the Antisodius users doing genomics or things like that

1421
02:41:29,180 --> 02:41:30,860
where the data is actually sensitive?

1422
02:41:30,860 --> 02:41:34,140
Some are. We talked about that a bit on day one, and I think we

1423
02:41:34,140 --> 02:41:37,020
mentioned the CSC services, so here.

1424
02:41:37,020 --> 02:41:42,060
So for this kind of thing, there is the ePoL, and then there is

1425
02:41:42,060 --> 02:41:50,060
also this Pirate Container Cloud that can be useful for some things.

1426
02:41:50,460 --> 02:41:54,860
I heard that some of you earlier were

1427
02:41:54,860 --> 02:41:57,740
somehow interested but also about the programming.

1428
02:41:57,740 --> 02:42:03,180
I won't go in much details how to actually do the parallel programming use.

1429
02:42:03,180 --> 02:42:06,940
This is just the very brief overview what kind of

1430
02:42:06,940 --> 02:42:11,020
approaches one typically uses using supercomputers and I mean what

1431
02:42:11,020 --> 02:42:18,460
you can use in if you're using a CSC. So of course the basic C, C++, C, Fortran, Python,

1432
02:42:18,460 --> 02:42:25,340
Archulia. For parallel programming you can use MPI for distributed memory parallelism,

1433
02:42:25,340 --> 02:42:33,500
more openMP for shared memory. There are some high-performance libraries and GPUs.

1434
02:42:35,180 --> 02:42:39,980
If you're doing machine learning you might be using PyTorch or something like that which is

1435
02:42:39,980 --> 02:42:45,340
be readily available in the system or if you want to do the GPU graph on yourself

1436
02:42:46,300 --> 02:42:54,780
you might be using OpenMP offloading, OpenACC, CUDA, the HIP. CSC provides also some

1437
02:42:55,420 --> 02:43:02,940
performance analysis and debugging tools for parallel applications. So maybe you mention

1438
02:43:02,940 --> 02:43:08,940
this later but do you have training for all of these different types of things? Yes, I mentioned

1439
02:43:08,940 --> 02:43:12,380
bit of I'll come to try a little bit at the end of the talk.

1440
02:43:12,380 --> 02:43:17,180
The technical details I think I will now

1441
02:43:17,180 --> 02:43:20,780
skip but I'm in the end of slides so if you're interested you can

1442
02:43:20,780 --> 02:43:26,540
you can have some of the actual numbers checked there. You should find pretty

1443
02:43:26,540 --> 02:43:31,820
much the same information also from the CSC web pages or user

1444
02:43:31,820 --> 02:43:34,940
documentation. I'll include that also in the

1445
02:43:34,940 --> 02:43:41,580
presentation. Okay, let us come then to the other

1446
02:43:41,580 --> 02:43:47,020
aspect I mentioned that you might be dealing directly with

1447
02:43:47,020 --> 02:43:50,940
data or when you're doing the computations

1448
02:43:50,940 --> 02:43:56,620
and you're creating data that you would like to do also a little bit

1449
02:43:56,620 --> 02:44:01,100
more than just have it in the

1450
02:44:01,100 --> 02:44:04,940
in the file system in the supercomputer.

1451
02:44:05,340 --> 02:44:10,540
One data service we have is the ALLAS object storage

1452
02:44:10,540 --> 02:44:17,180
and you can upload the data from the supercomputer or actually from

1453
02:44:17,180 --> 02:44:21,180
anywhere so that's something the data which is in the

1454
02:44:21,180 --> 02:44:24,460
supercomputer file system or HPC file system

1455
02:44:24,460 --> 02:44:28,700
you can only access from the HPC system but the data you put in this object

1456
02:44:28,700 --> 02:44:36,380
storage you can in principle access from anywhere from the internet and you can also have the data

1457
02:44:36,380 --> 02:44:42,140
shared to other users. Lumi has actually its own optic storage also which is

1458
02:44:44,300 --> 02:44:52,140
it's a physically separate from that so if you you can use Allas either from Lumi or from

1459
02:44:52,140 --> 02:44:57,300
Mahti and Puhti. Lumi has this Lumi own object storage, which

1460
02:44:57,300 --> 02:45:00,860
similar you can actually use from from different places.

1461
02:45:01,340 --> 02:45:01,740
Okay.

1462
02:45:03,900 --> 02:45:09,780
CSC provides also some, I don't know if the data findable

1463
02:45:09,820 --> 02:45:16,500
accessible, interoperable, reusable is a concept common to

1464
02:45:16,500 --> 02:45:20,020
you. But basically it means that you can you can publish your

1465
02:45:20,020 --> 02:45:23,380
data set and together with the data set there is also

1466
02:45:23,380 --> 02:45:27,220
some metadata and there is also some tools for

1467
02:45:27,220 --> 02:45:34,260
searching these data sets so CSC has some services

1468
02:45:34,260 --> 02:45:40,420
for example EDA is one such a service that we provide.

1469
02:45:40,420 --> 02:45:49,420
Okay, training was already mentioned by [name].

1470
02:45:49,420 --> 02:45:57,420
So we have a quite a large amount of training every year in various aspects.

1471
02:45:57,420 --> 02:46:03,420
So supercomputing, just how to use our systems, might be related just to Puhti or Mahti,

1472
02:46:03,420 --> 02:46:06,420
might be related how to use Lumi,

1473
02:46:06,420 --> 02:46:10,180
might be some programming course of parallel programming

1474
02:46:10,180 --> 02:46:14,260
or might be just how to use certain static applications like

1475
02:46:14,260 --> 02:46:17,060
like Chromux.

1476
02:46:18,340 --> 02:46:21,700
There is also possible to get some help if you

1477
02:46:21,700 --> 02:46:25,860
have a data set that you would like to visualize.

1478
02:46:25,860 --> 02:46:29,780
Something more than just making your xy graphs.

1479
02:46:29,780 --> 02:46:37,300
CSC can provide help and for example you can develop your own

1480
02:46:37,300 --> 02:46:42,180
scientific application and would need some help in getting more

1481
02:46:42,180 --> 02:46:46,340
performance from that. That's something where CSC can also

1482
02:46:46,340 --> 02:46:50,980
help. Yeah so if someone say wants to learn

1483
02:46:50,980 --> 02:46:55,940
MPI program or OpenMP programming or GPU programming you have these

1484
02:46:55,940 --> 02:47:01,620
kind of courses? Or links to them from Lumi or something?

1485
02:47:01,660 --> 02:47:06,500
Yeah, yeah, yeah, definitely. Yeah. Here are just examples of

1486
02:47:07,060 --> 02:47:11,700
some of the courses we are currently offering. So I think

1487
02:47:11,700 --> 02:47:16,580
if you're just want to learn a bit more about the

1488
02:47:16,580 --> 02:47:20,740
supercomputers in sort of general level, we have this

1489
02:47:20,740 --> 02:47:27,300
online course, Elements of Supercomputing, should be quite accessible and might be

1490
02:47:28,420 --> 02:47:34,580
and has been developed in principle for mostly people who don't necessarily have any previous

1491
02:47:34,580 --> 02:47:43,460
background of HPC, so it's quite general level. We have also this ongoing online course about

1492
02:47:43,460 --> 02:47:50,740
how to use CSC computing environment. We're going to have in the June-July time frame,

1493
02:47:51,460 --> 02:47:57,940
we have now for, I think this is now 13th time, we have in the summer school about high-performance

1494
02:47:57,940 --> 02:48:04,580
computing. That's a deadline for that has already passed, but that's sort of 10 days

1495
02:48:04,580 --> 02:48:12,500
is where you really go through all the basic aspects of how to

1496
02:48:12,500 --> 02:48:16,660
do parallel programming with the MPI, how to use OpenMP, how to

1497
02:48:16,660 --> 02:48:23,100
do GPU programming. So I think that's something if you if you

1498
02:48:23,100 --> 02:48:27,140
think that you will be doing a parallel program, something you

1499
02:48:27,140 --> 02:48:31,660
might want to look for in next year, I think that for next year

1500
02:48:31,660 --> 02:48:38,220
registration probably opens somewhere in January, February. Some of the courses we

1501
02:48:38,220 --> 02:48:41,740
already have in calendar of these hardware formats are in September.

1502
02:48:44,460 --> 02:48:54,140
Okay. Yeah, did you have some comment [name]? No, no, please continue. Okay, so all these

1503
02:48:54,140 --> 02:49:02,060
wonderful services we provide, so how you how you get to use them. First of all, similar to for

1504
02:49:02,060 --> 02:49:13,820
example Triton, you need to have an account and for CSC if you already have Haka available, I mean

1505
02:49:13,820 --> 02:49:19,980
you have been using Haka in your university, you can just go to my.csc.fi and with a few mouse

1506
02:49:19,980 --> 02:49:27,180
clicks you will get the csc user account. Username will probably be different.

1507
02:49:27,900 --> 02:49:34,620
Fast forward ssh keys so that you will be using the DNS cluster but you do not need any

1508
02:49:35,260 --> 02:49:42,860
really extra steps other than the account navigates. As discussed, in order to really

1509
02:49:42,860 --> 02:49:51,900
get some computational resources and use them there needs to be a project for that and you

1510
02:49:51,900 --> 02:50:01,180
one needs to apply billing units for the project and for that not not quite anybody can apply for

1511
02:50:01,180 --> 02:50:07,260
the project but on the other hand it doesn't have to be the big processor so basically

1512
02:50:07,260 --> 02:50:13,820
experienced researcher postdoc or something like that. I mean if you're doing some research work

1513
02:50:13,820 --> 02:50:20,620
there is typically some more experienced researcher that can apply for the project

1514
02:50:21,260 --> 02:50:28,860
and he or she can apply for billing unit and with the project the project manager can in principle

1515
02:50:28,860 --> 02:50:40,140
add any CSC user to the project. Maybe not so relevant for this audience but just to let you

1516
02:50:40,140 --> 02:50:48,140
know that if you're teaching a course and you want to use some CSC resources for that so that is also

1517
02:50:48,860 --> 02:50:56,780
possible to apply for a project for that. For courses you typically do not need to apply

1518
02:50:56,780 --> 02:51:03,980
billing units. So there is always a fixed amount of resources given there. The duration

1519
02:51:03,980 --> 02:51:08,740
is also typically fixed to six months, so it's a bit shorter too. And once a month

1520
02:51:08,740 --> 02:51:14,820
you need to then have the CSC account in order to actually use these resources within the

1521
02:51:14,820 --> 02:51:15,820
course.

1522
02:51:15,820 --> 02:51:21,780
Yeah. And I can say sometimes people come and ask us, oh, what do these questions mean

1523
02:51:21,780 --> 02:51:27,140
when applying for resources, but when in doubt, just send an email to the CSC service desk

1524
02:51:27,140 --> 02:51:33,860
or even in, if you're that brave, call and ask for details.

1525
02:51:33,860 --> 02:51:40,740
And I've always gotten good answers that way and, you know, some solution.

1526
02:51:40,740 --> 02:51:45,260
So yeah, it is some work, but don't be afraid.

1527
02:51:45,260 --> 02:51:46,260
Yeah.

1528
02:51:46,260 --> 02:51:47,260
Okay.

1529
02:51:47,260 --> 02:51:54,260
And I mean, when you have these questions, so yeah, we have the service desk.

1530
02:51:54,260 --> 02:52:00,180
If you go to our main webpages, you can actually find the phone numbers there, all the user

1531
02:52:00,180 --> 02:52:02,100
guides and so on.

1532
02:52:02,100 --> 02:52:08,180
You can go to indux.csc.fi, free data and so on, and all the trainings we have coming

1533
02:52:08,180 --> 02:52:09,180
there.

1534
02:52:09,180 --> 02:52:11,860
You can see there.

1535
02:52:11,860 --> 02:52:22,420
So us, the Aalto Scientific Computing team here, we also do, we support CSC things,

1536
02:52:22,980 --> 02:52:30,180
if you bring questions to us. So maybe you wouldn't want to for things, but for example,

1537
02:52:30,180 --> 02:52:36,420
for using Lumi, our recommendation is to come to us and let us try to figure out how GPU stuff

1538
02:52:36,420 --> 02:52:44,180
works there and then you do it yourself. We have people come and asking about CSC virtual machines

1539
02:52:44,180 --> 02:52:55,620
or other small debugging things on CSC resources. Okay, yeah. Yeah, that's pretty much what I have

1540
02:52:55,620 --> 02:53:03,460
to present and I mean if there are any still more questions or something like that I'm

1541
02:53:03,460 --> 02:53:13,980
happy to answer. Let's see, I'll flip to the notes here. So there was a question

1542
02:53:13,980 --> 02:53:19,140
where does the abbreviation CSC come from and from Finnish Wikipedia I see it

1543
02:53:19,140 --> 02:53:24,980
does come from Center for Scientific Computing decades ago. Decades ago and at

1544
02:53:24,980 --> 02:53:30,780
some point I think that was sort of abandoned. There was a time when

1545
02:53:30,780 --> 02:53:35,100
Then actually the official name was Center for Scientific Computing, and then at some

1546
02:53:35,100 --> 02:53:42,100
point, I don't know, somewhere, someone made the decision that, okay, now it's just CIC,

1547
02:53:42,100 --> 02:53:46,580
IT Center for Science, is what the original comes from.

1548
02:53:46,580 --> 02:53:47,580
Yeah.

1549
02:53:47,580 --> 02:53:51,340
Which is more appropriate these days, anyway.

1550
02:53:51,340 --> 02:54:00,660
There's a question, my code is not using GPUs, so Lumi's only beneficial for GPU work.

1551
02:54:00,660 --> 02:54:04,660
Lumi, as said, it has also this CPU partition,

1552
02:54:06,420 --> 02:54:10,100
which, I mean, it's basically the same type of CPUs

1553
02:54:10,100 --> 02:54:14,300
than in Mahti, one generation newer.

1554
02:54:15,620 --> 02:54:19,060
I guess whether you should be using Lumi or Mahti

1555
02:54:19,060 --> 02:54:24,060
or Puhti in that case is really how...

1556
02:54:24,420 --> 02:54:27,700
Is it really just how big and where you're used to

1557
02:54:27,700 --> 02:54:30,500
and what has enough spare resources right now?

1558
02:54:30,660 --> 02:54:44,660
Something like that, and sometimes it might be just a matter of that depending on time of year or whatever there is more free resources in one supercomputer than in another.

1559
02:54:44,660 --> 02:54:46,660
Yeah.

1560
02:54:46,660 --> 02:54:58,500
I would say that for Finnish users having only CPU code marked is probably big.

1561
02:54:58,500 --> 02:55:06,180
I mean, for Volumio also, you can apply access via this my.csc.dev file.

1562
02:55:06,180 --> 02:55:10,180
But for some things, I would say that it's probably easier to work with.

1563
02:55:10,180 --> 02:55:18,180
Yeah. Okay. Someone put in the notes here a pointer to the CSC weekly user support session.

1564
02:55:18,180 --> 02:55:21,180
Ah, that's actually, yeah.

1565
02:55:21,180 --> 02:55:26,180
This is really good. It's like our daily garage at Aalto.

1566
02:55:26,180 --> 02:55:33,180
So you can go there and ask questions and join some experts and figure out stuff live.

1567
02:55:33,180 --> 02:55:36,220
Yeah definitely something I should add also to the slide something we

1568
02:55:37,260 --> 02:55:44,140
we already had for quite some time but yeah that's that's true something I mean there was this that

1569
02:55:44,140 --> 02:55:49,340
okay you you can send email and but I mean this is this is a member Wednesday you can just online

1570
02:55:49,340 --> 02:55:59,020
join and hopefully be like a low barrier place to ask questions. Yeah and so is CSC a non-profit

1571
02:55:59,020 --> 02:56:08,620
organization, or is it a company? Yes, yes. We are a private company, but a special company,

1572
02:56:08,620 --> 02:56:12,860
non-profit company. We're actually not allowed to make any real profit than a

1573
02:56:12,860 --> 02:56:19,500
sale loan by the Ministry of Education, so. Okay, yeah, yeah. We are here to make the world a better

1574
02:56:19,500 --> 02:56:26,380
place, not to make money. Yeah, and yes, our taxes go to supporting CSC, just like they go to

1575
02:56:26,380 --> 02:56:30,540
supporting research because CSC is part of research.

1576
02:56:30,540 --> 02:56:36,140
Yes, exactly. Anyway, yeah, thank you very much for coming. I

1577
02:56:36,140 --> 02:56:40,860
guess UC can stay around in the

1578
02:56:40,860 --> 02:56:45,580
notes and answer any further questions that appear there.

1579
02:56:45,580 --> 02:56:53,260
I'll just put it in the chat with UC. So yeah, thanks a lot and

1580
02:56:53,260 --> 02:56:58,140
see you next year or maybe before next year.

1581
02:56:58,140 --> 02:56:59,260
Yeah, we'll see.

1582
02:56:59,260 --> 02:57:00,380
Yeah, OK.

1583
02:57:00,380 --> 02:57:02,380
Thanks a lot.

1584
02:57:02,380 --> 02:57:03,540
Bye.

1585
02:57:03,540 --> 02:57:04,040
Bye.

1586
02:57:09,460 --> 02:57:11,620
OK.

1587
02:57:11,620 --> 02:57:12,140
So.

1588
02:57:12,140 --> 02:57:13,420
Yeah, that was a great talk.

1589
02:57:13,420 --> 02:57:14,180
Yeah, very good.

1590
02:57:14,180 --> 02:57:15,260
Thank you.

1591
02:57:15,260 --> 02:57:17,860
Thank you.

1592
02:57:17,860 --> 02:57:19,980
Mostly kept in time, I think.

1593
02:57:19,980 --> 02:57:21,780
That's, that's always the difficulty.

1594
02:57:28,220 --> 02:57:33,020
So next up is MPI Parallelism, as I just added here.

1595
02:57:34,020 --> 02:57:37,980
Uh, did I spell that right?

1596
02:57:38,380 --> 02:57:38,740
Okay.

1597
02:57:38,820 --> 02:57:47,660
So, um, I guess we're back to the thing, so I'll switch back to my screen here.

1598
02:57:47,660 --> 02:57:54,980
So there we go.

1599
02:57:54,980 --> 02:57:57,380
So MPI.

1600
02:57:57,380 --> 02:58:05,140
So we have, what, 50 minus 10, 40 minutes

1601
02:58:05,140 --> 02:58:09,820
to do MPI in GPU plus 10 minutes for break

1602
02:58:09,820 --> 02:58:12,540
before we get to the Q&A.

1603
02:58:12,540 --> 02:58:14,940
How should we divide this up?

1604
02:58:14,940 --> 02:58:23,420
Well, I'd say that let's go quickly through the MPI and then spend more of the time in the GPU

1605
02:58:23,420 --> 02:58:28,060
part, because I think a lot of people are interested in that. But I think Jules' talk was

1606
02:58:28,060 --> 02:58:35,180
a great introduction to what we are going to be talking about now, because these next technologies,

1607
02:58:35,180 --> 02:58:41,180
well, GPUs are used everywhere, but MPI is definitely the workhorse in many of the

1608
02:58:41,180 --> 02:58:49,740
the supercomputers, and this kind of like codes are used in this, especially what [name] was saying

1609
02:58:49,740 --> 02:58:57,820
about Mahti, which is this kind of like machine designed especially for running these very big

1610
02:58:57,820 --> 02:59:03,740
parallel programs, and those programs utilize MPI to do the parallelization.

1611
02:59:04,700 --> 02:59:09,180
Yeah, so what's the most important things to know? Let's say you have a program and you

1612
02:59:09,180 --> 02:59:15,340
know it uses MPI, because the docs say so. What options do we need for SLURP?

1613
02:59:16,540 --> 02:59:22,860
So the first thing with the MPI program, if you need to compile it yourself,

1614
02:59:23,420 --> 02:59:30,140
you need to usually use some MPI libraries that are provided by the system administrators.

1615
02:59:30,140 --> 02:59:35,820
Because MPI, when we talked about the different packetization strategies,

1616
02:59:35,820 --> 02:59:40,220
it's message passing. So a lot of stuff is transferred throughout the network.

1617
02:59:40,940 --> 02:59:47,100
So usually these libraries, these MPI libraries are compiled or created in a way that they can

1618
02:59:47,100 --> 02:59:54,380
understand the hardware very well. So they can understand the high speed in network that is

1619
02:59:54,380 --> 03:00:01,820
between the computers. So that's why all of these like high-performance clusters that have these

1620
03:00:01,820 --> 03:00:09,180
kind of like, or want to run these like big jobs, they have this usually this high-speed network

1621
03:00:09,180 --> 03:00:14,460
and they have their own MPI installation that knows about this high-speed network.

1622
03:00:15,180 --> 03:00:21,580
So when you're doing the MPI communication, the program is literally knowing about the

1623
03:00:21,580 --> 03:00:26,220
low-level hardware and sending messages as fast as possible.

1624
03:00:26,220 --> 03:00:32,300
in many cases like it tries to bypass the processor itself so in for example when there's

1625
03:00:32,300 --> 03:00:37,980
communication happening it tries to like go straight from the memory of the computer to

1626
03:00:37,980 --> 03:00:43,100
to the network card and from there through the network straight into the memory of another

1627
03:00:43,100 --> 03:00:47,980
computer basically injecting into the memory of another computer without ever going through the

1628
03:00:47,980 --> 03:00:53,660
processor to speed up these communications and that for that reason there needs to be it's called

1629
03:00:53,660 --> 03:00:57,820
called like remote data management access.

1630
03:00:58,740 --> 03:01:02,780
I think like it's the short term, RDMA like anyway,

1631
03:01:02,780 --> 03:01:05,860
but basically these kinds of things.

1632
03:01:05,860 --> 03:01:07,460
So you first need to find out,

1633
03:01:07,460 --> 03:01:09,820
if you need to compile something,

1634
03:01:09,820 --> 03:01:11,940
you need to find out the MPI libraries

1635
03:01:11,940 --> 03:01:16,540
that are provided by the system and then use those.

1636
03:01:16,540 --> 03:01:20,780
Like you need to find those in a module somewhere

1637
03:01:20,780 --> 03:01:22,460
and you need to remember to load these

1638
03:01:22,460 --> 03:01:25,140
when you're running the MPI program as well.

1639
03:01:25,140 --> 03:01:28,340
If you are using pre-existing MPI program

1640
03:01:28,340 --> 03:01:31,980
that is installed by the system administrators,

1641
03:01:31,980 --> 03:01:35,860
for example, when I used the LAMPS demo on the first day,

1642
03:01:35,860 --> 03:01:39,300
I used the LAMPS installation that we had installed.

1643
03:01:39,300 --> 03:01:42,140
And in that case, you don't have to compile anything,

1644
03:01:42,140 --> 03:01:43,260
you can just use that.

1645
03:01:43,260 --> 03:01:44,660
It's same with CSC,

1646
03:01:44,660 --> 03:01:46,620
like if you want to use something like GROMACS

1647
03:01:46,620 --> 03:01:49,260
or CP2K or something.

1648
03:01:49,260 --> 03:01:52,540
In CSC machines, they already have existing installations

1649
03:01:52,540 --> 03:01:54,740
and it's usually a good idea to use those

1650
03:01:54,740 --> 03:01:57,420
because those have been tested and designed

1651
03:01:57,420 --> 03:02:01,060
to use the high-speed interconnects.

1652
03:02:01,060 --> 03:02:05,100
But from the queue side, what do you need?

1653
03:02:05,100 --> 03:02:08,820
So from the queue side, you need to tell the queue

1654
03:02:08,820 --> 03:02:13,560
how many the so-called MPI tasks you need to specify.

1655
03:02:13,560 --> 03:02:16,780
So again, like there's going to be more terminology.

1656
03:02:16,780 --> 03:02:24,580
it's unfortunate, but because there's differences in the meaning, there's going to be new terms.

1657
03:02:24,580 --> 03:02:31,540
So in MPI world, there's going to be these, how many nodes do you want?

1658
03:02:31,540 --> 03:02:38,700
Usually you want, if all of your tasks fit into one node, so task is usually one CPU,

1659
03:02:38,700 --> 03:02:44,940
but it can have multiple CPUs in the task.

1660
03:02:44,940 --> 03:02:51,580
So previously when we asked for multiple CPUs, we had the CPUs per task, and nobody asked

1661
03:02:51,580 --> 03:02:54,980
what the task is, but basically that's the MPI task.

1662
03:02:54,980 --> 03:03:00,540
So when we are running this shared memory parallelism, we're asking for CPUs per task,

1663
03:03:00,540 --> 03:03:05,060
but we're asking for only one task because we're not using MPI.

1664
03:03:05,060 --> 03:03:12,540
But in the case of MPI, you want to ask for multiple of these MPI tasks.

1665
03:03:12,540 --> 03:03:18,460
And each of these is typically one CPU, but can be more.

1666
03:03:18,460 --> 03:03:25,960
And typically, you want to ask for either one node, or you want to divide the program

1667
03:03:25,960 --> 03:03:31,940
in a way that it's distributed among the computers in some even fashion.

1668
03:03:31,940 --> 03:03:38,200
So in the abstract, there's nodes and n tasks per node.

1669
03:03:38,200 --> 03:03:46,120
For example, you would want to have one computer with 20 CPUs and another computer with 20 CPUs,

1670
03:03:46,120 --> 03:03:52,360
because this means that then it's more balanced, the job requirement.

1671
03:03:55,560 --> 03:04:00,520
Usually, at least in Triton, you want to use srun to run the program because that

1672
03:04:00,520 --> 03:04:09,560
tells the program to use this MPI framework that is built into the system, so that the MPI asks

1673
03:04:09,560 --> 03:04:17,720
the Slurm, where should I place all of my tasks? And it does this communication layer underneath it

1674
03:04:18,680 --> 03:04:25,400
to do it. But in practice, yeah. For most users, this can all be considered magic,

1675
03:04:25,400 --> 03:04:28,840
and you know you need these options and that it works.

1676
03:04:28,840 --> 03:04:33,400
Yeah, like usually I would recommend just checking like the documentation here

1677
03:04:33,400 --> 03:04:39,080
and just using those like values and try to like allocate

1678
03:04:40,360 --> 03:04:45,480
like what how many programs you want and try to allocate it in the way that it's mentioned there.

1679
03:04:47,160 --> 03:04:51,720
So if we look at the example here, do we want to go through it or?

1680
03:04:51,720 --> 03:04:58,120
I think it's probably not needed. I mean, I'd rather go to the GPUs.

1681
03:04:58,840 --> 03:05:05,480
Yeah, so the MPI example here is basically just there's like an MPI implementation of the same

1682
03:05:06,520 --> 03:05:14,440
Pi code, but it uses multiple processors to do it. And basically what you do is you load

1683
03:05:15,160 --> 03:05:19,960
an MPI installation. In our case, in our cluster, it's OpenMPI that we mainly use.

1684
03:05:19,960 --> 03:05:27,360
use, and then you compile the code. So, this is C code. So, it's low-level code, and then

1685
03:05:27,360 --> 03:05:33,560
you run it. But yeah, if your program uses MPI, you'll know it because they mentioned

1686
03:05:33,560 --> 03:05:41,120
it. So, you'll know about it because it mentions MPI. So, if it doesn't have the letters MP

1687
03:05:41,120 --> 03:05:48,280
and I somewhere mentioned, it's not using MPI, really. So, try to search the documentation

1688
03:05:48,280 --> 03:05:54,120
that sort of stuff if you want to do this MPI programs. And come and ask us if you have problems

1689
03:05:54,120 --> 03:06:01,640
with this. Because all of the network layer and that sort of stuff, this is high-performance

1690
03:06:01,640 --> 03:06:09,960
computing that basically marries the program with the hardware in a different fashion than

1691
03:06:09,960 --> 03:06:15,240
the previous programs we were running. Here we have a much more closely knit connection,

1692
03:06:15,240 --> 03:06:21,320
So that's why there's lots of complications and that's why it's mainly used for these

1693
03:06:21,320 --> 03:06:25,160
very complex programs that want to run very big simulations.

1694
03:06:25,160 --> 03:06:25,720
Yeah, yeah.

1695
03:06:27,400 --> 03:06:27,640
Okay.

1696
03:06:27,640 --> 03:06:30,040
If you have any questions, then do ask.

1697
03:06:31,720 --> 03:06:37,080
For those people who don't want to use MPI, I wouldn't touch the nodes and end tasks.

1698
03:06:37,080 --> 03:06:41,320
I would just know that these are not meant for me.

1699
03:06:41,320 --> 03:06:43,160
I don't care for those.

1700
03:06:43,160 --> 03:06:46,220
Because I just want to use multiple processors.

1701
03:06:46,220 --> 03:06:50,220
I don't care for these tasks, things.

1702
03:06:50,220 --> 03:06:51,220
Yeah.

1703
03:06:51,220 --> 03:06:52,220
Okay.

1704
03:06:52,220 --> 03:06:59,300
There's no questions about this, which is roughly what I'd expect, to be honest.

1705
03:06:59,300 --> 03:07:04,620
I mean, it's a thing that you'll learn when you need it.

1706
03:07:04,620 --> 03:07:10,060
And if you're using it, probably there's someone around you that figured this out already.

1707
03:07:10,060 --> 03:07:16,300
If you know that you want to code with MPI, I highly recommend going to the CSC MPI courses.

1708
03:07:16,300 --> 03:07:24,300
They are very nice courses. I have taken a few of those myself just to get more information.

1709
03:07:24,940 --> 03:07:26,540
They are very good courses on that.

1710
03:07:29,180 --> 03:07:32,780
Okay. Should we go to the break now a little bit early

1711
03:07:32,780 --> 03:07:36,940
and come back and have a bit more time for GPU stuff?

1712
03:07:37,580 --> 03:07:38,860
Yeah. Sounds good.

1713
03:07:40,060 --> 03:07:50,060
until 58. Okay, great. So keep asking questions, if any, and see you in 10 minutes.

1714
03:07:50,060 --> 03:07:51,060
Yep.

1715
03:07:51,060 --> 03:07:52,060
Bye.

1716
03:08:10,060 --> 03:08:12,120
you

1717
03:08:40,060 --> 03:08:42,120
you

1718
03:09:10,060 --> 03:09:12,120
you

1719
03:09:40,060 --> 03:09:42,120
you

1720
03:10:10,060 --> 03:10:12,120
you

1721
03:10:40,060 --> 03:10:42,120
you

1722
03:11:10,060 --> 03:11:12,120
you

1723
03:11:40,060 --> 03:11:42,120
you

1724
03:12:10,060 --> 03:12:12,120
you

1725
03:12:40,060 --> 03:12:42,120
you

1726
03:13:10,060 --> 03:13:12,120
you

1727
03:13:40,060 --> 03:13:42,120
you

1728
03:14:10,060 --> 03:14:12,120
you

1729
03:14:40,060 --> 03:14:42,120
you

1730
03:15:10,060 --> 03:15:12,120
you

1731
03:15:40,060 --> 03:15:42,120
you

1732
03:16:10,060 --> 03:16:12,120
you

1733
03:16:40,060 --> 03:16:42,120
you

1734
03:17:10,060 --> 03:17:12,120
you

1735
03:17:40,060 --> 03:17:42,120
you

1736
03:18:10,060 --> 03:18:29,740
Hello, we are back for the last little bit.

1737
03:18:29,740 --> 03:18:31,000
So GPUs.

1738
03:18:31,000 --> 03:18:36,620
So I can say I'm a person that hasn't used GPUs very much.

1739
03:18:36,620 --> 03:18:43,740
So my main questions from the next sections would be, if I have code that already uses

1740
03:18:43,740 --> 03:18:50,220
GPU, what options in Slurm do I need?

1741
03:18:50,220 --> 03:18:53,620
How do I monitor their performance?

1742
03:18:53,620 --> 03:19:02,960
And what do I need in my own software or Python environments or whatever in order to use these?

1743
03:19:02,960 --> 03:19:04,660
So do you think we can go over that?

1744
03:19:04,660 --> 03:19:11,660
Yeah, I think that's a great task list for this session.

1745
03:19:11,660 --> 03:19:14,660
Do you want to throw into my screen and maybe I can...

1746
03:19:14,660 --> 03:19:19,660
Sure. Here we go. Okay, there you go.

1747
03:19:19,660 --> 03:19:30,660
Yeah, so TPUs, this is like a topic with lots of interest and lots of things to cover,

1748
03:19:30,660 --> 03:19:37,860
But hopefully, we can get through the most important things in this session.

1749
03:19:38,900 --> 03:19:44,580
So what are TPUs? We already talked about it. They're basically the chicken skewers of the

1750
03:19:48,420 --> 03:19:53,940
massive parallel processors that do one thing very good. And the one thing that they do good

1751
03:19:53,940 --> 03:19:57,540
is the matrix multiplication and that sort of stuff.

1752
03:19:57,540 --> 03:20:01,860
So they do that very well.

1753
03:20:01,860 --> 03:20:04,220
And that's why they're used in all kinds of codes

1754
03:20:04,220 --> 03:20:07,820
from physics to deep learning and that sort of stuff.

1755
03:20:07,820 --> 03:20:10,260
And everybody wants to use them because they make stuff

1756
03:20:10,260 --> 03:20:12,660
go room, basically.

1757
03:20:12,660 --> 03:20:19,980
So when we want the GPUs, how Slurm thinks about them,

1758
03:20:19,980 --> 03:20:22,460
how Slurm knows about them, it knows about them

1759
03:20:22,460 --> 03:20:25,700
in these so-called generic resources.

1760
03:20:25,700 --> 03:20:30,700
So you need to specifically request for these resources.

1761
03:20:30,820 --> 03:20:34,620
There's two ways you can do to request these resources.

1762
03:20:34,620 --> 03:20:39,620
The old way is the dash-dash-gress GPU one,

1763
03:20:44,500 --> 03:20:47,980
but the newer way is this dash-dash-GPUs equal one.

1764
03:20:47,980 --> 03:20:55,260
The newer way, if it's supported, I would personally use that because it also allows

1765
03:20:55,260 --> 03:21:01,180
you to, like, if you need to do multi-GPU stuff and that sort of stuff later on, there's

1766
03:21:01,180 --> 03:21:05,900
other flags that you can use to make it so that you can, like, allocate certain number

1767
03:21:05,900 --> 03:21:10,020
of CPUs per GPU and that sort of stuff.

1768
03:21:10,020 --> 03:21:13,540
You can make certain that your job gets the correct kind of an allocation.

1769
03:21:13,540 --> 03:21:17,620
So the newer method has more, like, leeway on that.

1770
03:21:17,620 --> 03:21:23,140
both work so you can use whichever one you want to use. Some clusters have the

1771
03:21:23,140 --> 03:21:26,420
GPUs in a separate partition so you might need to

1772
03:21:26,420 --> 03:21:30,260
ask for a separate partition.

1773
03:21:30,500 --> 03:21:37,140
So basically just like processors and memory Slurm tracks it

1774
03:21:37,140 --> 03:21:40,980
and look up your own clusters info for how to request it

1775
03:21:40,980 --> 03:21:45,220
and then you got it. Okay that's pretty easy.

1776
03:21:45,220 --> 03:21:47,060
Yeah, and when Slurm allocates the job,

1777
03:21:47,060 --> 03:21:51,340
it basically makes certain that, OK, you get access to it

1778
03:21:51,340 --> 03:21:53,740
and nobody else gets access to it.

1779
03:21:53,740 --> 03:21:57,060
So then you will get access to the GPU.

1780
03:21:57,060 --> 03:22:00,780
In our cluster, there's also this debug partition

1781
03:22:00,780 --> 03:22:04,820
that you can use to debug short jobs, which is very nice.

1782
03:22:04,820 --> 03:22:08,780
If you want to do a small debug, run, see that your code runs,

1783
03:22:08,780 --> 03:22:12,860
and then just stop it, and then put the actual run going.

1784
03:22:12,860 --> 03:22:14,340
Because the GPUs are very popular,

1785
03:22:14,340 --> 03:22:16,660
or sometimes there's a lot of people in the queue.

1786
03:22:18,500 --> 03:22:27,060
And yeah, let's jump right in and let's start dealing with the actual problems of the GPU

1787
03:22:27,060 --> 03:22:31,300
computing or the actual things where you need to, what you need to think about.

1788
03:22:32,660 --> 03:22:42,020
So the first thing you need to think about is that the GPUs, like you cannot simply

1789
03:22:42,020 --> 03:22:45,460
like talk with the GPU, like you do it,

1790
03:22:45,460 --> 03:22:49,140
like you cannot write a normal program with them.

1791
03:22:49,140 --> 03:22:50,860
You need to discuss with the GPU

1792
03:22:50,860 --> 03:22:52,860
because it's just like a specialized hardware.

1793
03:22:52,860 --> 03:22:53,700
Okay.

1794
03:22:53,700 --> 03:22:56,300
You need to use this thing called CUDA toolkit

1795
03:22:56,300 --> 03:22:58,060
to discuss with that.

1796
03:22:58,060 --> 03:23:01,020
So like, because there's so many different like hardware,

1797
03:23:01,020 --> 03:23:03,060
there's different kinds of GPUs

1798
03:23:03,060 --> 03:23:06,580
and nobody wants to write to a specific like CPU

1799
03:23:06,580 --> 03:23:07,940
or GPU architecture.

1800
03:23:07,940 --> 03:23:09,980
Like you don't want to write the machine code

1801
03:23:09,980 --> 03:23:17,660
only for like certain GPU architecture, what the people at NVIDIA have created and which is

1802
03:23:17,660 --> 03:23:25,180
probably the best product, like even more important I would say maybe than the actual hardware

1803
03:23:25,740 --> 03:23:34,860
is this kind of like development kit called CUDA, like CUDA development kit that basically

1804
03:23:34,860 --> 03:23:42,140
and handle whatever GPU you have underneath it. The CUDA development kit is this kind of

1805
03:23:45,580 --> 03:23:52,780
thing that makes it possible for your code to discuss with the GPU. If you have a program

1806
03:23:52,780 --> 03:24:01,020
that is like low-level program, you need to compile your code for the GPU.

1807
03:24:01,020 --> 03:24:08,460
Okay. So here's an example of a, like, so, so is this like, there's the driver on the

1808
03:24:08,460 --> 03:24:14,080
node itself, but depending on what GPU you're using, there may be different driver versions.

1809
03:24:14,080 --> 03:24:20,420
So the toolkit translates your program to what's needed for this particular model of

1810
03:24:20,420 --> 03:24:22,980
GPU with these particular driver versions.

1811
03:24:22,980 --> 03:24:26,740
Yeah. Well, maybe I'll show this picture already.

1812
03:24:26,740 --> 03:24:27,740
Okay.

1813
03:24:27,740 --> 03:24:33,260
So there's a picture of how the thing actually works.

1814
03:24:33,260 --> 03:24:38,840
So what there is in the machine,

1815
03:24:38,840 --> 03:24:43,380
you have the GPU connected to the motherboard of the machine.

1816
03:24:43,380 --> 03:24:45,180
In the operating system,

1817
03:24:45,180 --> 03:24:46,900
there's this CUDA driver.

1818
03:24:46,900 --> 03:24:50,480
This CUDA driver can then discuss with the GPU,

1819
03:24:50,480 --> 03:24:54,700
but you don't want to code your code for this CUDA driver

1820
03:24:54,700 --> 03:24:57,180
because the driver might be updated, right?

1821
03:24:57,180 --> 03:24:59,060
Like [name] said, it might be updated,

1822
03:24:59,060 --> 03:25:01,180
it might be different.

1823
03:25:01,180 --> 03:25:03,940
The GPU underneath is different.

1824
03:25:03,940 --> 03:25:05,820
Like if you go into a different computer,

1825
03:25:05,820 --> 03:25:07,420
it might be a different GPU.

1826
03:25:07,420 --> 03:25:09,020
So you don't want to lock yourself

1827
03:25:09,020 --> 03:25:12,620
only to the specific hardware configuration that you have,

1828
03:25:12,620 --> 03:25:15,340
a hardware and operating system configuration you want.

1829
03:25:15,340 --> 03:25:17,700
So what the programs usually do,

1830
03:25:17,700 --> 03:25:20,420
like for example, PyTorch or something,

1831
03:25:20,420 --> 03:25:24,660
They don't actually talk with the driver that much.

1832
03:25:24,660 --> 03:25:27,020
They don't deal with that.

1833
03:25:27,020 --> 03:25:31,420
They actually are written with respect

1834
03:25:31,420 --> 03:25:35,020
to the CUDA libraries, so libraries

1835
03:25:35,020 --> 03:25:37,580
like Kublas and QFT that implement

1836
03:25:37,580 --> 03:25:43,180
certain mathematical operations that run on GPUs,

1837
03:25:43,180 --> 03:25:45,260
and then the runtime library that

1838
03:25:45,260 --> 03:25:47,740
can be called to, let's say, give information,

1839
03:25:47,740 --> 03:25:50,100
is there a GPU available.

1840
03:25:50,100 --> 03:25:56,980
So this topmost part is usually specific for your program.

1841
03:25:56,980 --> 03:26:00,300
So you have a program, and that program

1842
03:26:00,300 --> 03:26:05,980
needs to be compiled against a certain version of CUDA toolkit

1843
03:26:05,980 --> 03:26:06,860
usually.

1844
03:26:06,860 --> 03:26:11,100
And the driver can handle multiple different toolkits

1845
03:26:11,100 --> 03:26:18,180
and then handle the discussion between the program and the GP

1846
03:26:18,180 --> 03:26:18,740
basically.

1847
03:26:18,740 --> 03:26:21,900
and the CUDA toolkit and the GPM.

1848
03:26:21,900 --> 03:26:26,580
So if we look at the example, if we quickly go show it.

1849
03:26:27,660 --> 03:26:30,900
So in this example, we are dealing with quite low level

1850
03:26:30,900 --> 03:26:32,580
like CUDA code.

1851
03:26:32,580 --> 03:26:35,700
So for this, we need to compile it actually.

1852
03:26:35,700 --> 03:26:40,700
So for the compilation, I need to load certain modules.

1853
03:26:41,940 --> 03:26:44,700
So a compiler and a CUDA toolkit.

1854
03:26:44,700 --> 03:26:45,820
So this CUDA toolkit,

1855
03:26:45,820 --> 03:26:55,820
I'm now in the login node of the cluster, so this doesn't have a GPU available, but

1856
03:26:55,820 --> 03:27:03,460
I have a CUDA toolkit, so I can compile my code with respect to this CUDA toolkit, so

1857
03:27:03,460 --> 03:27:05,500
it's linking to that.

1858
03:27:05,500 --> 03:27:15,100
And because there's no GPU available, I need to tell my compilation that I need it to work

1859
03:27:15,100 --> 03:27:19,560
with different kinds of GPU architectures, so I need to give this kind of a monstrosity

1860
03:27:19,560 --> 03:27:26,540
of a command that, okay, build my code so that it runs on these different kinds of GPU

1861
03:27:26,540 --> 03:27:27,540
architectures.

1862
03:27:27,540 --> 03:27:32,180
So, this is the kind of stuff that usually happens, like, that somebody else has done

1863
03:27:32,180 --> 03:27:33,180
for you.

1864
03:27:33,180 --> 03:27:35,860
Usually, you don't have to, like, write these kinds of commands.

1865
03:27:35,860 --> 03:27:38,740
Yeah, that's a pretty crazy long line.

1866
03:27:38,740 --> 03:27:39,740
Yeah.

1867
03:27:39,740 --> 03:27:44,440
like you can, there's a link to article about this.

1868
03:27:44,440 --> 03:27:46,800
Well, what are these architectures?

1869
03:27:46,800 --> 03:27:49,320
That is great article that I always use as a reference,

1870
03:27:49,320 --> 03:27:54,220
like to check which GPUs these architectures,

1871
03:27:54,220 --> 03:27:58,800
like, well, what do they mean?

1872
03:27:58,800 --> 03:28:02,240
So now that we have the program,

1873
03:28:02,240 --> 03:28:05,040
like we have this compiled pi.gpu,

1874
03:28:06,000 --> 03:28:07,720
we can run it with the GPU.

1875
03:28:07,720 --> 03:28:11,280
So basically what I'm asking is one GPU

1876
03:28:11,280 --> 03:28:13,880
and then we can run it.

1877
03:28:13,880 --> 03:28:15,120
Okay, yeah.

1878
03:28:15,120 --> 03:28:17,560
And it's in the queue and it's done

1879
03:28:17,560 --> 03:28:19,600
and it's done basically instantaneously.

1880
03:28:19,600 --> 03:28:22,080
And because GPUs are so fast,

1881
03:28:22,080 --> 03:28:23,800
so this number is pretty similar

1882
03:28:23,800 --> 03:28:25,400
that we run with the CPUs.

1883
03:28:25,400 --> 03:28:28,280
So let's add like few zeros here.

1884
03:28:28,280 --> 03:28:29,120
Okay, yeah.

1885
03:28:29,120 --> 03:28:31,960
And let's run it with the GPU and it's basically done.

1886
03:28:31,960 --> 03:28:34,000
That's still instant, basically.

1887
03:28:34,000 --> 03:28:35,600
Yeah, it's basically like instant.

1888
03:28:35,600 --> 03:28:39,360
So this is like a lot faster than the Python code

1889
03:28:39,360 --> 03:28:41,320
that we previously run.

1890
03:28:41,320 --> 03:28:44,800
Like in order to get like actual some run time,

1891
03:28:44,800 --> 03:28:46,580
well, we need to add more zeros.

1892
03:28:46,580 --> 03:28:47,420
Yeah.

1893
03:28:48,800 --> 03:28:51,240
But this is a lot of work, right?

1894
03:28:51,240 --> 03:28:55,280
Like the compilation and that sort of like writing the code

1895
03:28:55,280 --> 03:28:59,900
as this kind of like C code and then writing the code,

1896
03:28:59,900 --> 03:29:01,800
like dealing with the CUDA.

1897
03:29:01,800 --> 03:29:05,160
And most of the programs that use GPUs don't do this.

1898
03:29:05,160 --> 03:29:08,920
they are built against a certain version of Huda Toolkit.

1899
03:29:08,920 --> 03:29:11,240
And then we just like...

1900
03:29:11,240 --> 03:29:14,280
Like they come from whoever else provides them.

1901
03:29:14,280 --> 03:29:16,040
They're built against a certain version.

1902
03:29:16,040 --> 03:29:16,880
Okay. Yeah.

1903
03:29:16,880 --> 03:29:18,320
Yes.

1904
03:29:18,320 --> 03:29:23,320
So let's say we have this now, this program.

1905
03:29:23,320 --> 03:29:26,080
The second question you asked at the start of this session

1906
03:29:26,080 --> 03:29:28,240
was how to do monitoring.

1907
03:29:30,360 --> 03:29:33,200
And in our cluster, there's this monitoring script

1908
03:29:33,200 --> 03:29:35,500
but because we did release an installation,

1909
03:29:35,500 --> 03:29:39,180
it currently doesn't work, but we will fix it.

1910
03:29:39,180 --> 03:29:41,200
But there's another way that you can do a monitoring,

1911
03:29:41,200 --> 03:29:43,940
which is like going to the compute node

1912
03:29:43,940 --> 03:29:48,260
while the job is running.

1913
03:29:48,260 --> 03:29:51,140
And this works with other jobs as well.

1914
03:29:51,140 --> 03:29:52,060
In different clusters,

1915
03:29:52,060 --> 03:29:56,540
you might have a different way of working on it.

1916
03:29:56,540 --> 03:29:59,300
I'll add a few zeros here so that it actually runs

1917
03:30:01,020 --> 03:30:01,920
a bit longer.

1918
03:30:03,200 --> 03:30:10,640
So, you notice that my job is now running, like I have an interactive job running.

1919
03:30:10,640 --> 03:30:17,960
I'll open a new terminal and I can use the Slurm queue to check which node it is running

1920
03:30:17,960 --> 03:30:20,080
in.

1921
03:30:20,080 --> 03:30:23,160
And while the job is running, I'm allowed to go there.

1922
03:30:23,160 --> 03:30:28,280
And what happens is that I'm basically going to the same reservation, so I'm using the

1923
03:30:28,280 --> 03:30:29,280
same resources.

1924
03:30:29,280 --> 03:30:35,560
So, if I start new programs, they are basically taking resources away from the program that

1925
03:30:35,560 --> 03:30:39,040
I'm currently running in the queue.

1926
03:30:39,040 --> 03:30:47,120
And in here, I can use this tool called NVIDIA SMI, which is provided by the driver, NVIDIA

1927
03:30:47,120 --> 03:30:48,120
driver.

1928
03:30:48,120 --> 03:30:54,000
And it will tell me the usage or the utilization.

1929
03:30:54,000 --> 03:30:57,040
So if I make this a bit bigger.

1930
03:30:57,040 --> 03:31:05,840
can see this sort of an output. What we see here is that the GPU name, we have a certain GPU name,

1931
03:31:07,520 --> 03:31:14,320
we have a GPU number, we have the power usage and that sort of stuff, we have a memory usage,

1932
03:31:15,120 --> 03:31:22,240
and then we have this GPU utilization. This basically means that is the GPU actually doing

1933
03:31:22,240 --> 03:31:30,080
something. And below here, we see the processes that are actually using the GPU. So this can be

1934
03:31:30,080 --> 03:31:35,680
used as this kind of like a measurement thing that you can go there and check is it actually

1935
03:31:35,680 --> 03:31:44,800
doing something. Yeah. So I see it says 100% usage here. Is that typical? Like do most people's GPU

1936
03:31:44,800 --> 03:31:54,560
drops hit 100%? No, no. For physics code and that sort of code, it can get to the 100% or close to

1937
03:31:54,560 --> 03:32:01,920
the 100%. But in many cases, the situation is that you don't necessarily reach the 100%.

1938
03:32:02,880 --> 03:32:10,240
And the reason for this is that quite often the GPUs need something to work on. So if you're

1939
03:32:10,240 --> 03:32:16,080
you're dealing with, let's say, a deep learning code,

1940
03:32:16,080 --> 03:32:22,160
if we look at this picture above here, this one.

1941
03:32:22,160 --> 03:32:25,240
The program memory RAM.

1942
03:32:25,240 --> 03:32:26,080
OK, yeah.

1943
03:32:26,080 --> 03:32:26,800
Yeah.

1944
03:32:26,800 --> 03:32:29,480
So in many cases, you have a situation

1945
03:32:29,480 --> 03:32:33,240
where your program, let's say a deep learning code,

1946
03:32:33,240 --> 03:32:37,160
it will need to parse a data set or process a data set that

1947
03:32:37,160 --> 03:32:47,240
then used for the deep learning process. And for this to happen, the CPU part of the program

1948
03:32:47,880 --> 03:32:56,040
usually needs to load data in and process that data and then send it to the GPU for the calculation.

1949
03:32:58,040 --> 03:33:06,760
And if the GPU doesn't have enough data, it will just idle and wait for the CPU to

1950
03:33:06,760 --> 03:33:14,600
process the data. So what can happen is that the GPU utilization gets low because the CPU part

1951
03:33:14,600 --> 03:33:22,120
hasn't done its job fast enough. So it's like, I think something you've said before, the GPU is so

1952
03:33:22,120 --> 03:33:30,360
fast it can, what's the thing, resource starvation. So the CPUs can't pump data into it fast enough to

1953
03:33:30,360 --> 03:33:31,360
Yeah.

1954
03:33:31,360 --> 03:33:32,360
Okay.

1955
03:33:32,360 --> 03:33:38,760
And that means that in many cases, like when using GPU code, the actual thing you're coding

1956
03:33:38,760 --> 03:33:42,040
is the left part of this program.

1957
03:33:42,040 --> 03:33:47,640
You're trying to get, let's say, a data loading system to work so that you get enough data

1958
03:33:47,640 --> 03:33:49,720
for the GPU.

1959
03:33:49,720 --> 03:33:56,040
And this also means that when you're going from, let's say, your workstation to working

1960
03:33:56,040 --> 03:34:01,880
on a cluster, you might encounter a situation where your code is slower again on the GPU.

1961
03:34:02,680 --> 03:34:07,320
And you might wonder what's happening. I thought these GPUs are really powerful,

1962
03:34:07,320 --> 03:34:13,720
why is it slower? And the reason isn't that the GPU is bad or slower. The reason is that the data

1963
03:34:13,720 --> 03:34:21,240
isn't close to the GPU. So often you need to copy, let's say, the data sets to the local

1964
03:34:21,240 --> 03:34:25,240
local drive in these machines.

1965
03:34:25,240 --> 03:34:29,240
So all of our GPU machines have a local SSD drive there,

1966
03:34:29,240 --> 03:34:33,240
so that you can use that as this kind of like a buffer

1967
03:34:33,240 --> 03:34:37,240
for your data, so that you can fill out the GPU fast enough.

1968
03:34:37,240 --> 03:34:41,240
Because otherwise the GPU will be starving and it won't

1969
03:34:41,240 --> 03:34:45,240
get enough data. And often you want to

1970
03:34:45,240 --> 03:34:49,240
also utilize this multi-CPU, like the shared memory

1971
03:34:49,240 --> 03:34:56,840
a parallelism thing where you have multiple data loaders feeding one GPU enough stuff.

1972
03:34:58,600 --> 03:35:05,240
And this is like a whole can of worms. But basically, nowadays, often GPU programming

1973
03:35:05,240 --> 03:35:12,520
isn't actually programming on the GPU. You have some existing library that uses GPU or framework,

1974
03:35:12,520 --> 03:35:20,360
like by torch and then you have you try to like make certain that the gpu has enough stuff to do

1975
03:35:20,360 --> 03:35:27,560
yeah so in the pasta metaphor but if we're in this restaurant we have the chicken cooker

1976
03:35:28,520 --> 03:35:35,800
and the chicken cooker can cook 20 chickens at once but we don't have enough chickens in our

1977
03:35:35,800 --> 03:35:41,400
restaurant so every time like oh we don't have enough and you send someone to the store to go

1978
03:35:41,400 --> 03:35:46,040
buy some and come back and put them on. But by the time they get back, they need even more. So,

1979
03:35:46,040 --> 03:35:50,760
okay, go to the store again and come back. And data transfer...

1980
03:35:50,760 --> 03:35:51,720
That's an excellent...

1981
03:35:51,720 --> 03:35:53,000
Yeah, that's excellent.

1982
03:35:53,000 --> 03:35:58,280
And compared to the speed of a processor, data transfer is slow, really slow.

1983
03:36:00,440 --> 03:36:04,040
I linked some slides up above that expanded on the kitchen metaphor,

1984
03:36:04,040 --> 03:36:10,840
and it was really surprising to me how slow the processing or the data transfer really is.

1985
03:36:10,840 --> 03:36:17,720
There's this adage in high-performance computing that computing is fast, but pipes are slow.

1986
03:36:18,760 --> 03:36:26,440
So basically, everything that needs to transfer data, that's the part where the stuff usually

1987
03:36:26,440 --> 03:36:35,800
gets slow. In the case of a GPU, the problem usually is that the slow part isn't the GPU

1988
03:36:35,800 --> 03:36:43,040
The slow part isn't the GPU memory or not even the transfer from the CPU memory to the

1989
03:36:43,040 --> 03:36:44,040
GPU part.

1990
03:36:44,040 --> 03:36:48,520
The slow part is outside of this picture, some hard drive or something that you need

1991
03:36:48,520 --> 03:36:52,080
to transfer stuff to the CPU memory.

1992
03:36:52,080 --> 03:36:55,760
And that's usually the bottleneck in this case situation.

1993
03:36:55,760 --> 03:37:01,080
There's also a great question in the chat of, do you need to separately reserve a memory

1994
03:37:01,080 --> 03:37:04,600
for the GPU?

1995
03:37:04,600 --> 03:37:15,240
The answer is no. You will get the whole GPU memory. Let's say I run here

1996
03:37:15,960 --> 03:37:23,960
GPUs 1, NVIDIA, SMI. I will just run the monitoring program.

1997
03:37:23,960 --> 03:37:28,900
.

1998
03:37:28,900 --> 03:37:34,940
So you notice that this run on a V100 with 16 gigabytes of memory.

1999
03:37:34,940 --> 03:37:41,580
So all of this VRAM in the GPU, it's for mine to use.

2000
03:37:41,580 --> 03:37:52,220
And so all of this memory here is mine for use, even though I didn't request CPU memory

2001
03:37:52,220 --> 03:37:53,220
that much.

2002
03:37:53,220 --> 03:37:57,900
So unlike the normal processor in one of our computers,

2003
03:37:57,900 --> 03:37:59,500
which can be shared by many things,

2004
03:37:59,500 --> 03:38:04,420
the GPU is always one GPU, one person, no sharing.

2005
03:38:04,420 --> 03:38:05,020
Yeah.

2006
03:38:05,020 --> 03:38:05,660
OK.

2007
03:38:05,660 --> 03:38:09,600
And also I will mention that quite often you still

2008
03:38:09,600 --> 03:38:11,220
need to reserve quite a bit of memory

2009
03:38:11,220 --> 03:38:15,060
because, for example, in order to load a large language

2010
03:38:15,060 --> 03:38:18,860
model or something, you need to first load it from the disk,

2011
03:38:18,860 --> 03:38:23,140
load it into the memory, and then give it to the GPU memory.

2012
03:38:23,140 --> 03:38:29,700
So you need to reserve enough memory so that the stuff can fit into the CPU's RAM before

2013
03:38:29,700 --> 03:38:32,580
it's transferred into the GPU's RAM.

2014
03:38:32,580 --> 03:38:39,620
And these memory considerations are quite important when it comes to these big models

2015
03:38:39,620 --> 03:38:47,460
or if you want to do large data processing, you need more and more memory.

2016
03:38:47,460 --> 03:38:51,940
And many of these large language models, for example, they might require so much memory

2017
03:38:51,940 --> 03:38:57,540
that they don't fit into one GPU. So, you have to use multiple GPUs to run those.

2018
03:38:58,100 --> 03:39:05,860
Yeah. Okay. I actually feel like I understand more than when I started, especially about the,

2019
03:39:07,220 --> 03:39:12,100
or mainly about the toolkit and the library things.

2020
03:39:13,460 --> 03:39:19,460
There's also one excellent question that, can you get an interactive session on a GPU? And the

2021
03:39:19,460 --> 03:39:24,180
Answer is in principle, yes, but I would highly recommend not doing that.

2022
03:39:24,180 --> 03:39:31,860
The first thing is that for the reason that the GPUs are very important for many calculations,

2023
03:39:31,860 --> 03:39:38,340
and they're very expensive, there's internal billing in Slurm that basically your priority

2024
03:39:38,340 --> 03:39:44,740
in the queue is determined by the resources you use. And because the GPUs are so important,

2025
03:39:44,740 --> 03:39:51,740
and the billing for the GPUs is higher.

2026
03:39:51,740 --> 03:40:03,700
So basically, if you use one hour of GPU interactively, you're basically using like 80 hours of CPU

2027
03:40:03,700 --> 03:40:05,380
time basically.

2028
03:40:05,380 --> 03:40:08,260
So there's this kind of like a billing factor there.

2029
03:40:08,260 --> 03:40:12,140
So it will lower your priority a lot.

2030
03:40:12,140 --> 03:40:18,320
And secondly, again, like if you're using it interactively, nobody else can use it.

2031
03:40:18,320 --> 03:40:24,520
So what I personally usually like to do is I write my stuff, then I run it with S run

2032
03:40:24,520 --> 03:40:28,060
or something like that and see how it performs.

2033
03:40:28,060 --> 03:40:32,420
And let's say I want to do like a deep learning training or something.

2034
03:40:32,420 --> 03:40:37,220
It's enough for me to know that the training starts to know that my code is correct.

2035
03:40:37,220 --> 03:40:39,540
And then I can like submit to a longer job.

2036
03:40:39,540 --> 03:40:46,820
Usually, if I want to just test it out, I can test out half an epoch or one epoch or

2037
03:40:46,820 --> 03:40:53,060
something like that and test out interactively this very short time and then actually submit

2038
03:40:53,940 --> 03:41:00,020
it for a longer time and not reserve it constantly because then my priority will suffer

2039
03:41:00,020 --> 03:41:06,740
and other people cannot use the GPUs. Okay.

2040
03:41:09,540 --> 03:41:10,540
Yeah.

2041
03:41:10,540 --> 03:41:14,220
So, was that all the most important things that I had said?

2042
03:41:14,220 --> 03:41:22,100
So, let's say there's some existing program, like a Python program, that uses GPUs, I want

2043
03:41:22,100 --> 03:41:27,060
to use PyTorch, so what do I need to do to make that work?

2044
03:41:27,060 --> 03:41:36,540
Yes, so this is the bane of many people's existence, that what actually happens, and

2045
03:41:36,540 --> 03:41:41,740
person in the chat was already discussing like they have a specific case where they want to use

2046
03:41:42,380 --> 03:41:48,780
TensorFlow and PyTorch. Both of these are these kind of like deep learning frameworks and some

2047
03:41:48,780 --> 03:41:54,380
extra packages on top of it and it's a lot of work to get that installed and getting that working

2048
03:41:54,940 --> 03:42:02,220
and that is a complicated thing to do and we have some pre-installed environments but they

2049
03:42:02,220 --> 03:42:05,220
they don't, of course, contain your custom packages.

2050
03:42:05,220 --> 03:42:07,380
And because it's so complicated,

2051
03:42:07,380 --> 03:42:08,820
usually we recommend that people

2052
03:42:08,820 --> 03:42:10,840
install their own environments.

2053
03:42:11,980 --> 03:42:14,260
In some sites, the recommended way

2054
03:42:14,260 --> 03:42:17,580
is to install these environments in containers.

2055
03:42:17,580 --> 03:42:20,100
For example, in CSC, the recommended way,

2056
03:42:20,100 --> 03:42:21,860
because all of these environments

2057
03:42:21,860 --> 03:42:23,740
can create millions of files,

2058
03:42:23,740 --> 03:42:26,020
and it can cause some problems.

2059
03:42:26,020 --> 03:42:29,420
But the recommended way, usually, is to use these, like...

2060
03:42:29,420 --> 03:42:36,020
So we, for example, recommend using Conda a lot, which is this kind of like framework

2061
03:42:36,020 --> 03:42:43,460
that can provide already installed, already compiled binaries for you easily.

2062
03:42:43,460 --> 03:42:45,900
And yeah, there's a question here.

2063
03:42:45,900 --> 03:42:51,880
So every time you want to use GPUs, you first need to load module load GCC CUDA.

2064
03:42:51,880 --> 03:42:55,740
But with Python are things you don't need to.

2065
03:42:55,740 --> 03:42:56,740
Yes.

2066
03:42:56,740 --> 03:43:09,020
Yes, so if you want to use GPUs and you want to compile yourself, you need to load the

2067
03:43:09,020 --> 03:43:10,580
development kit.

2068
03:43:10,580 --> 03:43:14,620
So basically these parts, this is provided by the module load CUDA.

2069
03:43:14,620 --> 03:43:21,840
It provides this part and then you can create your program with respect to these.

2070
03:43:21,840 --> 03:43:29,040
If you have an already existing program, let's say PyTorch or framework like PyTorch or TensorFlow,

2071
03:43:29,040 --> 03:43:35,760
what you want to do is that you want to install the program and you want that program to decide,

2072
03:43:35,760 --> 03:43:43,200
okay, I want the CUDA toolkit that I've been compiled against. Instead of going from bottom up,

2073
03:43:43,200 --> 03:43:51,760
you go from top to bottom. You first decide that, okay, I want PyTorch and what CUDA toolkit does

2074
03:43:51,760 --> 03:43:59,600
my PyTorch need. And then you let a packaging manager like Gonda decide which CUDA toolkit

2075
03:43:59,600 --> 03:44:06,640
fits with the PyTorch. So, you don't start with some, like, you don't basically force

2076
03:44:07,440 --> 03:44:17,040
the program to use a certain CUDA toolkit. You let the program decide what CUDA toolkit it needs.

2077
03:44:17,040 --> 03:44:22,720
So this is like the dependencies should be declared properly and then it works.

2078
03:44:22,720 --> 03:44:23,720
Okay.

2079
03:44:23,720 --> 03:44:24,720
Yeah.

2080
03:44:24,720 --> 03:44:34,040
So there's some instructions in our pages, in our like Conda page.

2081
03:44:34,040 --> 03:44:41,200
We have a separate page for how to use Conda in our cluster and how you can get the corresponding

2082
03:44:41,200 --> 03:44:42,200
CUDA toolkit.

2083
03:44:42,200 --> 03:44:47,520
If you are using these environments, you should not load the CUDA, because then you have doubled

2084
03:44:47,520 --> 03:44:52,920
the toolkits and now the programs don't know what they should be using and everything gets

2085
03:44:52,920 --> 03:44:55,080
like super complicated.

2086
03:44:55,080 --> 03:45:03,520
You should always have only one CUDA toolkit, either from the module system or provided

2087
03:45:03,520 --> 03:45:10,000
by the Conda installation.

2088
03:45:10,000 --> 03:45:16,120
There's also a question about MATLAB, and MATLAB is different because it does just-in-time

2089
03:45:16,120 --> 03:45:23,400
compilation, so it will use the modules, module CUDA.

2090
03:45:23,400 --> 03:45:30,680
What MATLAB does is that when you create your MATLAB GPU arrays, for example, it will just-in-time

2091
03:45:30,680 --> 03:45:41,800
compile these. When it needs them, it will compile a code that uses them when it's running,

2092
03:45:41,800 --> 03:45:54,240
so it will need the module system once. But this is a bit of a mess. There's so many different

2093
03:45:54,240 --> 03:46:04,300
ways of getting these toolkits and basically it means that whenever you want to install

2094
03:46:04,300 --> 03:46:09,620
PyTorch, you need to get like a few gigabytes of toolkits usually.

2095
03:46:09,620 --> 03:46:14,360
And this is currently the situation, but the reason behind this is that it's so much faster

2096
03:46:14,360 --> 03:46:21,360
to work with, let's say, a framework like PyTorch or TensorFlow or something and write

2097
03:46:21,360 --> 03:46:28,800
your code in this higher level language, like Python, instead of going to the C route and

2098
03:46:28,800 --> 03:46:32,400
writing everything against the CUDA toolkit.

2099
03:46:32,400 --> 03:46:43,280
Okay. Are we done with GPU? Should we go to the Ask Us Anything and then we can also answer

2100
03:46:43,280 --> 03:46:50,000
more GPU questions? Or is there anything else from this page?

2101
03:46:50,000 --> 03:46:55,000
Yeah, I think we are good, I would say.

2102
03:46:55,520 --> 03:46:59,760
A huge number of the questions in our daily grad session

2103
03:46:59,760 --> 03:47:02,680
are about getting GPU stuff to work.

2104
03:47:02,680 --> 03:47:05,200
So if you can't get it, it's okay.

2105
03:47:05,200 --> 03:47:07,160
Just come and ask, it's normal.

2106
03:47:07,160 --> 03:47:09,600
And if you're interested about this,

2107
03:47:09,600 --> 03:47:13,600
we have a course on Python for Scientific Computing

2108
03:47:13,600 --> 03:47:17,440
where I've given at least talk about how to use Conda.

2109
03:47:17,440 --> 03:47:20,480
So the talks are in Code Refinery, I think,

2110
03:47:20,480 --> 03:47:22,040
Code Refinery YouTube.

2111
03:47:22,040 --> 03:47:25,600
And the materials are in our webpage.

2112
03:47:25,600 --> 03:47:26,720
You can get a link there

2113
03:47:26,720 --> 03:47:29,280
or in the Code Refinery page as well.

2114
03:47:29,280 --> 03:47:33,480
And they're like, we try to go through the process

2115
03:47:33,480 --> 03:47:35,920
of how do you like get the corresponding

2116
03:47:35,920 --> 03:47:40,320
like toolkits and stuff and that sort of stuff.

2117
03:47:40,320 --> 03:47:41,160
Yeah.

2118
03:47:42,760 --> 03:47:45,720
Okay. I'm going to switch to the notes then.

2119
03:47:47,440 --> 03:47:54,880
And hit auto-scroll.

2120
03:47:54,880 --> 03:47:59,120
Whoever has the dependency thing at the bottom of the screen, can you move it?

2121
03:47:59,120 --> 03:48:03,680
Because then auto-scroll doesn't work.

2122
03:48:03,680 --> 03:48:10,680
Yeah, there's a question there.

2123
03:48:10,680 --> 03:48:12,880
Can a metaphor be made for the pasta example?

2124
03:48:12,880 --> 03:48:19,720
for example, Conda, TensorFlow, PyTorch, et cetera.

2125
03:48:19,720 --> 03:48:24,680
Oh, and we can somewhat add a new section for Q&A

2126
03:48:24,680 --> 03:48:26,060
and maybe the feedback stuff.

2127
03:48:26,060 --> 03:48:27,360
My hands are a bit full.

2128
03:48:27,360 --> 03:48:31,800
And as you know, cat is the most important thing.

2129
03:48:37,720 --> 03:48:41,360
Post example, TensorFlow, PyTorch, CUDA.

2130
03:48:41,360 --> 03:48:44,760
That's like, I would say that, yeah.

2131
03:48:45,760 --> 03:48:51,080
I mean, TensorFlow is a library that helps you to use the, um,

2132
03:48:51,640 --> 03:48:57,440
chicken cooker because it's a lot harder than a chicken cooker to use.

2133
03:48:59,280 --> 03:49:03,640
And almost no one goes and tries to use the chicken cooker directly.

2134
03:49:04,240 --> 03:49:04,520
Yeah.

2135
03:49:04,760 --> 03:49:07,840
I would say that, like, maybe, maybe there, it would be something like,

2136
03:49:07,840 --> 03:49:13,120
Like, if you think about, you want to cook the chickens in the GPU, right?

2137
03:49:13,120 --> 03:49:18,560
And you don't want, you want somebody to probably, like, prepare them beforehand, like butcher

2138
03:49:18,560 --> 03:49:20,160
them and that sort of stuff.

2139
03:49:20,160 --> 03:49:28,440
So basically, like, the CUDA toolkit is the person in the store that makes certain that

2140
03:49:28,440 --> 03:49:36,120
the chickens are, like, prepared so that you can just buy the, like, the chicken in a way

2141
03:49:36,120 --> 03:49:44,680
that you can just like safely cook it and it's not like, it doesn't have a skin and everything on it

2142
03:49:44,680 --> 03:49:51,800
and that sort of stuff. So basically like it's an intermediate layer that makes it so that

2143
03:49:51,800 --> 03:49:54,920
like you can do the hard stuff on that underneath it. Yeah.

2144
03:49:54,920 --> 03:50:07,560
But yeah, go ahead and please tell us, like, all the questions about anything from today.

2145
03:50:07,560 --> 03:50:08,560
Yeah.

2146
03:50:08,560 --> 03:50:14,960
And also, yeah, like, we might have guests joining us, like, other RCs and other people

2147
03:50:14,960 --> 03:50:15,960
from us.

2148
03:50:15,960 --> 03:50:16,960
Yeah.

2149
03:50:16,960 --> 03:50:19,960
Any of the staff that are around.

2150
03:50:19,960 --> 03:50:31,080
You should know where to get the studio info out, you don't need to do that.

2151
03:50:40,680 --> 03:50:43,320
Yeah, but for the

2152
03:50:43,320 --> 03:50:53,280
Basically, like with the GPUs, it has moved, like the thing has moved rapidly throughout

2153
03:50:53,280 --> 03:50:54,280
the years.

2154
03:50:54,280 --> 03:50:55,280
Hi [name].

2155
03:50:55,280 --> 03:50:56,280
Hey.

2156
03:50:56,280 --> 03:51:01,840
So, it used to be that GPUs were like, the only way of coding GPUs used to be that you

2157
03:51:01,840 --> 03:51:02,840
write the CUDA code.

2158
03:51:02,840 --> 03:51:09,840
So, basically you write C or C++ code and then you compile that and use that CUDA toolkit

2159
03:51:09,840 --> 03:51:20,560
to like to use that and then of course people didn't find this like very nice in the long term

2160
03:51:20,560 --> 03:51:25,200
like they wanted to do something else on that so they created like many frameworks that that make

2161
03:51:25,200 --> 03:51:29,760
it possible to do this kind of like stuff so that you don't have to write the C code the low-level

2162
03:51:29,760 --> 03:51:38,880
code so then once you get that into that part you still need to compile against this like

2163
03:51:38,880 --> 03:51:45,760
low-level code. So what they do is they compile against this low-level code, but then you run

2164
03:51:45,760 --> 03:51:50,480
into problems. Okay, you need the dependencies for this low-level code. They need to be solved.

2165
03:51:50,480 --> 03:51:55,280
So, okay, how do you manage that? And then it's like, okay, you need to have these packaging

2166
03:51:55,280 --> 03:52:02,240
managers to manage the whole environment. And what I usually think about is that,

2167
03:52:02,240 --> 03:52:07,920
okay, what is the version of, let's say, PyTorch I want to install? And then let the guards of the

2168
03:52:07,920 --> 03:52:20,240
packaging managers decide what versions of CUDA toolkit I get. The person who decided this thing

2169
03:52:20,240 --> 03:52:28,960
was some guy in GitHub who made the pull request or made the build for a specific version of PyTorch.

2170
03:52:30,640 --> 03:52:36,080
Some guy in an open source community, most of these are nowadays installed from conda forge,

2171
03:52:36,080 --> 03:52:41,920
which is this open source community. Some guy who's interested about this, or maybe is hired to

2172
03:52:41,920 --> 03:52:51,120
do this, decided that, okay, I will build my PyTorch with these versions, and then they decided

2173
03:52:51,120 --> 03:52:57,680
for me which version of the CUDA toolkit needed to be used, and I will trust them to get the correct

2174
03:52:57,680 --> 03:53:04,240
version. I try to give an answer to that rule of thumb whether to use

2175
03:53:04,240 --> 03:53:12,640
GPUs or just the other CPUs. Personally, if whatever, well, if there are tools around that

2176
03:53:12,640 --> 03:53:22,560
support GPUs, go for them. Why not? You can see how long it takes and at some point

2177
03:53:22,560 --> 03:53:31,120
essentially make the decision is it worth waiting for the cpu for the gpu resources or is it worth

2178
03:53:31,120 --> 03:53:40,240
waiting for a long time to get the to get your results commonly the gpus are quite a substantial

2179
03:53:40,240 --> 03:53:50,560
bit faster but i personally would not start writing gpu specific code if if i'm not really

2180
03:53:50,560 --> 03:53:58,720
pressed for time. So, if I need to really start writing GPU-specific code,

2181
03:54:01,200 --> 03:54:09,600
yeah, I would need to have a good incentive as in my calculations take weeks and then,

2182
03:54:10,160 --> 03:54:16,400
okay, well, maybe it's better to think about whether GPUs could speed this up.

2183
03:54:16,400 --> 03:54:24,480
Yeah, and often the first question I would ask myself is that, okay, the scientific problem

2184
03:54:24,480 --> 03:54:32,680
that I have in hand, what toolkits and what things are there that solve this scientific

2185
03:54:32,680 --> 03:54:33,680
problem?

2186
03:54:33,680 --> 03:54:42,720
So, for example, I already mentioned, let's say, Python does NumPy, the numerical library.

2187
03:54:42,720 --> 03:54:51,360
uses these libraries called BLAST libraries and FFT libraries that are already existing solutions

2188
03:54:51,360 --> 03:54:57,680
for solving linear algebra problems, and they are underneath the whole thing very heavily.

2189
03:54:58,880 --> 03:55:04,240
If somebody has already created a library, reusing that library is a very good idea.

2190
03:55:04,960 --> 03:55:10,800
Basically, if I would want to do, let's say, machine learning, traditional machine learning,

2191
03:55:10,800 --> 03:55:15,920
I wouldn't create my own machine learning framework. I would use something like Scikit Learn

2192
03:55:15,920 --> 03:55:22,480
that is already existing and I would just extend on that or maybe PyTorch or something like that

2193
03:55:22,480 --> 03:55:29,280
if I want to do. If that's my goal. If I need to start from scratch then the question

2194
03:55:30,560 --> 03:55:37,440
like it's like creating a completely new code that doesn't have an existing like mathematical

2195
03:55:37,440 --> 03:55:44,240
solver for that. I still would try to look for existing mathematical solvers because

2196
03:55:44,240 --> 03:55:49,480
I don't trust that my matrix multiplication code is better than what the last people.

2197
03:55:49,480 --> 03:55:55,080
They have done it for 20 years or 30 years or even more. They have optimized it for different

2198
03:55:55,080 --> 03:56:01,440
kinds of systems and edge cases and for mathematical rigor. I don't trust myself to do that sort

2199
03:56:01,440 --> 03:56:11,880
So, I would trust to use as much as possible from people that have already done existing

2200
03:56:11,880 --> 03:56:15,640
libraries.

2201
03:56:15,640 --> 03:56:21,800
And if those support GPUs, then it's more for me, but I completely agree.

2202
03:56:21,800 --> 03:56:36,600
I think there are a few occasions where the worst product in basic libraries has gotten

2203
03:56:36,600 --> 03:56:39,920
more popular than the better one.

2204
03:56:39,920 --> 03:56:45,940
The only reasons, well, there are two reasons why that happens from time to time.

2205
03:56:45,940 --> 03:56:57,380
is that the worse speed wise one is way easier to use. So usability is a big factor also in coding.

2206
03:56:57,380 --> 03:57:06,580
If I have an obscure, extremely fast solver for something, then people might not use it because

2207
03:57:06,580 --> 03:57:14,260
it's just too difficult and people can't really be bothered to handle that and the speed up is not

2208
03:57:14,260 --> 03:57:21,380
or the speedup is not big enough, or where this unfortunately also has happened, but that's not

2209
03:57:21,380 --> 03:57:30,020
so often on open source things, where one site got better publicity. So it kind of got into the

2210
03:57:30,020 --> 03:57:37,780
market earlier and essentially covered the market with their product. And also with the GPU stuff,

2211
03:57:37,780 --> 03:57:43,300
I would mention that throughout the years, there's been a lot of this kind of situation,

2212
03:57:43,300 --> 03:57:49,380
because the field is still like now it's there's already like established solutions for many things

2213
03:57:49,380 --> 03:57:55,940
but the field is still moving so rapidly that like instructions on let's say how do you replicate

2214
03:57:55,940 --> 03:58:02,020
my results how do you replicate this environment like they can be like very hard to follow even

2215
03:58:02,660 --> 03:58:09,860
half year like after the publication. I submitted for one paper that was published like

2216
03:58:09,860 --> 03:58:14,860
like last spring, there was published paper

2217
03:58:15,500 --> 03:58:18,100
and one of our users wanted to replicate it

2218
03:58:18,100 --> 03:58:20,860
and the instructions were already like broken

2219
03:58:20,860 --> 03:58:24,380
even though it was like published like a half year earlier.

2220
03:58:24,380 --> 03:58:26,940
So, because the field is moving so fast,

2221
03:58:26,940 --> 03:58:29,100
so there's this kind of like additional,

2222
03:58:31,100 --> 03:58:33,380
additional, how could I say it,

2223
03:58:33,380 --> 03:58:38,380
like need for maintenance when it comes to these

2224
03:58:38,380 --> 03:58:44,220
it comes to these more cutting-edge technologies. Some of these MPI codes that have been running

2225
03:58:44,220 --> 03:58:54,060
since the 90s, you can see in the webpages, they don't have a pressing need to update necessarily

2226
03:58:54,060 --> 03:59:02,700
the installation instructions because the technology is so well orchestrated and already

2227
03:59:02,700 --> 03:59:09,660
existing but for many of these GPU codes you need to do a lot more like these kinds of usability

2228
03:59:09,660 --> 03:59:15,740
maintenance in order to make it replicable by other researchers as well. But it's

2229
03:59:15,740 --> 03:59:20,540
of course it's great technology for many cases but you really need to

2230
03:59:21,500 --> 03:59:24,380
you need to know if it's worth the effort and the hassle.

2231
03:59:24,380 --> 03:59:38,500
Come on, please ask us more questions. We're here. Unless everyone just wants to leave

2232
03:59:38,500 --> 03:59:45,940
early. But please give feedback. I see there's about 50 people watching now, and there's

2233
03:59:45,940 --> 03:59:54,820
not 50 answers here. Also maybe people could answer in the feedback why would

2234
03:59:54,820 --> 04:00:00,580
you not attend this course? So in the past few years the number of attendees

2235
04:00:00,580 --> 04:00:05,500
has been going down so of course that could be because all the materials

2236
04:00:05,500 --> 04:00:11,420
online and people don't need to attend which is actually a success but we would

2237
04:00:11,420 --> 04:00:21,120
just like your feedback, is this, let's see, this is like,

2238
04:00:21,220 --> 04:00:23,520
this would be good for us to know, is the material good

2239
04:00:23,520 --> 04:00:26,920
enough? We don't need to be here teaching anymore. Do you value

2240
04:00:26,920 --> 04:00:31,020
us taking the time to go through it? Do you value the videos

2241
04:00:31,020 --> 04:00:34,320
we're making? Are the videos good enough to replace them?

2242
04:00:34,320 --> 04:00:39,820
Yeah. And also like what form of like training do you prefer?

2243
04:00:39,820 --> 04:00:46,700
do you prefer reading the manuals? Do you prefer like watching videos? Do you prefer these kind of

2244
04:00:46,700 --> 04:00:52,940
like live courses where we were like, do you have like averaged talk through of the manuals,

2245
04:00:52,940 --> 04:01:02,540
basically? There's a question about, yeah, what kind of, what to do or when to come to Garage

2246
04:01:02,540 --> 04:01:04,540
and how much time to spend before.

2247
04:01:04,540 --> 04:01:07,540
Personally, I would say

2248
04:01:07,540 --> 04:01:10,540
if you have kind of

2249
04:01:10,540 --> 04:01:13,540
used the search function on our docs

2250
04:01:13,540 --> 04:01:16,540
and didn't immediately find a solution

2251
04:01:16,540 --> 04:01:19,540
for your problem, drop by.

2252
04:01:19,540 --> 04:01:22,540
That's, I think, or

2253
04:01:22,540 --> 04:01:25,540
if you have spent like 10-15 minutes

2254
04:01:25,540 --> 04:01:28,540
googling and you didn't find anything useful

2255
04:01:28,540 --> 04:01:31,580
Yeah, drop by. That's perfectly fine.

2256
04:01:33,580 --> 04:01:38,220
Yeah, the garages, like there was a question, are they similar to these Zoom sessions?

2257
04:01:38,220 --> 04:01:43,100
Yeah, it's basically like we are talking with each other about like what we are doing, like

2258
04:01:43,100 --> 04:01:47,740
having internal discussions and when the customer drops in, we stop that and then we

2259
04:01:47,740 --> 04:01:51,740
ask what the customer wants. And then we try to like,

2260
04:01:52,780 --> 04:01:57,660
usually we go into a breakout room and then somebody tries to help you with the problem.

2261
04:01:58,540 --> 04:02:03,060
It's very informal and we try to keep it that way.

2262
04:02:03,060 --> 04:02:08,980
I think the point with the garage sessions and attending there or coming there and asking

2263
04:02:08,980 --> 04:02:16,220
is we might, if it's something that we think, okay, this is already on the documents and

2264
04:02:16,220 --> 04:02:21,540
we have the impression we have talked about it for, I don't know how many times, we might

2265
04:02:21,540 --> 04:02:28,460
point you to a video or to the resources that are available, but that's perfectly fine.

2266
04:02:28,460 --> 04:02:32,300
Yeah, like if you can't find it. Coming in is perfectly fine.

2267
04:02:32,300 --> 04:02:35,660
Yeah, like if you can't find it and you come and say hey iSearch is there a

2268
04:02:35,660 --> 04:02:38,300
page on this and then we tell you that okay I'll

2269
04:02:38,300 --> 04:02:41,180
come back later. Like sometimes people come and go

2270
04:02:41,180 --> 04:02:45,660
several times in a day or in the hour as they try to

2271
04:02:45,660 --> 04:02:48,860
work themselves some.

2272
04:02:49,100 --> 04:02:53,340
Yeah and the overall like the goal of the thing is

2273
04:02:53,340 --> 04:02:57,420
to help you do stuff right.

2274
04:02:57,420 --> 04:03:06,060
And whatever makes it easier for you to do stuff is, like, if we can help you, that's our top

2275
04:03:06,060 --> 04:03:15,100
profile. So that helps keep us employed if we have people to help. So like, yeah.

2276
04:03:18,380 --> 04:03:23,820
So in the long term, where do you think scientific computing is going? Will HPC

2277
04:03:23,820 --> 04:03:28,780
clusters still be around in 10 years or is there some other solution coming up?

2278
04:03:33,980 --> 04:03:42,140
Or is that it really went with a with a simple question to answer yeah like i i think personally

2279
04:03:42,140 --> 04:03:47,900
like like there's always going to be like the more the technologies move forward like let's say

2280
04:03:47,900 --> 04:03:53,100
for example what has happened with the chat gpt and that sort of stuff there's always going to be

2281
04:03:53,820 --> 04:03:57,540
Like the more, the easier stuff gets,

2282
04:03:57,540 --> 04:04:00,220
like let's say by PyTorch, like also,

2283
04:04:00,220 --> 04:04:03,540
like when we create an interface

2284
04:04:03,540 --> 04:04:06,100
that makes it easier to access, let's say GPUs,

2285
04:04:06,100 --> 04:04:08,300
or in the case of ChatGPT,

2286
04:04:08,300 --> 04:04:12,900
you make it easier to use large language models.

2287
04:04:12,900 --> 04:04:15,740
You create more demand for that thing, right?

2288
04:04:15,740 --> 04:04:18,500
Like there's more people suddenly starting to use that.

2289
04:04:18,500 --> 04:04:20,940
And if you have more people using,

2290
04:04:20,940 --> 04:04:23,140
like demanding this thing,

2291
04:04:23,140 --> 04:04:27,620
it creates more demand on this lower end side

2292
04:04:27,620 --> 04:04:30,340
or the hardware and setting this up.

2293
04:04:30,340 --> 04:04:31,700
How does this actually work?

2294
04:04:31,700 --> 04:04:33,860
How do we manage this demand?

2295
04:04:33,860 --> 04:04:37,540
How can we do this kind of thing?

2296
04:04:37,540 --> 04:04:38,980
And underneath all of it,

2297
04:04:38,980 --> 04:04:41,100
there's going to be either like a cloud service

2298
04:04:41,100 --> 04:04:43,380
or high performance computing service.

2299
04:04:43,380 --> 04:04:45,500
But I think anyways,

2300
04:04:45,500 --> 04:04:48,100
there's going to be something at the bottom

2301
04:04:48,100 --> 04:04:51,820
that is going to require low level stuff.

2302
04:04:51,820 --> 04:04:58,380
And knowing about the whole, like the whole onion, like what's at the surface and what's

2303
04:04:58,380 --> 04:05:05,460
at the deep end is still going to be important because it makes you, makes it easier to navigate

2304
04:05:05,460 --> 04:05:06,460
the whole thing.

2305
04:05:06,460 --> 04:05:08,900
What is the eventual queue system?

2306
04:05:08,900 --> 04:05:13,260
Is it, is the queue system like Slurm or in the cloud infrastructure, is it Kubernetes

2307
04:05:13,260 --> 04:05:14,260
or something?

2308
04:05:14,260 --> 04:05:15,260
That doesn't really matter.

2309
04:05:15,260 --> 04:05:19,180
As long as you know, like that, okay, there's going to be some sort of a queue and some

2310
04:05:19,180 --> 04:05:25,660
sort of an account and I need to run my code in some sort of, let's say, virtual machine

2311
04:05:25,660 --> 04:05:28,860
or node or something.

2312
04:05:28,860 --> 04:05:34,340
As long as you know how the sausage is made, it makes your life easier.

2313
04:05:34,340 --> 04:05:39,140
I think that won't go away ever.

2314
04:05:39,140 --> 04:05:46,220
Maybe there will be more of these bigger systems, but I think there's still going to be these

2315
04:05:46,220 --> 04:05:51,080
systems.

2316
04:05:51,080 --> 04:05:57,480
I would like to also add to the question, is it an appropriate question?

2317
04:05:57,480 --> 04:06:03,960
I think the only questions where I would think a bit like, shouldn't you answer that question

2318
04:06:03,960 --> 04:06:08,280
is if it's really the research question you have in mind.

2319
04:06:08,280 --> 04:06:15,280
We can help you with expertise that we have, and we might actually have someone who has

2320
04:06:15,280 --> 04:06:23,520
worked in the field because most of us are originally researchers, but we are not the

2321
04:06:23,520 --> 04:06:29,920
people to solve your research question. We can try to help you find the right tools to

2322
04:06:29,920 --> 04:06:40,520
solve it, but in the end, you are the expert of your field, so you might need to explain

2323
04:06:40,520 --> 04:06:44,000
to us what you're actually doing and what you're actually trying to achieve so that

2324
04:06:44,000 --> 04:06:51,000
we can give a suggestion on, okay, this might be a good approach to do that.

2325
04:06:51,000 --> 04:06:59,360
But that's kind of, I think, for me, at least the limit of where questions start to not

2326
04:06:59,360 --> 04:07:02,440
make a lot of sense anymore for me.

2327
04:07:02,440 --> 04:07:11,420
Because if someone comes in with a question about a very, very specific niche concept

2328
04:07:11,420 --> 04:07:19,220
of their research field, yeah, it might take quite a bit of time to me to just get what

2329
04:07:19,220 --> 04:07:21,180
they are talking about.

2330
04:07:21,180 --> 04:07:22,180
Yeah.

2331
04:07:22,180 --> 04:07:33,740
There's also a good question about, I completely agree on [name]' answer on the previous one.

2332
04:07:33,740 --> 04:07:37,140
There's a good question of what's the difference between Mamba and Konda.

2333
04:07:37,140 --> 04:07:43,940
basically Mamba, like we nowadays recommend Mamba instead of Conda for most things. So Conda is this

2334
04:07:43,940 --> 04:07:50,100
packaging manager that was originally developed by this Anaconda Incorporated. It used to be

2335
04:07:50,100 --> 04:07:56,820
Continuum Analytics, the firm. They basically created this kind of like, okay, let's make

2336
04:07:56,820 --> 04:08:02,020
installing these data science environments, like virtual environments easier. Because like

2337
04:08:02,020 --> 04:08:07,540
the many Python packages, they depend on these libraries, like, for example, CUDA toolkits and

2338
04:08:07,540 --> 04:08:12,740
that sort of stuff. And installing them via peep can sometimes be quite complicated. So,

2339
04:08:12,740 --> 04:08:18,820
they created this kind of like packaging manager that can do this. And it, like, got popular.

2340
04:08:18,820 --> 04:08:24,260
And then people created this CondaForge, this kind of like distribution channel that people

2341
04:08:24,260 --> 04:08:30,100
can distribute basically like a Linux operating system to wherever Linux system you run. So,

2342
04:08:30,100 --> 04:08:38,100
you can get very reproducible, well, somewhat reproducible environments in all kinds of

2343
04:08:38,100 --> 04:08:43,860
different kinds of computation systems. And for that, they use some packaging manager called

2344
04:08:43,860 --> 04:08:49,060
Conda. And this Conda was originally written in Python, and it contains this kind of like

2345
04:08:49,060 --> 04:08:54,580
SAT solver that tries to solve these environments, like, okay, I want these packages,

2346
04:08:54,580 --> 04:09:02,980
I want these versions. And that was pretty slow. So what people did is that they did what I

2347
04:09:02,980 --> 04:09:08,900
mentioned previously. They used already existing low-level SAT solvers that had been created.

2348
04:09:09,460 --> 04:09:18,420
And on top of that, they rewrote basically Conda functionality in C++ for Mamba. It might be Go

2349
04:09:18,420 --> 04:09:24,960
nowadays. I'm not certain. I think it's C still. But anyway, some low-level code that

2350
04:09:24,960 --> 04:09:33,700
rewrote basically the Conda packaging manager. And it's much more faster than Conda when

2351
04:09:33,700 --> 04:09:39,300
it solves the environments because it uses these low-level SAT solvers to solve how do

2352
04:09:39,300 --> 04:09:44,260
these packages want to. These packets need these packets, these packets need these packets.

2353
04:09:44,260 --> 04:09:45,800
solves the environment for you.

2354
04:09:45,800 --> 04:09:46,300
Yeah.

2355
04:09:49,540 --> 04:09:52,700
It comes down a little bit to the different programming

2356
04:09:52,700 --> 04:09:55,260
languages are there for different purposes.

2357
04:09:55,260 --> 04:10:01,660
And the additional, if you try to stay

2358
04:10:01,660 --> 04:10:04,700
within one of the languages, you might

2359
04:10:04,700 --> 04:10:09,340
get stuck with potentially the disadvantages of it.

2360
04:10:09,340 --> 04:10:12,340
And Python has done a really good job

2361
04:10:12,340 --> 04:10:19,460
in essentially incorporating other languages and making use of other languages being really fast

2362
04:10:19,460 --> 04:10:25,780
while python being really convenient so you have things yeah you have things like what [name]

2363
04:10:25,780 --> 04:10:32,580
mentioned earlier with numpy that's calling essentially c and other highly efficient

2364
04:10:32,580 --> 04:10:41,380
libraries if you would implement the same thing purely in python it would be pretty slow but since

2365
04:10:41,380 --> 04:10:46,500
you can just call other functionality, it's fast.

2366
04:10:50,980 --> 04:10:53,140
That's kind of the problem of conda here.

2367
04:10:55,940 --> 04:11:04,420
Yeah. Somebody creates a new fast thing,

2368
04:11:05,140 --> 04:11:09,540
like a low-level thing that is fast, and then somebody creates an interface

2369
04:11:09,540 --> 04:11:16,740
for that using some higher level, I mean, like easier to understand language and easier

2370
04:11:16,740 --> 04:11:18,940
to write language like Python.

2371
04:11:18,940 --> 04:11:23,540
And then people are like, okay, that looks nice.

2372
04:11:23,540 --> 04:11:24,740
I would want to use that.

2373
04:11:24,740 --> 04:11:26,100
How do I install that?

2374
04:11:26,100 --> 04:11:31,460
And then you first have installation instructions that are very cryptic, very technical.

2375
04:11:31,460 --> 04:11:35,300
And then somebody's like, okay, I would want this to be easier.

2376
04:11:35,300 --> 04:11:39,640
And then they create a packaging manager or something that installs it.

2377
04:11:39,640 --> 04:11:45,540
And now you have a packaging manager that does something really complicated, like you

2378
04:11:45,540 --> 04:11:48,860
start to have this kind of cascade.

2379
04:11:48,860 --> 04:11:54,500
And then there are, of course, alternatives, like, for example, Julia is quite popular

2380
04:11:54,500 --> 04:12:02,620
nowadays that does try to be fast and easy to use at the same time.

2381
04:12:02,620 --> 04:12:04,620
And there's different things.

2382
04:12:04,620 --> 04:12:09,420
But there's this kind of like, usually there's fancy new stuff that is hard to install.

2383
04:12:09,420 --> 04:12:12,900
And it's because it's hard to install, nobody can use it.

2384
04:12:12,900 --> 04:12:17,180
And when it becomes easier to install, more people start to use it.

2385
04:12:17,180 --> 04:12:21,900
And then it starts to get bigger and bigger and somebody figures out, okay, let's create

2386
04:12:21,900 --> 04:12:22,900
a new thing.

2387
04:12:22,900 --> 04:12:28,820
This old thing is too bloated or it has, it's too like, it's not nice anymore.

2388
04:12:28,820 --> 04:12:31,140
And then they create a new fast thing.

2389
04:12:31,140 --> 04:12:37,260
And then that's how the circle of software life or scientific software life basically

2390
04:12:37,260 --> 04:12:38,260
progresses.

2391
04:12:38,260 --> 04:12:39,260
Yeah.

2392
04:12:39,260 --> 04:12:41,820
And Python is always there on the background.

2393
04:12:41,820 --> 04:12:46,340
Like I like that thing, I will take that thing basically, like I will incorporate that into

2394
04:12:46,340 --> 04:12:47,340
my ecosystem.

2395
04:12:47,340 --> 04:12:50,180
And they're very good at doing that.

2396
04:12:50,180 --> 04:12:51,180
Yeah.

2397
04:12:51,180 --> 04:12:52,180
I'm a good Kraken.

2398
04:12:52,180 --> 04:12:53,180
Okay.

2399
04:12:53,180 --> 04:12:54,180
Yeah.

2400
04:12:54,180 --> 04:12:55,180
Yeah.

2401
04:12:55,180 --> 04:12:59,700
I take that and that.

2402
04:12:59,700 --> 04:13:08,980
give some comments on the overall future of your careers. So there is usually comments that our

2403
04:13:08,980 --> 04:13:16,740
courses are both too basic and too advanced. So here we see more people said too advanced, but

2404
04:13:17,620 --> 04:13:25,060
it's really hard to give a course that's both simple enough for everyone and advanced enough

2405
04:13:25,060 --> 04:13:31,460
for everyone. Because what we're doing here is built on so many layers. I think I've said this

2406
04:13:31,460 --> 04:13:37,940
before, but there's like this Linux command line that underlies everything, and then there's like

2407
04:13:37,940 --> 04:13:43,140
data storage which sort of fits in there, then there's the connecting to Triton, then there's

2408
04:13:43,140 --> 04:13:47,380
the Slurm, and then finally there's your program that you're actually trying to run on there.

2409
04:13:47,380 --> 04:13:56,120
And when you know all of these little things down below, using the cluster is not that hard.

2410
04:13:56,120 --> 04:14:00,720
But most people starting your career at a discourse, that's not the case.

2411
04:14:00,720 --> 04:14:05,240
So you might know little bits here and there, but most people don't know all of them.

2412
04:14:05,240 --> 04:14:09,640
And many don't know any very well, and that's okay.

2413
04:14:09,640 --> 04:14:15,820
So the point of this course has been to give a big overview that has something for everyone,

2414
04:14:15,820 --> 04:14:19,300
but most people do need to go follow up some.

2415
04:14:19,300 --> 04:14:22,460
But this also means don't get frustrated here.

2416
04:14:22,460 --> 04:14:25,140
There is a lot of new things to learn.

2417
04:14:25,140 --> 04:14:29,620
And maybe you want to learn it, maybe you don't.

2418
04:14:29,620 --> 04:14:32,460
You have a choice on where you'll go next.

2419
04:14:37,140 --> 04:14:41,380
Yeah, I would add to that that, yes,

2420
04:14:41,380 --> 04:14:50,260
like a lot of this complexity is like technical complexities that are like for historical reasons

2421
04:14:50,260 --> 04:14:56,260
so there's like or like there is a reason for that and that was done like 20 years ago and

2422
04:14:56,260 --> 04:15:00,500
something like that like there's lots of like because it's an incremental process when these

2423
04:15:00,500 --> 04:15:06,180
things happen so there's like lots huge amounts of like this kind of like okay why Mamba or

2424
04:15:06,180 --> 04:15:15,220
something. And then I will give a 10-minute explanation why it's that way. But you don't

2425
04:15:15,220 --> 04:15:23,940
necessarily need to know all of that. You can just go straight to the... What we try in this course

2426
04:15:23,940 --> 04:15:30,260
also, we try to cut to the chase and get to the actual meat of the subject. Like, okay,

2427
04:15:30,260 --> 04:15:37,380
it's not so complicated if you don't care about all of that baggage that comes in these technical

2428
04:15:37,380 --> 04:15:46,260
things. It's a good idea to usually look at or try to go through the things and try to cut through

2429
04:15:46,260 --> 04:15:51,540
the technicalities and get to the actual, okay, what is the thing that I need to focus here?

2430
04:15:53,620 --> 04:15:58,740
Of course, it's complicated when there's so much stuff coming. This course probably is

2431
04:15:58,740 --> 04:16:06,580
overwhelming for many people and that's unfortunate because it would be better if the

2432
04:16:06,580 --> 04:16:14,260
subjects wouldn't be that overwhelming. We try to still give a condensed

2433
04:16:14,260 --> 04:16:17,700
version of the information but there's so much information but don't get

2434
04:16:17,700 --> 04:16:24,820
overwhelmed. Try to check what is important to you and your case and

2435
04:16:24,820 --> 04:16:29,540
Just use that information for your benefit.

2436
04:16:33,540 --> 04:16:34,420
Okay, let's see.

2437
04:16:35,700 --> 04:16:38,900
There's the question about using on-demand on Jupyter

2438
04:16:40,020 --> 04:16:43,380
and whether things are closing automatically.

2439
04:16:44,260 --> 04:16:51,540
At least for Jupyter, if you click on, I think it's shutdown, so file shutdown,

2440
04:16:51,540 --> 04:16:56,540
the job will be finished, so the job will stop.

2441
04:16:56,540 --> 04:17:01,540
If you just close the tab, the job is still run,

2442
04:17:01,720 --> 04:17:06,720
and you can essentially manually delete it

2443
04:17:07,800 --> 04:17:11,460
in the interactive sessions tab.

2444
04:17:11,460 --> 04:17:14,960
And delete there just means that it calls sCancel,

2445
04:17:14,960 --> 04:17:16,440
so it cancels the job.

2446
04:17:16,440 --> 04:17:20,840
So it looks bad, like, oh, do I want to delete stuff?

2447
04:17:20,840 --> 04:17:27,720
like what will happen, but actually it's a Slack cancels the job, so it's a bit more like, yeah.

2448
04:17:31,400 --> 04:17:36,920
And yes, if you click on delete while you still have the Jupyter tab open,

2449
04:17:36,920 --> 04:17:39,800
the Jupyter tab will essentially disconnect as well.

2450
04:17:39,800 --> 04:17:42,800
yeah

2451
04:17:49,400 --> 04:17:54,800
for advanced courses that's probably other people oh there was follow-ups

2452
04:17:54,800 --> 04:17:58,200
that I had started writing here what would you recommend people to do next

2453
04:17:58,200 --> 04:18:04,520
then we can end we have a page Python environment with conda on SciComp Aalto

2454
04:18:04,520 --> 04:18:21,960
I think the path is... I think this is the right path for it. But yeah, this tells

2455
04:18:21,960 --> 04:18:28,000
you a lot about installing your own software in Python. We have two other big

2456
04:18:28,000 --> 04:18:34,280
workshops that are also livestreamed. There's Code Refinery, which is more sort

2457
04:18:34,280 --> 04:18:40,440
version control software testing, like software development, tools for researchers, and a Python

2458
04:18:40,440 --> 04:18:46,120
for scientific computing. Of course, that's also live stream. Through Code Refinery, you can find

2459
04:18:47,480 --> 04:18:55,160
other workshops, which are by our partners. And while CSC has the big training calendars with

2460
04:18:55,160 --> 04:19:02,760
all kinds of advanced things. But for particular pages of SciComp Aalto fee, people should know

2461
04:19:02,760 --> 04:19:07,560
for Aalto usage. Is there anything else that we really have to do?

2462
04:19:14,440 --> 04:19:20,440
What I would just suggest for people is that repetition just makes better. Don't worry

2463
04:19:20,440 --> 04:19:26,840
about it too much about using the cluster. And there's never really a good time to learn new

2464
04:19:26,840 --> 04:19:32,360
skills, right? There's always stuff to do. You have a next deadline or next thing to write.

2465
04:19:32,360 --> 04:19:41,240
So at some point, every one of us has to bite the bullet and just start using a new thing.

2466
04:19:43,320 --> 04:19:52,040
We just have to start. If we want to use a new thing, we don't have to use new things. I still

2467
04:19:52,040 --> 04:20:03,000
use the Veeam, for example, when VS Code exists. We get drawn into our own habits, but if you really

2468
04:20:03,000 --> 04:20:08,360
want to use a new thing, the best way to use it is just to try using it as much as possible.

2469
04:20:10,040 --> 04:20:18,680
In the cluster, you shouldn't be able to do anything that hurts anybody else, except maybe

2470
04:20:18,680 --> 04:20:27,320
reserve resources that somebody else might use, but it's not a big thing. So just try using it,

2471
04:20:27,320 --> 04:20:33,400
and if you run into problems, then contact us. Yeah.

2472
04:20:33,400 --> 04:20:51,040
Okay, should we say goodbye? And hopefully you can use this later and you know how to

2473
04:20:51,040 --> 04:20:58,560
find this if there's more. Any final words?

2474
04:21:01,520 --> 04:21:10,800
I guess not. Okay, thanks a lot, [name]. Yeah, thanks for attending. Yes, thank you.

2475
04:21:10,800 --> 04:21:14,800
Bye. Bye.

2476
04:21:21,040 --> 04:21:23,100
you

2477
04:21:51,040 --> 04:21:53,100
you

2478
04:22:21,040 --> 04:22:23,100
you

2479
04:22:51,040 --> 04:22:53,100
you

2480
04:23:21,040 --> 04:23:23,100
you

2481
04:23:51,040 --> 04:23:53,100
you

2482
04:24:21,040 --> 04:24:23,100
you

2483
04:24:51,040 --> 04:24:53,100
you

2484
04:25:21,040 --> 04:25:23,100
you

2485
04:25:51,040 --> 04:25:53,100
you

2486
04:26:21,040 --> 04:26:23,100
you

