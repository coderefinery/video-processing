1
00:00:00,000 --> 00:00:01,900
Yeah. Okay. Yeah. And for the next two, do we give a

2
00:00:01,900 --> 00:00:06,060
very high-level overview, but

3
00:00:07,740 --> 00:00:15,180
we don't do as many examples? Yeah, yeah. Like, we will run

4
00:00:15,180 --> 00:00:22,220
through some examples, but yeah, so we won't necessarily, because these are very dependent

5
00:00:22,220 --> 00:00:29,420
on your program, so we don't necessarily like, it's very hard to do like a good example because

6
00:00:29,420 --> 00:00:34,540
it depends on what program you're using, but we can demonstrate how the queue thinks about

7
00:00:34,540 --> 00:00:40,300
these things. So the array job, like if we think about the array job that we just had,

8
00:00:40,300 --> 00:00:47,860
But we are running basically the same serial job, but we're just running with more of those.

9
00:00:47,860 --> 00:00:53,960
We are running more of those simultaneously and separately of each other.

10
00:00:53,960 --> 00:00:59,140
But all of those will, like, let's say if we run one of those and that takes like an

11
00:00:59,140 --> 00:01:05,200
hour to run, the other ones will also most likely take an hour to run as well if they're

12
00:01:05,200 --> 00:01:07,240
quite similar jobs.

13
00:01:07,240 --> 00:01:11,120
But in many cases, you would want it to run faster.

14
00:01:11,120 --> 00:01:13,920
You want it to actually finish faster.

15
00:01:13,920 --> 00:01:21,920
And in that case, you might want to try to use multiple processors to make it run faster.

16
00:01:21,920 --> 00:01:26,800
And the first question here is that, can your program actually do it?

17
00:01:26,800 --> 00:01:34,600
Does your program support multiple processors so that it can internally run faster?

18
00:01:34,600 --> 00:01:43,800
If we look at the picture below, again, we see that in the shared memory jobs,

19
00:01:46,360 --> 00:01:55,480
we get for the one single job, and I mean, again, let's focus, let's put the array idea that we had

20
00:01:55,480 --> 00:02:01,080
and put that in the back of our mind, but let's think about the serial job. We start one,

21
00:02:01,080 --> 00:02:08,760
we submit one job first to the queue and in the shared memory job, we would want to give this one

22
00:02:09,480 --> 00:02:15,400
job more resources. We wanted to give that to have access to multiple CPUs.

23
00:02:16,520 --> 00:02:21,880
So in the array job, we had individual jobs that are completely independent and all of those were

24
00:02:21,880 --> 00:02:30,920
running with one CPU. But in this case, we want to give like, okay, let's have one of these scripts

25
00:02:31,080 --> 00:02:35,720
and let this script should be able to use multiple CPUs.

26
00:02:37,240 --> 00:02:40,920
And it's very easy to ask for these resources.

27
00:02:40,920 --> 00:02:45,320
So you can just give this dash dash CPUs per task

28
00:02:45,320 --> 00:02:49,560
and then a number in this Sbatch comment.

29
00:02:52,360 --> 00:02:56,680
Yes, and you can then reserve multiple CPUs for a job.

30
00:02:56,680 --> 00:03:04,860
The question is that does your program actually use those, or can it use those?

31
00:03:04,860 --> 00:03:11,060
In many cases, the program might recognize that there's multiple processes there, and

32
00:03:11,060 --> 00:03:13,940
in some cases it doesn't.

33
00:03:13,940 --> 00:03:22,020
What is usually important in your program, if you see something like nTasks or nProcs

34
00:03:22,020 --> 00:03:30,300
or nCPUs, or CPUs, or processors, something like this,

35
00:03:30,300 --> 00:03:35,620
like a flag that tells that, OK, this can be run in parallel.

36
00:03:35,620 --> 00:03:37,220
What you usually want to do is you

37
00:03:37,220 --> 00:03:39,460
want this number that the process,

38
00:03:39,460 --> 00:03:43,540
like what the program takes in, to be

39
00:03:43,540 --> 00:03:46,020
equal to the number of processors

40
00:03:46,020 --> 00:03:48,620
you have requested from the queue.

41
00:03:48,620 --> 00:03:51,340
And this doesn't happen automatically all the time.

42
00:03:52,220 --> 00:03:59,420
Yes, it doesn't because the program doesn't know where it's being run. The program doesn't

43
00:03:59,420 --> 00:04:05,660
understand that it's in a computer cluster. It's being run in this allocation of, let's say,

44
00:04:05,660 --> 00:04:12,140
four CPUs. The program doesn't necessarily know that, okay, I've been given four CPUs,

45
00:04:12,140 --> 00:04:18,140
And it might see that there's like 40 CPUs or 100 CPUs in the machine.

46
00:04:18,140 --> 00:04:24,140
And it might think that, hey, I can go full ham and I can take all of these CPUs.

47
00:04:24,140 --> 00:04:28,140
But in reality, it's only been given four.

48
00:04:28,140 --> 00:04:41,140
So you might get completely overloaded by having the program at start like 100 processes on those four CPUs.

49
00:04:41,140 --> 00:04:46,000
So, what you usually want to do is you want to check your program, where your program

50
00:04:46,000 --> 00:04:53,520
takes the value of how much parallelism I should use, how much parallelism I should do, and

51
00:04:53,520 --> 00:04:58,520
then make certain that that number matches the request that's in there.

52
00:04:58,520 --> 00:05:01,640
I think I remember some mention of this at the bottom of the page.

53
00:05:01,640 --> 00:05:06,680
Should we look there now, or maybe come back to it after we've seen it?

54
00:05:06,680 --> 00:05:10,800
We can maybe come back to it, but maybe we should just, like, this is too theoretical,

55
00:05:10,800 --> 00:05:13,520
So run something and see if it's faster.

56
00:05:13,520 --> 00:05:14,360
OK.

57
00:05:14,360 --> 00:05:16,000
So I'll scroll down some.

58
00:05:19,040 --> 00:05:20,760
Where's an example?

59
00:05:20,760 --> 00:05:22,440
Here's an example.

60
00:05:22,440 --> 00:05:26,280
Running an example shared memory program.

61
00:05:26,280 --> 00:05:27,320
Yeah.

62
00:05:27,320 --> 00:05:31,880
So the program here is the same Py Script

63
00:05:31,880 --> 00:05:34,120
that we have used as an example.

64
00:05:34,120 --> 00:05:38,800
And it has this flag called dash dash nprocs.

65
00:05:38,800 --> 00:05:40,200
So how many procs?

66
00:05:40,200 --> 00:05:43,240
like it uses Python multiprocessing to start

67
00:05:43,240 --> 00:05:49,560
do the calculation in parallel and any if you give it the number then

68
00:05:49,560 --> 00:05:53,240
it will use multiple processors yeah okay so

69
00:05:53,240 --> 00:05:58,200
let's try running this example over here

70
00:05:58,760 --> 00:06:04,120
so cpus per task equals two so i tell it two processors

71
00:06:04,120 --> 00:06:17,000
Yes. And then, yeah, and the usual suspects like the memory and time and then we call it

72
00:06:18,040 --> 00:06:25,480
and then the number of processes. And do note that now [name] is putting the numbers to be

73
00:06:25,480 --> 00:06:31,560
like the same on like the number of CPUs per task and the number of processors.

74
00:06:31,560 --> 00:06:35,320
Because it's the same over here.

75
00:06:35,320 --> 00:06:38,000
So let's try running it.

76
00:06:38,000 --> 00:06:39,000
Okay.

77
00:06:39,000 --> 00:06:40,000
Okay.

78
00:06:40,000 --> 00:06:46,680
Oh, here it says using two processes.

79
00:06:46,680 --> 00:06:49,800
I guess that's the program printing that out.

80
00:06:49,800 --> 00:06:50,800
Yeah.

81
00:06:50,800 --> 00:06:51,800
Yeah.

82
00:06:51,800 --> 00:06:54,240
It knows that it now needs to use multiple processes.

83
00:06:54,240 --> 00:06:55,720
So maybe let's try it out.

84
00:06:55,720 --> 00:06:58,960
Can you put time at the start of the call?

85
00:06:58,960 --> 00:06:59,960
Okay.

86
00:06:59,960 --> 00:07:05,800
In Linux, there's this program called time that you can put, if you put it before the Srun.

87
00:07:05,800 --> 00:07:07,400
Should I do it before Python?

88
00:07:07,400 --> 00:07:08,200
Yeah, before that.

89
00:07:08,200 --> 00:07:09,240
Yeah, that's good.

90
00:07:09,240 --> 00:07:09,800
Yeah, that's good.

91
00:07:10,600 --> 00:07:15,240
So what the time does is that it will give some like timing information,

92
00:07:15,240 --> 00:07:18,040
like how long did the program execution take.

93
00:07:18,040 --> 00:07:21,560
Let's try it out and see how long it took.

94
00:07:23,240 --> 00:07:25,640
0.91 seconds, user.

95
00:07:25,640 --> 00:07:27,040
or?

96
00:07:27,040 --> 00:07:30,240
Maybe we should add a 0 there into the number of.

97
00:07:30,240 --> 00:07:31,800
OK.

98
00:07:31,800 --> 00:07:34,760
So now we're running 10 times more.

99
00:07:34,760 --> 00:07:35,260
Yeah.

100
00:07:39,080 --> 00:07:40,480
Processes.

101
00:07:40,480 --> 00:07:48,120
So it says 8.7 seconds user time and 4.4 seconds elapsed.

102
00:07:48,120 --> 00:07:52,040
So it did go about twice as fast.

103
00:07:52,040 --> 00:07:52,800
Yes.

104
00:07:52,800 --> 00:07:58,440
We also note that time also produces this 198% CPU.

105
00:07:58,440 --> 00:08:00,120
OK, yeah.

106
00:08:00,120 --> 00:08:03,840
So we can also use the seff that we previously yesterday

107
00:08:03,840 --> 00:08:06,680
talked about to see the utilization.

108
00:08:06,680 --> 00:08:10,440
So if you use the seff and the job ID,

109
00:08:10,440 --> 00:08:15,120
we can quickly check that the utilization, what was it?

110
00:08:15,120 --> 00:08:23,440
So nine seconds.

111
00:08:23,440 --> 00:08:26,000
Here it says only 112.

112
00:08:26,000 --> 00:08:26,520
Yeah.

113
00:08:26,520 --> 00:08:30,920
It might be that maybe it didn't have time to,

114
00:08:30,920 --> 00:08:33,040
because it's so short, maybe it didn't have time

115
00:08:33,040 --> 00:08:34,800
to measure it correctly.

116
00:08:34,800 --> 00:08:36,400
Yeah, probably.

117
00:08:36,400 --> 00:08:37,960
Probably, if that is the case.

118
00:08:37,960 --> 00:08:39,160
Yeah.

119
00:08:39,160 --> 00:08:41,600
So it wasn't that bad, right?

120
00:08:41,600 --> 00:08:44,600
Yeah, I mean, that's not easy.

121
00:08:44,600 --> 00:08:49,360
What happens if I don't give the nprocs here?

122
00:08:49,360 --> 00:08:50,200
Let's try it out.

123
00:08:50,200 --> 00:08:55,960
So if you run it like that without the nprocs.

124
00:08:55,960 --> 00:08:57,040
I remove nprocs.

125
00:09:03,560 --> 00:09:06,520
So it most likely will take about 16 seconds.

126
00:09:06,520 --> 00:09:09,640
So I would assume if the previous one was eight.

127
00:09:09,640 --> 00:09:13,520
So it took, it used 8.4 seconds of CPU

128
00:09:13,520 --> 00:09:17,920
and took 8.5.

129
00:09:17,920 --> 00:09:21,040
So basically, it only ran on one CPU.

130
00:09:21,040 --> 00:09:27,960
And everything, we wasted one CPU.

131
00:09:27,960 --> 00:09:34,600
What happens if I use four CPUs, or I request four CPUs?

132
00:09:34,600 --> 00:09:38,520
Or I go like this.

133
00:09:38,520 --> 00:09:41,600
So I'm telling the program to use more CPUs.

134
00:09:41,600 --> 00:09:45,760
put something bigger there, put something like 32 or something

135
00:09:45,760 --> 00:09:51,040
so that we see an actual thing there.

136
00:09:51,040 --> 00:09:54,360
So now we are overbooking, basically, the CPUs.

137
00:09:54,360 --> 00:09:56,960
We have only requested two CPUs, but we

138
00:09:56,960 --> 00:09:59,800
are running 32 processors there.

139
00:09:59,800 --> 00:10:09,680
So you notice that it didn't run any faster or any slower here.

140
00:10:09,680 --> 00:10:18,080
Yeah. So you notice that it runs at the same speed basically as if it would have two CPUs.

141
00:10:19,840 --> 00:10:24,480
But the difference is, like this is a toy program. If the program would do something

142
00:10:24,480 --> 00:10:29,520
like really complicated, it would slow down the program because it's oversubscribing.

143
00:10:29,520 --> 00:10:31,360
It definitely can slow down.

144
00:10:32,000 --> 00:10:34,880
Yeah. There's a good question in the notes that,

145
00:10:34,880 --> 00:10:40,440
But what is the difference and benefit of allocating multiple CPUs for a single program

146
00:10:40,440 --> 00:10:47,120
against allocating multiple arrays or like I assume like multiple jobs in an array?

147
00:10:47,120 --> 00:10:52,640
And that's an excellent question because like it raises the question that which is better?

148
00:10:52,640 --> 00:11:00,580
Like is it better to run one program twice as fast, but run it like multiple?

149
00:11:00,580 --> 00:11:09,140
Like is it better to run one program twice as fast for double the parameters or is it

150
00:11:09,140 --> 00:11:14,840
better to run two individual programs with different parameters?

151
00:11:14,840 --> 00:11:21,960
So basically, is it better to divide the program among parameters or just run it as fast as

152
00:11:21,960 --> 00:11:25,520
possible but then go through multiple parameters?

153
00:11:25,520 --> 00:11:33,400
And usually the case is that it's usually easier to do the embarrassingly parallel route.

154
00:11:33,400 --> 00:11:39,360
If you want to do maximum throughput or you want to do maximum amount of jobs, maximum

155
00:11:39,360 --> 00:11:45,680
amount of parameters and whatever, it's usually better to use small jobs than big jobs.

156
00:11:45,680 --> 00:11:47,760
So does it depend on the scaling?

157
00:11:47,760 --> 00:11:54,840
So in here, this program is actually under the hood,

158
00:11:54,840 --> 00:11:57,320
basically embarrassingly parallel.

159
00:11:57,320 --> 00:11:59,560
It divides it up.

160
00:11:59,560 --> 00:12:02,440
Each one runs separately.

161
00:12:02,440 --> 00:12:06,200
But some jobs get slower the more processors you add to it.

162
00:12:06,200 --> 00:12:09,120
So in that case, it's better to not use too many

163
00:12:09,120 --> 00:12:11,120
and use arrays instead.

164
00:12:11,120 --> 00:12:14,120
And I guess you can combine both of them at the same time.

165
00:12:14,120 --> 00:12:14,880
Yeah.

166
00:12:14,880 --> 00:12:21,560
The thing that you get with running with multiple processors is that if your program takes a

167
00:12:21,560 --> 00:12:29,640
long time to run, let's say it takes 10 hours to run, you want the results today, not in

168
00:12:29,640 --> 00:12:30,640
10 hours.

169
00:12:30,640 --> 00:12:37,400
When you submit a job, you want them to be done so that you can look at them earlier.

170
00:12:37,400 --> 00:12:44,600
And in that case, if your program can be parallelized, it's possible to maybe get them faster.

171
00:12:44,600 --> 00:12:51,800
results and you can view them faster. And also if the program is big enough that it just

172
00:12:51,800 --> 00:12:59,080
would take way too much time to run. It's good to parallelize it then. It's like the whole point

173
00:12:59,080 --> 00:13:06,520
of the cluster to get stuff done faster, I guess. Yeah. Okay, yeah. So, should we continue

174
00:13:06,520 --> 00:13:11,520
you scrolling down, or is there anything else here?

175
00:13:13,000 --> 00:13:14,240
Tell me when to stop.

176
00:13:14,240 --> 00:13:16,760
Common special cases.

177
00:13:16,760 --> 00:13:20,080
Let's look at the example of the submission script,

178
00:13:20,080 --> 00:13:21,880
how we would write it as a submission script.

179
00:13:21,880 --> 00:13:23,480
Okay, yeah.

180
00:13:23,480 --> 00:13:28,480
So, previously [name] ran the example

181
00:13:28,640 --> 00:13:31,760
with the, from the, like an interactive job,

182
00:13:31,760 --> 00:13:34,680
but we can also run this in, of course,

183
00:13:34,680 --> 00:13:36,100
in a submission script, and that's

184
00:13:36,100 --> 00:13:39,820
a better way of running it to write it into a CLI script.

185
00:13:39,820 --> 00:13:40,320
Yeah.

186
00:13:40,320 --> 00:13:45,000
So in this case, if you copy the script,

187
00:13:45,000 --> 00:13:47,640
or modify the existing, yeah.

188
00:13:47,640 --> 00:13:50,320
Modify.

189
00:13:50,320 --> 00:13:54,080
The previous script wasn't a Pi example, though, so I will.

190
00:13:54,080 --> 00:13:57,240
Yeah, maybe you want to do a completely new one.

191
00:14:01,400 --> 00:14:02,520
OK.

192
00:14:02,520 --> 00:14:07,000
I'll copy most of this over.

193
00:14:07,000 --> 00:14:08,760
Yeah.

194
00:14:08,760 --> 00:14:09,680
So.

195
00:14:09,680 --> 00:14:19,200
So sbatch, so two per task.

196
00:14:19,200 --> 00:14:20,880
Yeah.

197
00:14:20,880 --> 00:14:23,760
And now, because we are running this in the script,

198
00:14:23,760 --> 00:14:27,400
we can utilize the environment variable that Slurm sets.

199
00:14:27,400 --> 00:14:30,560
So we can utilize this environment variable

200
00:14:30,560 --> 00:14:39,840
called slurm CPUs per task, so that like slurm will fit when the code is being run,

201
00:14:39,840 --> 00:14:46,160
that environment variable will contain, similar to what the array ID was, it will contain the

202
00:14:46,160 --> 00:14:55,120
number of CPUs reserved for this program, so it will automatically fit that. This is very helpful

203
00:14:55,120 --> 00:14:59,040
if you're running something and first you run it with, and you want to test out, for example,

204
00:14:59,040 --> 00:15:04,960
scaling of the program, how well it behaves. If you run it with one CPU, if you run it with two

205
00:15:04,960 --> 00:15:14,880
CPUs, four, eight, 16, so forth, it's usually a good idea to use this environment variable because

206
00:15:14,880 --> 00:15:20,240
then you can easily test out different CPU numbers and you don't have to go into your

207
00:15:20,240 --> 00:15:27,360
program code or something to change the number to match the request number. Yeah, okay, so let's

208
00:15:27,360 --> 00:15:30,360
Submit it.

209
00:15:30,360 --> 00:15:31,560
Submit, yes.

210
00:15:34,480 --> 00:15:35,600
Do you think it will work?

211
00:15:39,200 --> 00:15:40,240
The command.

212
00:15:40,240 --> 00:15:40,880
Let's see.

213
00:15:44,680 --> 00:15:48,080
If you look at the Slurm queue, it's probably run already.

214
00:15:48,080 --> 00:15:50,040
It's running.

215
00:15:50,040 --> 00:15:53,320
I added an extra zero there, so it would run a bit slower.

216
00:15:53,320 --> 00:15:54,600
Yeah.

217
00:15:54,600 --> 00:15:56,960
OK, yeah, it finished.

218
00:15:56,960 --> 00:15:58,280
Yeah.

219
00:15:58,280 --> 00:15:59,440
OK.

220
00:15:59,440 --> 00:16:08,120
If we look at the output, I'll cat it.

221
00:16:08,120 --> 00:16:14,560
Using two processes, yeah.

222
00:16:14,560 --> 00:16:17,400
OK, should we seff?

223
00:16:17,400 --> 00:16:21,840
Yeah, and also in the chat or in the notes,

224
00:16:21,840 --> 00:16:23,640
there was a good question of, like,

225
00:16:23,640 --> 00:16:30,040
I used the term processors and processors and like all of this million of words, like

226
00:16:30,040 --> 00:16:34,920
the CPU, there's core, there's process, there's processor.

227
00:16:34,920 --> 00:16:40,960
Sometimes there's thread or some and like all of these have their own like specific

228
00:16:40,960 --> 00:16:47,800
different meaning, but they're basically like, like used in places like a synonyms in many

229
00:16:47,800 --> 00:16:48,800
cases.

230
00:16:48,800 --> 00:16:55,440
There are technical differences to all of these words, and the CPU and all of this.

231
00:16:55,440 --> 00:16:57,960
You shouldn't necessarily worry about it.

232
00:16:57,960 --> 00:17:04,720
The most important thing to note is that in the program, there's something that it can

233
00:17:04,720 --> 00:17:05,720
do.

234
00:17:05,720 --> 00:17:14,920
It can execute two programs at the same time within the same program, basically.

235
00:17:14,920 --> 00:17:23,960
do that, it should have a place to run it, basically. If you go back to the pasta analogy,

236
00:17:23,960 --> 00:17:31,400
instead of just cooking pasta in one pot, we are now cooking pasta in two pots or we're doing

237
00:17:32,440 --> 00:17:41,080
two pots in two burners or stovetops. We just add more stuff there.

238
00:17:41,080 --> 00:17:42,080
Yeah.

239
00:17:42,080 --> 00:17:48,820
So yeah, like whatever term is used, like if the program developers, they will also

240
00:17:48,820 --> 00:17:52,640
use all kinds of terms interchangeably all the time.

241
00:17:52,640 --> 00:17:54,640
So don't worry about it too much.

242
00:17:54,640 --> 00:18:00,240
And that's why I was saying that you should look for these various terms because you will

243
00:18:00,240 --> 00:18:03,280
never know what is the term used by the developer.

244
00:18:03,280 --> 00:18:10,360
They might talk about the process, they might talk about number of CPUs, number of processors,

245
00:18:10,360 --> 00:18:17,760
of tasks, like number of jobs, like the terms might be different, but the important thing

246
00:18:17,760 --> 00:18:26,000
is to know that, okay, these refer to, okay, how many processors or CPUs you want to use

247
00:18:26,000 --> 00:18:31,480
and to match that with the number of CPUs that you request from the queue.

248
00:18:31,480 --> 00:18:38,600
And for the queue, the important thing is the CPUs per task.

249
00:18:38,600 --> 00:18:50,040
I run seff on this thing we just did? Sure. And it looks pretty much like before, so not perfect

250
00:18:50,040 --> 00:18:58,440
efficiency, but it's so short that's expected. Yeah, and do note that here the CPU efficiency

251
00:18:58,440 --> 00:19:06,040
refers to the size of the request, like the reservation. So the CPU efficiency is like

252
00:19:06,040 --> 00:19:13,080
like, with respect to the amount of processors requested.

253
00:19:13,080 --> 00:19:20,120
So 100% would mean that you're using all of the requested CPUs.

254
00:19:20,120 --> 00:19:20,960
Yeah.

255
00:19:20,960 --> 00:19:23,800
And I think you've said this before,

256
00:19:23,800 --> 00:19:26,960
but the most important thing to me

257
00:19:26,960 --> 00:19:31,600
is that this nprocs gets automatically detected

258
00:19:31,600 --> 00:19:32,200
from Slurm.

259
00:19:32,200 --> 00:19:35,640
So I only have to keep it up to date in one place,

260
00:19:35,640 --> 00:19:42,680
which seems like it would save me a lot of time and effort and doing stuff wrong.

261
00:19:42,680 --> 00:19:52,120
Yeah. Below, if we scroll a bit below, there's like this using Slurm CPUs per task efficiently.

262
00:19:52,120 --> 00:20:00,520
And you can use this basically also for the array task ID. So here's a few examples on how you can

263
00:20:00,520 --> 00:20:03,880
use this parameter.

264
00:20:03,880 --> 00:20:06,880
So for example, in Python, like in the middle there,

265
00:20:06,880 --> 00:20:13,760
you can use the OS module to get that environment variable.

266
00:20:13,760 --> 00:20:18,360
So in this case, if the variable is set,

267
00:20:18,360 --> 00:20:21,960
the value of that is converted into an integer.

268
00:20:21,960 --> 00:20:25,160
If it's not set, the default value is now one.

269
00:20:25,160 --> 00:20:27,160
Is one, yeah, OK.

270
00:20:27,160 --> 00:20:35,640
So you can use this when in your program to like you can write this there so that it automatically

271
00:20:35,640 --> 00:20:41,960
detects how many CPUs you're using and then you can use the end CPUs in your like to give it to

272
00:20:41,960 --> 00:20:47,880
a library or something. Yeah and same to MATLAB. Okay yeah so basically programs so if programs

273
00:20:47,880 --> 00:20:53,880
were well designed they would all use something like this to automatically detect number of

274
00:20:53,880 --> 00:20:57,880
processors available.

275
00:20:57,880 --> 00:20:59,880
Okay.

276
00:20:59,880 --> 00:21:01,880
And this is, there is one

277
00:21:01,880 --> 00:21:03,880
extra, like

278
00:21:03,880 --> 00:21:05,880
I will add one extra

279
00:21:05,880 --> 00:21:07,880
technical thing, like there is no standard

280
00:21:07,880 --> 00:21:09,880
on how many CPUs you're going to be using.

281
00:21:09,880 --> 00:21:11,880
The closest thing to a standard

282
00:21:11,880 --> 00:21:13,880
that is used by many programs

283
00:21:13,880 --> 00:21:15,880
is this so-called OMP

284
00:21:15,880 --> 00:21:17,880
num threads. It's mentioned

285
00:21:17,880 --> 00:21:19,880
above

286
00:21:19,880 --> 00:21:21,880
there in the multi-threaded

287
00:21:21,880 --> 00:21:28,920
versus multiprocess and double-booking section, but you can read the whole thing if you want.

288
00:21:30,280 --> 00:21:36,440
Here's the technical stuff if you're interested, but the important thing is that many programs,

289
00:21:36,440 --> 00:21:43,800
for example, like NumPy libraries in Python, they utilize internal parallelism already,

290
00:21:43,800 --> 00:21:49,480
and it's done using this technology called OpenMP, open multiprocessing.

291
00:21:49,480 --> 00:22:01,920
And that basically takes this variable, this OMP non-threads that describes how many processors

292
00:22:01,920 --> 00:22:05,280
it should use.

293
00:22:05,280 --> 00:22:09,560
And this is used in many programs.

294
00:22:09,560 --> 00:22:15,840
Many programs use this as this kind of shorthand for how many processors you should use for

295
00:22:15,840 --> 00:22:17,080
the parallelism.

296
00:22:17,080 --> 00:22:25,340
So this is quite common for programs that don't even use OpenMP to do the parallelism.

297
00:22:25,340 --> 00:22:31,280
So this is the only standard that is basically defined, otherwise it's like a wild west that

298
00:22:31,280 --> 00:22:36,800
everybody uses their different environment variables for how many processors to use.

299
00:22:36,800 --> 00:22:37,800
Yeah.

300
00:22:37,800 --> 00:22:38,800
Okay.

301
00:22:38,800 --> 00:22:45,360
So, we have three minutes before we need to go to a break.

302
00:22:45,360 --> 00:22:48,600
What should we do?

303
00:22:48,600 --> 00:22:54,640
I propose we can see if there's any further questions,

304
00:22:54,640 --> 00:22:56,160
go to the break.

305
00:22:56,160 --> 00:23:01,120
We can quickly discuss the MPI parallelism along with GPUs.

306
00:23:01,120 --> 00:23:07,360
So does that sound good?

307
00:23:07,360 --> 00:23:09,480
Yeah.

308
00:23:09,480 --> 00:23:10,200
OK.

309
00:23:10,200 --> 00:23:12,120
Yeah.

310
00:23:12,120 --> 00:23:17,840
Yeah, so let's see.

311
00:23:17,840 --> 00:23:21,920
What else is there?

312
00:23:21,920 --> 00:23:24,160
I think [name] already answered the processes

313
00:23:24,160 --> 00:23:25,360
versus processors.

314
00:23:28,080 --> 00:23:31,600
So if you want to go deep into the computer architecture,

315
00:23:31,600 --> 00:23:33,720
you can learn all kinds of things

316
00:23:33,720 --> 00:23:37,680
about how Unix systems handle processes and threads

317
00:23:37,680 --> 00:23:39,440
and things like that.

318
00:23:39,440 --> 00:23:41,360
But for the most part, if you know

319
00:23:41,360 --> 00:23:43,720
the options that are present in this page,

320
00:23:43,720 --> 00:23:46,460
you can figure out how to run something,

321
00:23:46,460 --> 00:23:49,480
and then you don't need to know more.

322
00:23:54,040 --> 00:23:57,000
You're probably here because you want to do

323
00:23:57,000 --> 00:24:01,760
science stuff and not learn about computer architecture.

324
00:24:01,760 --> 00:24:05,040
I will quickly mention that a common thing that

325
00:24:05,040 --> 00:24:08,260
happens to many researchers is that,

326
00:24:08,260 --> 00:24:10,640
let's say you run a code in your laptop,

327
00:24:10,640 --> 00:24:15,840
And your laptop, like I mentioned previously, they nowadays have multiple processors there.

328
00:24:16,400 --> 00:24:22,800
And in many cases, if you run this code that can do multiprocessing internally,

329
00:24:22,800 --> 00:24:29,360
it can do something with parallel, without you noticing, basically. You don't even notice.

330
00:24:29,360 --> 00:24:34,480
The Zoom that I'm using now, it's doing something multiprocessing on the background.

331
00:24:34,480 --> 00:24:40,320
I don't even know about it, but it's doing it to make certain that the video stream is correct.

332
00:24:40,320 --> 00:24:44,480
and audio is processed correctly. It's doing multi-processing in the background.

333
00:24:45,760 --> 00:24:52,240
And in my laptop, it just assumes that, okay, I can use whatever resources are available there.

334
00:24:52,800 --> 00:25:00,400
And when people and resources go to the cluster where you need to actually request the resources,

335
00:25:00,400 --> 00:25:08,480
the programs usually just think that, okay, they will look at the hardware that is there.

336
00:25:08,480 --> 00:25:17,520
they see a server with 128 CPUs. And maybe when in the job request,

337
00:25:19,520 --> 00:25:26,800
you might forget to add the CPUs per task flag. So then when you run the code in the cluster,

338
00:25:26,800 --> 00:25:33,920
it's suddenly much slower. It suddenly takes four times as much time or eight times as much time.

339
00:25:33,920 --> 00:25:37,680
And then you might wonder, like, okay, what's going wrong?

340
00:25:37,680 --> 00:25:39,040
Like, am I doing something wrong?

341
00:25:39,040 --> 00:25:41,600
Because it's so much slower in the computing cluster.

342
00:25:41,600 --> 00:25:45,840
Maybe the computing cluster is a pile of crap and I don't want to use that.

343
00:25:46,480 --> 00:25:56,400
But the reality is that the program doesn't necessarily know about what number of processors

344
00:25:56,400 --> 00:25:57,120
it should use.

345
00:25:57,760 --> 00:26:02,800
And also, the program doesn't have access to all of those processors, because those

346
00:26:02,800 --> 00:26:11,920
are being reserved by other jobs in the queue. So when you're running a program in the cluster,

347
00:26:11,920 --> 00:26:18,400
it's usually a good idea to just run it with multiple CPUs per task and see how it goes.

348
00:26:18,400 --> 00:26:24,000
And you can also look at your own task manager in your own laptop and see how many processes,

349
00:26:24,000 --> 00:26:30,800
do you see the processes being used while I run my code? And just decipher, does it use some

350
00:26:30,800 --> 00:26:32,000
Multiprocessing.

351
00:26:32,000 --> 00:26:33,680
Yeah.

352
00:26:33,680 --> 00:26:35,320
OK, so we need to go to the break

353
00:26:35,320 --> 00:26:38,320
so we don't get too far behind.

354
00:26:38,320 --> 00:26:51,240
So break until one minute past the hour.

355
00:26:51,240 --> 00:26:54,960
And then we have to talk on CSC resources.

356
00:26:54,960 --> 00:26:56,960
So see you soon.

357
00:26:59,520 --> 00:27:00,640
Bye.

358
00:27:00,640 --> 00:27:01,140
Bye.

