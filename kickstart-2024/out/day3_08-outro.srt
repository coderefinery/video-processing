1
00:00:00,000 --> 00:00:09,280
Okay. Are we done with GPU? Should we go to the Ask Us Anything and then we can also answer

2
00:00:09,280 --> 00:00:16,000
more GPU questions? Or is there anything else from this page?

3
00:00:16,000 --> 00:00:21,000
Yeah, I think we are good, I would say.

4
00:00:21,520 --> 00:00:25,760
A huge number of the questions in our daily grad session

5
00:00:25,760 --> 00:00:28,680
are about getting GPU stuff to work.

6
00:00:28,680 --> 00:00:31,200
So if you can't get it, it's okay.

7
00:00:31,200 --> 00:00:33,160
Just come and ask, it's normal.

8
00:00:33,160 --> 00:00:35,600
And if you're interested about this,

9
00:00:35,600 --> 00:00:39,600
we have a course on Python for Scientific Computing

10
00:00:39,600 --> 00:00:43,440
where I've given at least talk about how to use Conda.

11
00:00:43,440 --> 00:00:46,480
So the talks are in Code Refinery, I think,

12
00:00:46,480 --> 00:00:48,040
Code Refinery YouTube.

13
00:00:48,040 --> 00:00:51,600
And the materials are in our webpage.

14
00:00:51,600 --> 00:00:52,720
You can get a link there

15
00:00:52,720 --> 00:00:55,280
or in the Code Refinery page as well.

16
00:00:55,280 --> 00:00:59,480
And they're like, we try to go through the process

17
00:00:59,480 --> 00:01:01,920
of how do you like get the corresponding

18
00:01:01,920 --> 00:01:06,320
like toolkits and stuff and that sort of stuff.

19
00:01:06,320 --> 00:01:07,160
Yeah.

20
00:01:08,760 --> 00:01:11,720
Okay. I'm going to switch to the notes then.

21
00:01:13,440 --> 00:01:20,880
And hit auto-scroll.

22
00:01:20,880 --> 00:01:25,120
Whoever has the dependency thing at the bottom of the screen, can you move it?

23
00:01:25,120 --> 00:01:29,680
Because then auto-scroll doesn't work.

24
00:01:29,680 --> 00:01:36,680
Yeah, there's a question there.

25
00:01:36,680 --> 00:01:38,880
Can a metaphor be made for the pasta example?

26
00:01:38,880 --> 00:01:45,720
for example, Conda, TensorFlow, PyTorch, et cetera.

27
00:01:45,720 --> 00:01:50,680
Oh, and we can somewhat add a new section for Q&A

28
00:01:50,680 --> 00:01:52,060
and maybe the feedback stuff.

29
00:01:52,060 --> 00:01:53,360
My hands are a bit full.

30
00:01:53,360 --> 00:01:57,800
And as you know, cat is the most important thing.

31
00:02:03,720 --> 00:02:07,360
Post example, TensorFlow, PyTorch, CUDA.

32
00:02:07,360 --> 00:02:10,760
That's like, I would say that, yeah.

33
00:02:11,760 --> 00:02:17,080
I mean, TensorFlow is a library that helps you to use the, um,

34
00:02:17,640 --> 00:02:23,440
chicken cooker because it's a lot harder than a chicken cooker to use.

35
00:02:25,280 --> 00:02:29,640
And almost no one goes and tries to use the chicken cooker directly.

36
00:02:30,240 --> 00:02:30,520
Yeah.

37
00:02:30,760 --> 00:02:33,840
I would say that, like, maybe, maybe there, it would be something like,

38
00:02:33,840 --> 00:02:39,120
Like, if you think about, you want to cook the chickens in the GPU, right?

39
00:02:39,120 --> 00:02:44,560
And you don't want, you want somebody to probably, like, prepare them beforehand, like butcher

40
00:02:44,560 --> 00:02:46,160
them and that sort of stuff.

41
00:02:46,160 --> 00:02:54,440
So basically, like, the CUDA toolkit is the person in the store that makes certain that

42
00:02:54,440 --> 00:03:02,120
the chickens are, like, prepared so that you can just buy the, like, the chicken in a way

43
00:03:02,120 --> 00:03:10,680
that you can just like safely cook it and it's not like, it doesn't have a skin and everything on it

44
00:03:10,680 --> 00:03:17,800
and that sort of stuff. So basically like it's an intermediate layer that makes it so that

45
00:03:17,800 --> 00:03:20,920
like you can do the hard stuff on that underneath it. Yeah.

46
00:03:20,920 --> 00:03:33,560
But yeah, go ahead and please tell us, like, all the questions about anything from today.

47
00:03:33,560 --> 00:03:34,560
Yeah.

48
00:03:34,560 --> 00:03:40,960
And also, yeah, like, we might have guests joining us, like, other RCs and other people

49
00:03:40,960 --> 00:03:41,960
from us.

50
00:03:41,960 --> 00:03:42,960
Yeah.

51
00:03:42,960 --> 00:03:45,960
Any of the staff that are around.

52
00:03:45,960 --> 00:03:57,080
You should know where to get the studio info out, you don't need to do that.

53
00:04:06,680 --> 00:04:09,320
Yeah, but for the

54
00:04:09,320 --> 00:04:19,280
Basically, like with the GPUs, it has moved, like the thing has moved rapidly throughout

55
00:04:19,280 --> 00:04:20,280
the years.

56
00:04:20,280 --> 00:04:21,280
Hi [name].

57
00:04:21,280 --> 00:04:22,280
Hey.

58
00:04:22,280 --> 00:04:27,840
So, it used to be that GPUs were like, the only way of coding GPUs used to be that you

59
00:04:27,840 --> 00:04:28,840
write the CUDA code.

60
00:04:28,840 --> 00:04:35,840
So, basically you write C or C++ code and then you compile that and use that CUDA toolkit

61
00:04:35,840 --> 00:04:46,560
to like to use that and then of course people didn't find this like very nice in the long term

62
00:04:46,560 --> 00:04:51,200
like they wanted to do something else on that so they created like many frameworks that that make

63
00:04:51,200 --> 00:04:55,760
it possible to do this kind of like stuff so that you don't have to write the C code the low-level

64
00:04:55,760 --> 00:05:04,880
code so then once you get that into that part you still need to compile against this like

65
00:05:04,880 --> 00:05:11,760
low-level code. So what they do is they compile against this low-level code, but then you run

66
00:05:11,760 --> 00:05:16,480
into problems. Okay, you need the dependencies for this low-level code. They need to be solved.

67
00:05:16,480 --> 00:05:21,280
So, okay, how do you manage that? And then it's like, okay, you need to have these packaging

68
00:05:21,280 --> 00:05:28,240
managers to manage the whole environment. And what I usually think about is that,

69
00:05:28,240 --> 00:05:33,920
okay, what is the version of, let's say, PyTorch I want to install? And then let the guards of the

70
00:05:33,920 --> 00:05:46,240
packaging managers decide what versions of CUDA toolkit I get. The person who decided this thing

71
00:05:46,240 --> 00:05:54,960
was some guy in GitHub who made the pull request or made the build for a specific version of PyTorch.

72
00:05:56,640 --> 00:06:02,080
Some guy in an open source community, most of these are nowadays installed from conda forge,

73
00:06:02,080 --> 00:06:07,920
which is this open source community. Some guy who's interested about this, or maybe is hired to

74
00:06:07,920 --> 00:06:17,120
do this, decided that, okay, I will build my PyTorch with these versions, and then they decided

75
00:06:17,120 --> 00:06:23,680
for me which version of the CUDA toolkit needed to be used, and I will trust them to get the correct

76
00:06:23,680 --> 00:06:30,240
version. I try to give an answer to that rule of thumb whether to use

77
00:06:30,240 --> 00:06:38,640
GPUs or just the other CPUs. Personally, if whatever, well, if there are tools around that

78
00:06:38,640 --> 00:06:48,560
support GPUs, go for them. Why not? You can see how long it takes and at some point

79
00:06:48,560 --> 00:06:57,120
essentially make the decision is it worth waiting for the cpu for the gpu resources or is it worth

80
00:06:57,120 --> 00:07:06,240
waiting for a long time to get the to get your results commonly the gpus are quite a substantial

81
00:07:06,240 --> 00:07:16,560
bit faster but i personally would not start writing gpu specific code if if i'm not really

82
00:07:16,560 --> 00:07:24,720
pressed for time. So, if I need to really start writing GPU-specific code,

83
00:07:27,200 --> 00:07:35,600
yeah, I would need to have a good incentive as in my calculations take weeks and then,

84
00:07:36,160 --> 00:07:42,400
okay, well, maybe it's better to think about whether GPUs could speed this up.

85
00:07:42,400 --> 00:07:50,480
Yeah, and often the first question I would ask myself is that, okay, the scientific problem

86
00:07:50,480 --> 00:07:58,680
that I have in hand, what toolkits and what things are there that solve this scientific

87
00:07:58,680 --> 00:07:59,680
problem?

88
00:07:59,680 --> 00:08:08,720
So, for example, I already mentioned, let's say, Python does NumPy, the numerical library.

89
00:08:08,720 --> 00:08:17,360
uses these libraries called BLAST libraries and FFT libraries that are already existing solutions

90
00:08:17,360 --> 00:08:23,680
for solving linear algebra problems, and they are underneath the whole thing very heavily.

91
00:08:24,880 --> 00:08:30,240
If somebody has already created a library, reusing that library is a very good idea.

92
00:08:30,960 --> 00:08:36,800
Basically, if I would want to do, let's say, machine learning, traditional machine learning,

93
00:08:36,800 --> 00:08:41,920
I wouldn't create my own machine learning framework. I would use something like Scikit Learn

94
00:08:41,920 --> 00:08:48,480
that is already existing and I would just extend on that or maybe PyTorch or something like that

95
00:08:48,480 --> 00:08:55,280
if I want to do. If that's my goal. If I need to start from scratch then the question

96
00:08:56,560 --> 00:09:03,440
like it's like creating a completely new code that doesn't have an existing like mathematical

97
00:09:03,440 --> 00:09:10,240
solver for that. I still would try to look for existing mathematical solvers because

98
00:09:10,240 --> 00:09:15,480
I don't trust that my matrix multiplication code is better than what the last people.

99
00:09:15,480 --> 00:09:21,080
They have done it for 20 years or 30 years or even more. They have optimized it for different

100
00:09:21,080 --> 00:09:27,440
kinds of systems and edge cases and for mathematical rigor. I don't trust myself to do that sort

101
00:09:27,440 --> 00:09:37,880
So, I would trust to use as much as possible from people that have already done existing

102
00:09:37,880 --> 00:09:41,640
libraries.

103
00:09:41,640 --> 00:09:47,800
And if those support GPUs, then it's more for me, but I completely agree.

104
00:09:47,800 --> 00:10:02,600
I think there are a few occasions where the worst product in basic libraries has gotten

105
00:10:02,600 --> 00:10:05,920
more popular than the better one.

106
00:10:05,920 --> 00:10:11,940
The only reasons, well, there are two reasons why that happens from time to time.

107
00:10:11,940 --> 00:10:23,380
is that the worse speed wise one is way easier to use. So usability is a big factor also in coding.

108
00:10:23,380 --> 00:10:32,580
If I have an obscure, extremely fast solver for something, then people might not use it because

109
00:10:32,580 --> 00:10:40,260
it's just too difficult and people can't really be bothered to handle that and the speed up is not

110
00:10:40,260 --> 00:10:47,380
or the speedup is not big enough, or where this unfortunately also has happened, but that's not

111
00:10:47,380 --> 00:10:56,020
so often on open source things, where one site got better publicity. So it kind of got into the

112
00:10:56,020 --> 00:11:03,780
market earlier and essentially covered the market with their product. And also with the GPU stuff,

113
00:11:03,780 --> 00:11:09,300
I would mention that throughout the years, there's been a lot of this kind of situation,

114
00:11:09,300 --> 00:11:15,380
because the field is still like now it's there's already like established solutions for many things

115
00:11:15,380 --> 00:11:21,940
but the field is still moving so rapidly that like instructions on let's say how do you replicate

116
00:11:21,940 --> 00:11:28,020
my results how do you replicate this environment like they can be like very hard to follow even

117
00:11:28,660 --> 00:11:35,860
half year like after the publication. I submitted for one paper that was published like

118
00:11:35,860 --> 00:11:40,860
like last spring, there was published paper

119
00:11:41,500 --> 00:11:44,100
and one of our users wanted to replicate it

120
00:11:44,100 --> 00:11:46,860
and the instructions were already like broken

121
00:11:46,860 --> 00:11:50,380
even though it was like published like a half year earlier.

122
00:11:50,380 --> 00:11:52,940
So, because the field is moving so fast,

123
00:11:52,940 --> 00:11:55,100
so there's this kind of like additional,

124
00:11:57,100 --> 00:11:59,380
additional, how could I say it,

125
00:11:59,380 --> 00:12:04,380
like need for maintenance when it comes to these

126
00:12:04,380 --> 00:12:10,220
it comes to these more cutting-edge technologies. Some of these MPI codes that have been running

127
00:12:10,220 --> 00:12:20,060
since the 90s, you can see in the webpages, they don't have a pressing need to update necessarily

128
00:12:20,060 --> 00:12:28,700
the installation instructions because the technology is so well orchestrated and already

129
00:12:28,700 --> 00:12:35,660
existing but for many of these GPU codes you need to do a lot more like these kinds of usability

130
00:12:35,660 --> 00:12:41,740
maintenance in order to make it replicable by other researchers as well. But it's

131
00:12:41,740 --> 00:12:46,540
of course it's great technology for many cases but you really need to

132
00:12:47,500 --> 00:12:50,380
you need to know if it's worth the effort and the hassle.

133
00:12:50,380 --> 00:13:04,500
Come on, please ask us more questions. We're here. Unless everyone just wants to leave

134
00:13:04,500 --> 00:13:11,940
early. But please give feedback. I see there's about 50 people watching now, and there's

135
00:13:11,940 --> 00:13:20,820
not 50 answers here. Also maybe people could answer in the feedback why would

136
00:13:20,820 --> 00:13:26,580
you not attend this course? So in the past few years the number of attendees

137
00:13:26,580 --> 00:13:31,500
has been going down so of course that could be because all the materials

138
00:13:31,500 --> 00:13:37,420
online and people don't need to attend which is actually a success but we would

139
00:13:37,420 --> 00:13:47,120
just like your feedback, is this, let's see, this is like,

140
00:13:47,220 --> 00:13:49,520
this would be good for us to know, is the material good

141
00:13:49,520 --> 00:13:52,920
enough? We don't need to be here teaching anymore. Do you value

142
00:13:52,920 --> 00:13:57,020
us taking the time to go through it? Do you value the videos

143
00:13:57,020 --> 00:14:00,320
we're making? Are the videos good enough to replace them?

144
00:14:00,320 --> 00:14:05,820
Yeah. And also like what form of like training do you prefer?

145
00:14:05,820 --> 00:14:12,700
do you prefer reading the manuals? Do you prefer like watching videos? Do you prefer these kind of

146
00:14:12,700 --> 00:14:18,940
like live courses where we were like, do you have like averaged talk through of the manuals,

147
00:14:18,940 --> 00:14:28,540
basically? There's a question about, yeah, what kind of, what to do or when to come to Garage

148
00:14:28,540 --> 00:14:30,540
and how much time to spend before.

149
00:14:30,540 --> 00:14:33,540
Personally, I would say

150
00:14:33,540 --> 00:14:36,540
if you have kind of

151
00:14:36,540 --> 00:14:39,540
used the search function on our docs

152
00:14:39,540 --> 00:14:42,540
and didn't immediately find a solution

153
00:14:42,540 --> 00:14:45,540
for your problem, drop by.

154
00:14:45,540 --> 00:14:48,540
That's, I think, or

155
00:14:48,540 --> 00:14:51,540
if you have spent like 10-15 minutes

156
00:14:51,540 --> 00:14:54,540
googling and you didn't find anything useful

157
00:14:54,540 --> 00:14:57,580
Yeah, drop by. That's perfectly fine.

158
00:14:59,580 --> 00:15:04,220
Yeah, the garages, like there was a question, are they similar to these Zoom sessions?

159
00:15:04,220 --> 00:15:09,100
Yeah, it's basically like we are talking with each other about like what we are doing, like

160
00:15:09,100 --> 00:15:13,740
having internal discussions and when the customer drops in, we stop that and then we

161
00:15:13,740 --> 00:15:17,740
ask what the customer wants. And then we try to like,

162
00:15:18,780 --> 00:15:23,660
usually we go into a breakout room and then somebody tries to help you with the problem.

163
00:15:24,540 --> 00:15:29,060
It's very informal and we try to keep it that way.

164
00:15:29,060 --> 00:15:34,980
I think the point with the garage sessions and attending there or coming there and asking

165
00:15:34,980 --> 00:15:42,220
is we might, if it's something that we think, okay, this is already on the documents and

166
00:15:42,220 --> 00:15:47,540
we have the impression we have talked about it for, I don't know how many times, we might

167
00:15:47,540 --> 00:15:54,460
point you to a video or to the resources that are available, but that's perfectly fine.

168
00:15:54,460 --> 00:15:58,300
Yeah, like if you can't find it. Coming in is perfectly fine.

169
00:15:58,300 --> 00:16:01,660
Yeah, like if you can't find it and you come and say hey iSearch is there a

170
00:16:01,660 --> 00:16:04,300
page on this and then we tell you that okay I'll

171
00:16:04,300 --> 00:16:07,180
come back later. Like sometimes people come and go

172
00:16:07,180 --> 00:16:11,660
several times in a day or in the hour as they try to

173
00:16:11,660 --> 00:16:14,860
work themselves some.

174
00:16:15,100 --> 00:16:19,340
Yeah and the overall like the goal of the thing is

175
00:16:19,340 --> 00:16:23,420
to help you do stuff right.

176
00:16:23,420 --> 00:16:32,060
And whatever makes it easier for you to do stuff is, like, if we can help you, that's our top

177
00:16:32,060 --> 00:16:41,100
profile. So that helps keep us employed if we have people to help. So like, yeah.

178
00:16:44,380 --> 00:16:49,820
So in the long term, where do you think scientific computing is going? Will HPC

179
00:16:49,820 --> 00:16:54,780
clusters still be around in 10 years or is there some other solution coming up?

180
00:16:59,980 --> 00:17:08,140
Or is that it really went with a with a simple question to answer yeah like i i think personally

181
00:17:08,140 --> 00:17:13,900
like like there's always going to be like the more the technologies move forward like let's say

182
00:17:13,900 --> 00:17:19,100
for example what has happened with the chat gpt and that sort of stuff there's always going to be

183
00:17:19,820 --> 00:17:23,540
Like the more, the easier stuff gets,

184
00:17:23,540 --> 00:17:26,220
like let's say by PyTorch, like also,

185
00:17:26,220 --> 00:17:29,540
like when we create an interface

186
00:17:29,540 --> 00:17:32,100
that makes it easier to access, let's say GPUs,

187
00:17:32,100 --> 00:17:34,300
or in the case of ChatGPT,

188
00:17:34,300 --> 00:17:38,900
you make it easier to use large language models.

189
00:17:38,900 --> 00:17:41,740
You create more demand for that thing, right?

190
00:17:41,740 --> 00:17:44,500
Like there's more people suddenly starting to use that.

191
00:17:44,500 --> 00:17:46,940
And if you have more people using,

192
00:17:46,940 --> 00:17:49,140
like demanding this thing,

193
00:17:49,140 --> 00:17:53,620
it creates more demand on this lower end side

194
00:17:53,620 --> 00:17:56,340
or the hardware and setting this up.

195
00:17:56,340 --> 00:17:57,700
How does this actually work?

196
00:17:57,700 --> 00:17:59,860
How do we manage this demand?

197
00:17:59,860 --> 00:18:03,540
How can we do this kind of thing?

198
00:18:03,540 --> 00:18:04,980
And underneath all of it,

199
00:18:04,980 --> 00:18:07,100
there's going to be either like a cloud service

200
00:18:07,100 --> 00:18:09,380
or high performance computing service.

201
00:18:09,380 --> 00:18:11,500
But I think anyways,

202
00:18:11,500 --> 00:18:14,100
there's going to be something at the bottom

203
00:18:14,100 --> 00:18:17,820
that is going to require low level stuff.

204
00:18:17,820 --> 00:18:24,380
And knowing about the whole, like the whole onion, like what's at the surface and what's

205
00:18:24,380 --> 00:18:31,460
at the deep end is still going to be important because it makes you, makes it easier to navigate

206
00:18:31,460 --> 00:18:32,460
the whole thing.

207
00:18:32,460 --> 00:18:34,900
What is the eventual queue system?

208
00:18:34,900 --> 00:18:39,260
Is it, is the queue system like Slurm or in the cloud infrastructure, is it Kubernetes

209
00:18:39,260 --> 00:18:40,260
or something?

210
00:18:40,260 --> 00:18:41,260
That doesn't really matter.

211
00:18:41,260 --> 00:18:45,180
As long as you know, like that, okay, there's going to be some sort of a queue and some

212
00:18:45,180 --> 00:18:51,660
sort of an account and I need to run my code in some sort of, let's say, virtual machine

213
00:18:51,660 --> 00:18:54,860
or node or something.

214
00:18:54,860 --> 00:19:00,340
As long as you know how the sausage is made, it makes your life easier.

215
00:19:00,340 --> 00:19:05,140
I think that won't go away ever.

216
00:19:05,140 --> 00:19:12,220
Maybe there will be more of these bigger systems, but I think there's still going to be these

217
00:19:12,220 --> 00:19:17,080
systems.

218
00:19:17,080 --> 00:19:23,480
I would like to also add to the question, is it an appropriate question?

219
00:19:23,480 --> 00:19:29,960
I think the only questions where I would think a bit like, shouldn't you answer that question

220
00:19:29,960 --> 00:19:34,280
is if it's really the research question you have in mind.

221
00:19:34,280 --> 00:19:41,280
We can help you with expertise that we have, and we might actually have someone who has

222
00:19:41,280 --> 00:19:49,520
worked in the field because most of us are originally researchers, but we are not the

223
00:19:49,520 --> 00:19:55,920
people to solve your research question. We can try to help you find the right tools to

224
00:19:55,920 --> 00:20:06,520
solve it, but in the end, you are the expert of your field, so you might need to explain

225
00:20:06,520 --> 00:20:10,000
to us what you're actually doing and what you're actually trying to achieve so that

226
00:20:10,000 --> 00:20:17,000
we can give a suggestion on, okay, this might be a good approach to do that.

227
00:20:17,000 --> 00:20:25,360
But that's kind of, I think, for me, at least the limit of where questions start to not

228
00:20:25,360 --> 00:20:28,440
make a lot of sense anymore for me.

229
00:20:28,440 --> 00:20:37,420
Because if someone comes in with a question about a very, very specific niche concept

230
00:20:37,420 --> 00:20:45,220
of their research field, yeah, it might take quite a bit of time to me to just get what

231
00:20:45,220 --> 00:20:47,180
they are talking about.

232
00:20:47,180 --> 00:20:48,180
Yeah.

233
00:20:48,180 --> 00:20:59,740
There's also a good question about, I completely agree on [name]' answer on the previous one.

234
00:20:59,740 --> 00:21:03,140
There's a good question of what's the difference between Mamba and Konda.

235
00:21:03,140 --> 00:21:09,940
basically Mamba, like we nowadays recommend Mamba instead of Conda for most things. So Conda is this

236
00:21:09,940 --> 00:21:16,100
packaging manager that was originally developed by this Anaconda Incorporated. It used to be

237
00:21:16,100 --> 00:21:22,820
Continuum Analytics, the firm. They basically created this kind of like, okay, let's make

238
00:21:22,820 --> 00:21:28,020
installing these data science environments, like virtual environments easier. Because like

239
00:21:28,020 --> 00:21:33,540
the many Python packages, they depend on these libraries, like, for example, CUDA toolkits and

240
00:21:33,540 --> 00:21:38,740
that sort of stuff. And installing them via peep can sometimes be quite complicated. So,

241
00:21:38,740 --> 00:21:44,820
they created this kind of like packaging manager that can do this. And it, like, got popular.

242
00:21:44,820 --> 00:21:50,260
And then people created this CondaForge, this kind of like distribution channel that people

243
00:21:50,260 --> 00:21:56,100
can distribute basically like a Linux operating system to wherever Linux system you run. So,

244
00:21:56,100 --> 00:22:04,100
you can get very reproducible, well, somewhat reproducible environments in all kinds of

245
00:22:04,100 --> 00:22:09,860
different kinds of computation systems. And for that, they use some packaging manager called

246
00:22:09,860 --> 00:22:15,060
Conda. And this Conda was originally written in Python, and it contains this kind of like

247
00:22:15,060 --> 00:22:20,580
SAT solver that tries to solve these environments, like, okay, I want these packages,

248
00:22:20,580 --> 00:22:28,980
I want these versions. And that was pretty slow. So what people did is that they did what I

249
00:22:28,980 --> 00:22:34,900
mentioned previously. They used already existing low-level SAT solvers that had been created.

250
00:22:35,460 --> 00:22:44,420
And on top of that, they rewrote basically Conda functionality in C++ for Mamba. It might be Go

251
00:22:44,420 --> 00:22:50,960
nowadays. I'm not certain. I think it's C still. But anyway, some low-level code that

252
00:22:50,960 --> 00:22:59,700
rewrote basically the Conda packaging manager. And it's much more faster than Conda when

253
00:22:59,700 --> 00:23:05,300
it solves the environments because it uses these low-level SAT solvers to solve how do

254
00:23:05,300 --> 00:23:10,260
these packages want to. These packets need these packets, these packets need these packets.

255
00:23:10,260 --> 00:23:11,800
solves the environment for you.

256
00:23:11,800 --> 00:23:12,300
Yeah.

257
00:23:15,540 --> 00:23:18,700
It comes down a little bit to the different programming

258
00:23:18,700 --> 00:23:21,260
languages are there for different purposes.

259
00:23:21,260 --> 00:23:27,660
And the additional, if you try to stay

260
00:23:27,660 --> 00:23:30,700
within one of the languages, you might

261
00:23:30,700 --> 00:23:35,340
get stuck with potentially the disadvantages of it.

262
00:23:35,340 --> 00:23:38,340
And Python has done a really good job

263
00:23:38,340 --> 00:23:45,460
in essentially incorporating other languages and making use of other languages being really fast

264
00:23:45,460 --> 00:23:51,780
while python being really convenient so you have things yeah you have things like what [name]

265
00:23:51,780 --> 00:23:58,580
mentioned earlier with numpy that's calling essentially c and other highly efficient

266
00:23:58,580 --> 00:24:07,380
libraries if you would implement the same thing purely in python it would be pretty slow but since

267
00:24:07,380 --> 00:24:12,500
you can just call other functionality, it's fast.

268
00:24:16,980 --> 00:24:19,140
That's kind of the problem of conda here.

269
00:24:21,940 --> 00:24:30,420
Yeah. Somebody creates a new fast thing,

270
00:24:31,140 --> 00:24:35,540
like a low-level thing that is fast, and then somebody creates an interface

271
00:24:35,540 --> 00:24:42,740
for that using some higher level, I mean, like easier to understand language and easier

272
00:24:42,740 --> 00:24:44,940
to write language like Python.

273
00:24:44,940 --> 00:24:49,540
And then people are like, okay, that looks nice.

274
00:24:49,540 --> 00:24:50,740
I would want to use that.

275
00:24:50,740 --> 00:24:52,100
How do I install that?

276
00:24:52,100 --> 00:24:57,460
And then you first have installation instructions that are very cryptic, very technical.

277
00:24:57,460 --> 00:25:01,300
And then somebody's like, okay, I would want this to be easier.

278
00:25:01,300 --> 00:25:05,640
And then they create a packaging manager or something that installs it.

279
00:25:05,640 --> 00:25:11,540
And now you have a packaging manager that does something really complicated, like you

280
00:25:11,540 --> 00:25:14,860
start to have this kind of cascade.

281
00:25:14,860 --> 00:25:20,500
And then there are, of course, alternatives, like, for example, Julia is quite popular

282
00:25:20,500 --> 00:25:28,620
nowadays that does try to be fast and easy to use at the same time.

283
00:25:28,620 --> 00:25:30,620
And there's different things.

284
00:25:30,620 --> 00:25:35,420
But there's this kind of like, usually there's fancy new stuff that is hard to install.

285
00:25:35,420 --> 00:25:38,900
And it's because it's hard to install, nobody can use it.

286
00:25:38,900 --> 00:25:43,180
And when it becomes easier to install, more people start to use it.

287
00:25:43,180 --> 00:25:47,900
And then it starts to get bigger and bigger and somebody figures out, okay, let's create

288
00:25:47,900 --> 00:25:48,900
a new thing.

289
00:25:48,900 --> 00:25:54,820
This old thing is too bloated or it has, it's too like, it's not nice anymore.

290
00:25:54,820 --> 00:25:57,140
And then they create a new fast thing.

291
00:25:57,140 --> 00:26:03,260
And then that's how the circle of software life or scientific software life basically

292
00:26:03,260 --> 00:26:04,260
progresses.

293
00:26:04,260 --> 00:26:05,260
Yeah.

294
00:26:05,260 --> 00:26:07,820
And Python is always there on the background.

295
00:26:07,820 --> 00:26:12,340
Like I like that thing, I will take that thing basically, like I will incorporate that into

296
00:26:12,340 --> 00:26:13,340
my ecosystem.

297
00:26:13,340 --> 00:26:16,180
And they're very good at doing that.

298
00:26:16,180 --> 00:26:17,180
Yeah.

299
00:26:17,180 --> 00:26:18,180
I'm a good Kraken.

300
00:26:18,180 --> 00:26:19,180
Okay.

301
00:26:19,180 --> 00:26:20,180
Yeah.

302
00:26:20,180 --> 00:26:21,180
Yeah.

303
00:26:21,180 --> 00:26:25,700
I take that and that.

304
00:26:25,700 --> 00:26:34,980
give some comments on the overall future of your careers. So there is usually comments that our

305
00:26:34,980 --> 00:26:42,740
courses are both too basic and too advanced. So here we see more people said too advanced, but

306
00:26:43,620 --> 00:26:51,060
it's really hard to give a course that's both simple enough for everyone and advanced enough

307
00:26:51,060 --> 00:26:57,460
for everyone. Because what we're doing here is built on so many layers. I think I've said this

308
00:26:57,460 --> 00:27:03,940
before, but there's like this Linux command line that underlies everything, and then there's like

309
00:27:03,940 --> 00:27:09,140
data storage which sort of fits in there, then there's the connecting to Triton, then there's

310
00:27:09,140 --> 00:27:13,380
the Slurm, and then finally there's your program that you're actually trying to run on there.

311
00:27:13,380 --> 00:27:22,120
And when you know all of these little things down below, using the cluster is not that hard.

312
00:27:22,120 --> 00:27:26,720
But most people starting your career at a discourse, that's not the case.

313
00:27:26,720 --> 00:27:31,240
So you might know little bits here and there, but most people don't know all of them.

314
00:27:31,240 --> 00:27:35,640
And many don't know any very well, and that's okay.

315
00:27:35,640 --> 00:27:41,820
So the point of this course has been to give a big overview that has something for everyone,

316
00:27:41,820 --> 00:27:45,300
but most people do need to go follow up some.

317
00:27:45,300 --> 00:27:48,460
But this also means don't get frustrated here.

318
00:27:48,460 --> 00:27:51,140
There is a lot of new things to learn.

319
00:27:51,140 --> 00:27:55,620
And maybe you want to learn it, maybe you don't.

320
00:27:55,620 --> 00:27:58,460
You have a choice on where you'll go next.

321
00:28:03,140 --> 00:28:07,380
Yeah, I would add to that that, yes,

322
00:28:07,380 --> 00:28:16,260
like a lot of this complexity is like technical complexities that are like for historical reasons

323
00:28:16,260 --> 00:28:22,260
so there's like or like there is a reason for that and that was done like 20 years ago and

324
00:28:22,260 --> 00:28:26,500
something like that like there's lots of like because it's an incremental process when these

325
00:28:26,500 --> 00:28:32,180
things happen so there's like lots huge amounts of like this kind of like okay why Mamba or

326
00:28:32,180 --> 00:28:41,220
something. And then I will give a 10-minute explanation why it's that way. But you don't

327
00:28:41,220 --> 00:28:49,940
necessarily need to know all of that. You can just go straight to the... What we try in this course

328
00:28:49,940 --> 00:28:56,260
also, we try to cut to the chase and get to the actual meat of the subject. Like, okay,

329
00:28:56,260 --> 00:29:03,380
it's not so complicated if you don't care about all of that baggage that comes in these technical

330
00:29:03,380 --> 00:29:12,260
things. It's a good idea to usually look at or try to go through the things and try to cut through

331
00:29:12,260 --> 00:29:17,540
the technicalities and get to the actual, okay, what is the thing that I need to focus here?

332
00:29:19,620 --> 00:29:24,740
Of course, it's complicated when there's so much stuff coming. This course probably is

333
00:29:24,740 --> 00:29:32,580
overwhelming for many people and that's unfortunate because it would be better if the

334
00:29:32,580 --> 00:29:40,260
subjects wouldn't be that overwhelming. We try to still give a condensed

335
00:29:40,260 --> 00:29:43,700
version of the information but there's so much information but don't get

336
00:29:43,700 --> 00:29:50,820
overwhelmed. Try to check what is important to you and your case and

337
00:29:50,820 --> 00:29:55,540
Just use that information for your benefit.

338
00:29:59,540 --> 00:30:00,420
Okay, let's see.

339
00:30:01,700 --> 00:30:04,900
There's the question about using on-demand on Jupyter

340
00:30:06,020 --> 00:30:09,380
and whether things are closing automatically.

341
00:30:10,260 --> 00:30:17,540
At least for Jupyter, if you click on, I think it's shutdown, so file shutdown,

342
00:30:17,540 --> 00:30:22,540
the job will be finished, so the job will stop.

343
00:30:22,540 --> 00:30:27,540
If you just close the tab, the job is still run,

344
00:30:27,720 --> 00:30:32,720
and you can essentially manually delete it

345
00:30:33,800 --> 00:30:37,460
in the interactive sessions tab.

346
00:30:37,460 --> 00:30:40,960
And delete there just means that it calls sCancel,

347
00:30:40,960 --> 00:30:42,440
so it cancels the job.

348
00:30:42,440 --> 00:30:46,840
So it looks bad, like, oh, do I want to delete stuff?

349
00:30:46,840 --> 00:30:53,720
like what will happen, but actually it's a Slack cancels the job, so it's a bit more like, yeah.

350
00:30:57,400 --> 00:31:02,920
And yes, if you click on delete while you still have the Jupyter tab open,

351
00:31:02,920 --> 00:31:05,800
the Jupyter tab will essentially disconnect as well.

352
00:31:05,800 --> 00:31:08,800
yeah

353
00:31:15,400 --> 00:31:20,800
for advanced courses that's probably other people oh there was follow-ups

354
00:31:20,800 --> 00:31:24,200
that I had started writing here what would you recommend people to do next

355
00:31:24,200 --> 00:31:30,520
then we can end we have a page Python environment with conda on SciComp Aalto

356
00:31:30,520 --> 00:31:47,960
I think the path is... I think this is the right path for it. But yeah, this tells

357
00:31:47,960 --> 00:31:54,000
you a lot about installing your own software in Python. We have two other big

358
00:31:54,000 --> 00:32:00,280
workshops that are also livestreamed. There's Code Refinery, which is more sort

359
00:32:00,280 --> 00:32:06,440
version control software testing, like software development, tools for researchers, and a Python

360
00:32:06,440 --> 00:32:12,120
for scientific computing. Of course, that's also live stream. Through Code Refinery, you can find

361
00:32:13,480 --> 00:32:21,160
other workshops, which are by our partners. And while CSC has the big training calendars with

362
00:32:21,160 --> 00:32:28,760
all kinds of advanced things. But for particular pages of SciComp Aalto fee, people should know

363
00:32:28,760 --> 00:32:33,560
for Aalto usage. Is there anything else that we really have to do?

364
00:32:40,440 --> 00:32:46,440
What I would just suggest for people is that repetition just makes better. Don't worry

365
00:32:46,440 --> 00:32:52,840
about it too much about using the cluster. And there's never really a good time to learn new

366
00:32:52,840 --> 00:32:58,360
skills, right? There's always stuff to do. You have a next deadline or next thing to write.

367
00:32:58,360 --> 00:33:07,240
So at some point, every one of us has to bite the bullet and just start using a new thing.

368
00:33:09,320 --> 00:33:18,040
We just have to start. If we want to use a new thing, we don't have to use new things. I still

369
00:33:18,040 --> 00:33:29,000
use the Veeam, for example, when VS Code exists. We get drawn into our own habits, but if you really

370
00:33:29,000 --> 00:33:34,360
want to use a new thing, the best way to use it is just to try using it as much as possible.

371
00:33:36,040 --> 00:33:44,680
In the cluster, you shouldn't be able to do anything that hurts anybody else, except maybe

372
00:33:44,680 --> 00:33:53,320
reserve resources that somebody else might use, but it's not a big thing. So just try using it,

373
00:33:53,320 --> 00:33:59,400
and if you run into problems, then contact us. Yeah.

374
00:33:59,400 --> 00:34:17,040
Okay, should we say goodbye? And hopefully you can use this later and you know how to

375
00:34:17,040 --> 00:34:24,560
find this if there's more. Any final words?

376
00:34:27,520 --> 00:34:36,800
I guess not. Okay, thanks a lot, [name]. Yeah, thanks for attending. Yes, thank you.

377
00:34:36,800 --> 00:34:40,800
Bye. Bye.

